

=== FILE: BlochQubitNode.py ===

"""
Bloch Qubit Node - Simulates a single qubit and plots its state on the Bloch Sphere.
Takes rotation angles as signal inputs.
Ported from qbit.py logic.
Requires: pip install numpy scipy
"""

import numpy as np
from PyQt6 import QtGui
import cv2
from scipy.linalg import expm # For matrix exponentiation

import __main__
BaseNode = __main__.BaseNode
PA_INSTANCE = getattr(__main__, "PA_INSTANCE", None)
QtGui = __main__.QtGui

try:
    from scipy.linalg import expm
    SCIPY_AVAILABLE = True
except ImportError:
    SCIPY_AVAILABLE = False
    print("Warning: BlochQubitNode requires 'scipy'.")
    
# --- Pauli and Gate Matrices ---
# Simplified Hamiltonians for rotation gates (H_i = -i/2 * sigma_i)
H_X = np.array([[0, 1], [1, 0]], dtype=complex) * 0.5
H_Y = np.array([[0, -1j], [1j, 0]], dtype=complex) * 0.5
H_Z = np.array([[1, 0], [0, -1]], dtype=complex) * 0.5
H_Hadamard = np.array([[1, 1], [1, -1]]) / np.sqrt(2)

class BlochQubitNode(BaseNode):
    NODE_CATEGORY = "Analysis"
    NODE_COLOR = QtGui.QColor(100, 100, 255) # Blue Qubit
    
    def __init__(self, hbar=1.0):
        super().__init__()
        self.node_title = "Bloch Qubit"
        
        self.inputs = {
            'rx_angle': 'signal',      # X-axis rotation angle
            'ry_angle': 'signal',      # Y-axis rotation angle
            'rz_angle': 'signal',      # Z-axis rotation angle
            'hadamard_trigger': 'signal' # Trigger Hadamard gate
        }
        self.outputs = {
            'bloch_x': 'signal',
            'bloch_y': 'signal',
            'bloch_z': 'signal',
            'prob_0': 'signal'
        }
        
        if not SCIPY_AVAILABLE:
            self.node_title = "Qubit (No SciPy!)"
            return

        self.hbar = float(hbar)
        
        # Initialize state: |0> (North Pole)
        self.state = np.array([1.0 + 0j, 0.0 + 0j], dtype=complex)
        
        # History for visualization
        self.bloch_coords_history = np.zeros((100, 3), dtype=np.float32)
        self.bloch_coords = self._get_bloch_coords(self.state)
        
        self.last_hadamard_trigger = 0.0

    def _get_bloch_coords(self, state):
        """Convert the qubit state to Bloch sphere coordinates (x,y,z)"""
        a, b = state
        x = 2 * np.real(a * np.conj(b))
        y = 2 * np.imag(a * np.conj(b))
        z = abs(a)**2 - abs(b)**2
        return np.array([x, y, z])

    def _apply_gate_hamiltonian(self, H, angle):
        """Evolve the state under Hamiltonian H for time proportional to angle."""
        if angle == 0.0: return
        
        # U = exp(-i H_matrix * angle / hbar)
        # We define H_matrix = H_i * 0.5 (from Qubit class example)
        # So U = expm(-1j * H * (angle / self.hbar))
        U = expm(-1j * H * (angle / self.hbar))
        self.state = U.dot(self.state)
        
        # Re-normalize (essential for numerical stability)
        norm = np.linalg.norm(self.state)
        if norm > 1e-9:
            self.state /= norm
    
    def step(self):
        if not SCIPY_AVAILABLE:
            return
            
        # 1. Get angle inputs (mapped from standard [-1, 1] signal)
        rx_angle = (self.get_blended_input('rx_angle', 'sum') or 0.0) * np.pi
        ry_angle = (self.get_blended_input('ry_angle', 'sum') or 0.0) * np.pi
        rz_angle = (self.get_blended_input('rz_angle', 'sum') or 0.0) * np.pi
        hadamard_trigger = self.get_blended_input('hadamard_trigger', 'sum') or 0.0
        
        # 2. Apply rotations (simulated via Hamiltonians from qbit.py)
        # These are applied sequentially in the simulation timestep
        self._apply_gate_hamiltonian(H_X, rx_angle)
        self._apply_gate_hamiltonian(H_Y, ry_angle)
        self._apply_gate_hamiltonian(H_Z, rz_angle)
        
        # 3. Handle Discrete Gates (Hadamard on rising edge)
        if hadamard_trigger > 0.5 and self.last_hadamard_trigger <= 0.5:
            self.state = H_Hadamard.dot(self.state)
            norm = np.linalg.norm(self.state)
            if norm > 1e-9:
                self.state /= norm

        self.last_hadamard_trigger = hadamard_trigger
        
        # 4. Update coordinates and history
        self.bloch_coords = self._get_bloch_coords(self.state)
        self.bloch_coords_history[:-1] = self.bloch_coords_history[1:]
        self.bloch_coords_history[-1] = self.bloch_coords
        

    def get_output(self, port_name):
        if port_name == 'bloch_x':
            return self.bloch_coords[0]
        elif port_name == 'bloch_y':
            return self.bloch_coords[1]
        elif port_name == 'bloch_z':
            return self.bloch_coords[2]
        elif port_name == 'prob_0':
            # Probability of measuring |0>
            return abs(self.state[0])**2
        return None
        
    def get_display_image(self):
        w, h = 96, 96
        img = np.zeros((h, w), dtype=np.uint8)
        
        # 1. Draw central cross (Bloch sphere axes visualization)
        cv2.line(img, (0, h//2), (w, h//2), 50, 1) # X-axis projection
        cv2.line(img, (w//2, 0), (w//2, h), 50, 1) # Z-axis projection
        
        # 2. Plot history trajectory
        # Scale coordinates from [-1, 1] to [4, 92] range
        scale = (w - 8) / 2
        offset = w // 2
        
        # Convert 3D Bloch coordinates to 2D screen projection (simple isometric view)
        x_proj = (self.bloch_coords_history[:, 0] - self.bloch_coords_history[:, 1] * 0.5) * scale * 0.8 + offset
        y_proj = (self.bloch_coords_history[:, 2] + self.bloch_coords_history[:, 1] * 0.5) * scale * -0.8 + offset
        
        # Draw trajectory
        for i in range(1, len(x_proj)):
            pt1 = (int(x_proj[i-1]), int(y_proj[i-1]))
            pt2 = (int(x_proj[i]), int(y_proj[i]))
            
            # Fade old points
            color = 120 + int(i / len(x_proj) * 135)
            cv2.line(img, pt1, pt2, color, 1)
            
        # 3. Plot current state (bright dot)
        cx, cy, cz = self.bloch_coords
        
        current_x_proj = int((cx - cy * 0.5) * scale * 0.8 + offset)
        current_y_proj = int((cz + cy * 0.5) * scale * -0.8 + offset)
        
        cv2.circle(img, (current_x_proj, current_y_proj), 3, 255, -1)
        
        img = np.ascontiguousarray(img)
        return QtGui.QImage(img.data, w, h, w, QtGui.QImage.Format.Format_Grayscale8)

    def get_config_options(self):
        return [
            ("Effective Ä§ (hbar)", "hbar", self.hbar, None),
        ]

=== FILE: CorticalReconstructionNode.py ===

"""
CorticalReconstructionNode - Attempts to visualize "brain images" from EEG signals.
---------------------------------------------------------------------------------
This node takes raw EEG or specific frequency band powers and projects them
onto a 2D cortical map, synthesizing a visual representation (reconstructed qualia)
based on brain-inspired principles of spatial organization and dynamic attention.

Inspired by:
- How different frequencies (alpha, theta, gamma) correspond to spatial processing
  (Lobe Emergence node).
- Dynamic scanning and gating mechanisms in perception (Theta-Gamma Scanner node).
- The idea of a holographic/fractal memory map encoding visual information.
- The "signal-centric" view where temporal dynamics are crucial for representation.

This is a speculative node for exploring the *concept* of brain-to-image
reconstruction within the Perception Lab's framework.

Place this file in the 'nodes' folder
"""

import numpy as np
import cv2
import __main__
BaseNode = __main__.BaseNode
PA_INSTANCE = getattr(__main__, "PA_INSTANCE", None)
QtGui = __main__.QtGui

try:
    from scipy.ndimage import gaussian_filter
    SCIPY_AVAILABLE = True
except ImportError:
    SCIPY_AVAILABLE = False
    print("Warning: CorticalReconstructionNode requires scipy")

class CorticalReconstructionNode(BaseNode):
    NODE_CATEGORY = "Visualization" # Or "Cognitive"
    NODE_COLOR = QtGui.QColor(100, 50, 200) # Deep Purple

    def __init__(self, output_size=128, decay_rate=0.95, alpha_influence=0.3, theta_influence=0.5, gamma_influence=0.8, noise_level=0.01):
        super().__init__()
        self.node_title = "Cortical Reconstruction"

        self.inputs = {
            'raw_eeg_signal': 'signal',   # Main EEG signal (e.g., raw_signal from EEG node)
            'alpha_power': 'signal',      # Alpha power (e.g., alpha from EEG node)
            'theta_power': 'signal',      # Theta power
            'gamma_power': 'signal',      # Gamma power
            'attention_focus': 'image',   # Optional: an image mask to bias reconstruction focus
        }

        self.outputs = {
            'reconstructed_image': 'image', # The synthesized "brain image"
            'alpha_contribution': 'image',  # Visualizing alpha's part
            'theta_contribution': 'image',  # Visualizing theta's part
            'gamma_contribution': 'image',  # Visualizing gamma's part
            'current_focus': 'image'        # Where the node is 'looking'
        }

        if not SCIPY_AVAILABLE or QtGui is None:
            self.node_title = "Cortical Reconstruction (ERROR)"
            self._error = True
            return
        self._error = False

        self.output_size = int(output_size)
        self.decay_rate = float(decay_rate)
        self.alpha_influence = float(alpha_influence) # Higher influence -> more visual output from this band
        self.theta_influence = float(theta_influence)
        self.gamma_influence = float(gamma_influence)
        self.noise_level = float(noise_level)

        # Internal 2D "mental canvas"
        self.reconstructed_image = np.zeros((self.output_size, self.output_size), dtype=np.float32)
        
        # Initialize some simple spatial filters for each band
        # These are highly speculative and can be made more complex
        self.alpha_filter = self._create_spatial_filter(self.output_size, 'smooth')
        self.theta_filter = self._create_spatial_filter(self.output_size, 'directional')
        self.gamma_filter = self._create_spatial_filter(self.output_size, 'detail')

        self.alpha_map = np.zeros_like(self.reconstructed_image)
        self.theta_map = np.zeros_like(self.reconstructed_image)
        self.gamma_map = np.zeros_like(self.reconstructed_image)
        self.current_focus_map = np.zeros_like(self.reconstructed_image)

    def _create_spatial_filter(self, size, type):
        """Creates a speculative spatial pattern for EEG band influence."""
        filter_map = np.zeros((size, size), dtype=np.float32)
        if type == 'smooth':
            filter_map = gaussian_filter(np.random.rand(size, size), sigma=size/8)
        elif type == 'directional':
            x = np.linspace(-1, 1, size)
            y = np.linspace(-1, 1, size)
            X, Y = np.meshgrid(x, y)
            angle = np.random.uniform(0, 2 * np.pi)
            filter_map = np.cos(X * np.cos(angle) * np.pi * 5 + Y * np.sin(angle) * np.pi * 5)
            filter_map = (filter_map + 1) / 2 # Normalize to 0-1
        elif type == 'detail':
            filter_map = np.random.rand(size, size)
            filter_map = cv2.Canny((filter_map * 255).astype(np.uint8), 50, 150) / 255.0 # Edge detection
        return filter_map / (filter_map.max() + 1e-9) # Normalize

    def step(self):
        if self._error: return

        # 1. Get EEG band powers (normalized roughly)
        raw_eeg = self.get_blended_input('raw_eeg_signal', 'sum') or 0.0
        alpha_power = self.get_blended_input('alpha_power', 'sum') or 0.0
        theta_power = self.get_blended_input('theta_power', 'sum') or 0.0
        gamma_power = self.get_blended_input('gamma_power', 'sum') or 0.0
        
        attention_focus_in = self.get_blended_input('attention_focus', 'mean')
        
        # Basic normalization for input signals (adjust as needed for real EEG ranges)
        alpha_power = np.clip(alpha_power, 0, 1) # Assuming 0-1 range for simplicity
        theta_power = np.clip(theta_power, 0, 1)
        gamma_power = np.clip(gamma_power, 0, 1)
        raw_eeg_norm = np.clip(raw_eeg + 0.5, 0, 1) # Roughly center 0 and scale to 0-1

        # 2. Update internal "mental canvas" based on EEG bands
        
        # Alpha: Influences smooth, global background or overall brightness
        self.alpha_map = self.alpha_filter * alpha_power * self.alpha_influence
        
        # Theta: Influences dynamic, directional elements or larger structures
        # We can make theta shift the filter dynamically based on raw_eeg
        # (This is a simplified way to model theta's role in "scanning" and memory)
        theta_shift_x = int((raw_eeg_norm - 0.5) * 10) # Raw EEG shifts the pattern
        theta_shifted_filter = np.roll(self.theta_filter, theta_shift_x, axis=1)
        self.theta_map = theta_shifted_filter * theta_power * self.theta_influence
        
        # Gamma: Influences fine details, edges, and sharp features
        self.gamma_map = self.gamma_filter * gamma_power * self.gamma_influence
        
        # Combine contributions
        current_reconstruction = (self.alpha_map + self.theta_map + self.gamma_map)
        
        # 3. Apply Attention Focus (if provided)
        if attention_focus_in is not None:
            if attention_focus_in.shape[0] != self.output_size:
                attention_focus_in = cv2.resize(attention_focus_in, (self.output_size, self.output_size))
            if attention_focus_in.ndim == 3:
                attention_focus_in = np.mean(attention_focus_in, axis=2)
            
            # Normalize attention mask
            attention_focus_in = attention_focus_in / (attention_focus_in.max() + 1e-9)
            self.current_focus_map = gaussian_filter(attention_focus_in, sigma=self.output_size / 20)
            
            # Only parts under focus are strongly reconstructed
            current_reconstruction *= (0.5 + 0.5 * self.current_focus_map) # Bias towards focused areas
        else:
            self.current_focus_map.fill(1.0) # Full attention if no input

        # Add some baseline noise for organic feel
        current_reconstruction += np.random.rand(self.output_size, self.output_size) * self.noise_level

        # Update the main reconstructed image with decay and new input
        self.reconstructed_image = self.reconstructed_image * self.decay_rate + current_reconstruction
        np.clip(self.reconstructed_image, 0, 1, out=self.reconstructed_image)
        
        # Apply a light gaussian blur for smoother "qualia"
        self.reconstructed_image = gaussian_filter(self.reconstructed_image, sigma=0.5)

    def get_output(self, port_name):
        if self._error: return None
        if port_name == 'reconstructed_image':
            return self.reconstructed_image
        elif port_name == 'alpha_contribution':
            return self.alpha_map
        elif port_name == 'theta_contribution':
            return self.theta_map
        elif port_name == 'gamma_contribution':
            return self.gamma_map
        elif port_name == 'current_focus':
            return self.current_focus_map
        return None

    def get_display_image(self):
        if self._error: return None
        
        display_w = 512
        display_h = 256
        display = np.zeros((display_h, display_w, 3), dtype=np.uint8)
        
        # Left side: Reconstructed Image
        reco_u8 = (np.clip(self.reconstructed_image, 0, 1) * 255).astype(np.uint8)
        reco_color = cv2.cvtColor(reco_u8, cv2.COLOR_GRAY2RGB)
        reco_resized = cv2.resize(reco_color, (display_h, display_h), interpolation=cv2.INTER_LINEAR)
        display[:, :display_h] = reco_resized
        
        # Right side: Band Contributions and Focus (blended for visualization)
        # Alpha: Green, Theta: Blue, Gamma: Red
        contributions_rgb = np.zeros((self.output_size, self.output_size, 3), dtype=np.float32)
        contributions_rgb[:, :, 0] = self.gamma_map # Red for Gamma (details)
        contributions_rgb[:, :, 1] = self.alpha_map # Green for Alpha (smoothness)
        contributions_rgb[:, :, 2] = self.theta_map # Blue for Theta (motion/structure)
        
        # Overlay focus map as an intensity
        focus_overlay = np.stack([self.current_focus_map]*3, axis=-1)
        contributions_rgb = (contributions_rgb * (0.5 + 0.5 * focus_overlay)) # Dim if not focused

        contr_u8 = (np.clip(contributions_rgb, 0, 1) * 255).astype(np.uint8)
        contr_resized = cv2.resize(contr_u8, (display_h, display_h), interpolation=cv2.INTER_LINEAR)
        display[:, display_w-display_h:] = contr_resized
        
        # Add dividing line
        display[:, display_h-1:display_h+1] = [255, 255, 255]
        
        # Add labels
        font = cv2.FONT_HERSHEY_SIMPLEX
        cv2.putText(display, 'RECONSTRUCTED QUALIA', (10, 20), font, 0.5, (255, 255, 255), 1, cv2.LINE_AA)
        cv2.putText(display, 'BAND CONTRIBUTIONS & FOCUS', (display_h + 10, 20), font, 0.5, (255, 255, 255), 1, cv2.LINE_AA)
        
        # Add input values for context
        alpha_val = self.get_blended_input('alpha_power', 'sum') or 0.0
        theta_val = self.get_blended_input('theta_power', 'sum') or 0.0
        gamma_val = self.get_blended_input('gamma_power', 'sum') or 0.0

        cv2.putText(display, f"ALPHA: {alpha_val:.2f}", (10, display_h - 40), font, 0.4, (0, 255, 0), 1, cv2.LINE_AA)
        cv2.putText(display, f"THETA: {theta_val:.2f}", (10, display_h - 25), font, 0.4, (255, 0, 0), 1, cv2.LINE_AA)
        cv2.putText(display, f"GAMMA: {gamma_val:.2f}", (10, display_h - 10), font, 0.4, (0, 0, 255), 1, cv2.LINE_AA) # Changed to blue for theta, red for gamma

        display = np.ascontiguousarray(display)
        return QtGui.QImage(display.data, display_w, display_h, 3*display_w, QtGui.QImage.Format.Format_RGB888)

    def get_config_options(self):
        return [
            ("Output Size", "output_size", self.output_size, None),
            ("Decay Rate", "decay_rate", self.decay_rate, None),
            ("Alpha Influence", "alpha_influence", self.alpha_influence, None),
            ("Theta Influence", "theta_influence", self.theta_influence, None),
            ("Gamma Influence", "gamma_influence", self.gamma_influence, None),
            ("Noise Level", "noise_level", self.noise_level, None),
        ]

=== FILE: EDF_EEG_loader.py ===

"""
EEG File Source Node - Loads a real .edf file and streams band power
Place this file in the 'nodes' folder
"""

import numpy as np
from PyQt6 import QtGui
import os
import sys

# Add parent directory to path to import BaseNode
# --- This is the new, correct block ---
import __main__
BaseNode = __main__.BaseNode
PA_INSTANCE = getattr(__main__, "PA_INSTANCE", None)
# ------------------------------------

try:
    import mne
    from scipy import signal
    MNE_AVAILABLE = True
except ImportError:
    MNE_AVAILABLE = False

# Define brain regions from brain_set_system.py
EEG_REGIONS = {
    "All": [],
    "Occipital": ['O1', 'O2', 'OZ', 'POZ', 'PO3', 'PO4', 'PO7', 'PO8'],
    "Temporal": ['T7', 'T8', 'TP7', 'TP8', 'FT7', 'FT8'],
    "Parietal": ['P1', 'P2', 'P3', 'P4', 'PZ', 'CP1', 'CP2'],
    "Frontal": ['FP1', 'FP2', 'FZ', 'F1', 'F2', 'F3', 'F4'],
    "Central": ['C1', 'C2', 'C3', 'C4', 'CZ', 'FC1', 'FC2']
}

class EEGFileSourceNode(BaseNode):
    NODE_CATEGORY = "Source"
    NODE_COLOR = QtGui.QColor(60, 140, 160) # A clinical blue
    
    def __init__(self, edf_file_path=""):
        super().__init__()
        self.node_title = "EEG File Source"
     
        self.outputs = {
            'delta': 'signal', 
            'theta': 'signal', 
            'alpha': 'signal', 
            'beta': 'signal', 
            'gamma': 'signal',
            # --- FIX: ADD NEW RAW SIGNAL OUTPUT ---
            'raw_signal': 'signal' 
        }
        
        self.edf_file_path = edf_file_path
        self.selected_region = "Occipital"
        self._last_path = ""
        self._last_region = ""
        
        self.raw = None
        self.fs = 100.0 # Resample to this frequency
        self.current_time = 0.0
        self.window_size = 1.0 # 1-second window
      
        self.output_powers = {band: 0.0 for band in self.outputs}
        self.output_powers['raw_signal'] = 0.0 # Initialize new output
        self.history = np.zeros(64) # For display

        if not MNE_AVAILABLE:
            self.node_title = "EEG (MNE Required!)"
            print("Error: EEGFileSourceNode requires 'mne' and 'scipy'.")
            print("Please run: pip install mne")

    def load_edf(self):
        """Loads or re-loads the EDF file based on config."""
        if not MNE_AVAILABLE or not os.path.exists(self.edf_file_path):
            self.raw = None
            self.node_title = f"EEG (File Not Found)"
            return

        try:
            raw = mne.io.read_raw_edf(self.edf_file_path, preload=True, verbose=False)
            raw.rename_channels(lambda name: name.strip().replace('.', '').upper())
            
            if self.selected_region != "All":
                region_channels = EEG_REGIONS[self.selected_region]
                available_channels = [ch for ch in region_channels if ch in raw.ch_names]
                if not available_channels:
                    print(f"Warning: No channels found for region {self.selected_region}")
                    self.raw = None
                    return
                raw.pick_channels(available_channels)
            
            raw.resample(self.fs, verbose=False)
            self.raw = raw
            self.current_time = 0.0
            self._last_path = self.edf_file_path
            self._last_region = self.selected_region
            self.node_title = f"EEG ({self.selected_region})"
            print(f"Successfully loaded EEG: {self.edf_file_path}")
           
        except Exception as e:
            self.raw = None
            self.node_title = f"EEG (Load Error)"
            print(f"Error loading EEG file {self.edf_file_path}: {e}")

    def step(self):
        # Check if config changed
        if self.edf_file_path != self._last_path or self.selected_region != self._last_region:
            self.load_edf()

        if self.raw is None:
            return # Do nothing if no data

        # Get data for the current time window
        start_sample = int(self.current_time * self.fs)
        end_sample = start_sample + int(self.window_size * self.fs)
        
        if end_sample >= self.raw.n_times:
            self.current_time = 0.0 # Loop
            start_sample = 0
            end_sample = int(self.window_size * self.fs)
            
        data, _ = self.raw[:, start_sample:end_sample]
        
        # Average across all selected channels
        if data.ndim > 1:
            data = np.mean(data, axis=0)

        if data.size == 0:
            return
            
        # --- FIX: Calculate and normalize the raw signal output ---
        # Output the *normalized* instantaneous level
        self.output_powers['raw_signal'] = np.mean(data) * 5.0 # Scale up for visibility
        # --- END FIX ---

        # Calculate band powers 
        bands = {
            'delta': (1, 4), 'theta': (4, 8), 'alpha': (8, 13), 
            'beta': (13, 30), 'gamma': (30, 45)
        }
        
        nyq = self.fs / 2.0
        
        for band, (low, high) in bands.items():
            if band in self.outputs:
                b, a = signal.butter(4, [low/nyq, high/nyq], btype='band')
                filtered = signal.filtfilt(b, a, data)
                power = np.log1p(np.mean(filtered**2))
                # Smooth the output
                self.output_powers[band] = self.output_powers[band] * 0.8 + power * 0.2
        
        # Update display history with alpha power
        self.history[:-1] = self.history[1:]
        self.history[-1] = self.output_powers['alpha'] * 0.5 # Scale for vis
        
        # Increment time
        self.current_time += (1.0 / 30.0) # Assume ~30fps step rate

    def get_output(self, port_name):
        return self.output_powers.get(port_name, 0.0)
        
    def get_display_image(self):
        w, h = 64, 64
        img = np.zeros((h, w), dtype=np.uint8)
        
        # Draw waveform (alpha history)
        vis_data = self.history
        vis_data = (vis_data - np.min(vis_data)) / (np.max(vis_data) - np.min(vis_data) + 1e-9)
        vis_data = vis_data * (h - 1)
        
        for i in range(w - 1):
            y1 = int(np.clip(vis_data[i], 0, h - 1))
            img[h - 1 - y1, i] = 255
            
        img = np.ascontiguousarray(img)
        return QtGui.QImage(img.data, w, h, w, QtGui.QImage.Format.Format_Grayscale8)

    def get_config_options(self):
        region_options = [(name, name) for name in EEG_REGIONS.keys()]
        
        return [
            ("EDF File Path", "edf_file_path", self.edf_file_path, None),
            ("Brain Region", "selected_region", self.selected_region, region_options),
        ]


=== FILE: EEGsignal_simulator.py ===

"""
EEG Simulator Node - Generates a simulated multi-channel EEG signal
Place this file in the 'nodes' folder
"""

import numpy as np
from PyQt6 import QtGui
import cv2
import sys
import os
# --- This is the new, correct block ---
import __main__
BaseNode = __main__.BaseNode
PA_INSTANCE = getattr(__main__, "PA_INSTANCE", None)
# ------------------------------------

class EEGSimulatorNode(BaseNode):
    NODE_CATEGORY = "Source"
    NODE_COLOR = QtGui.QColor(40, 120, 80) # Source Green
    
    def __init__(self, sample_rate=250.0):
        super().__init__()
        self.node_title = "EEG Simulator"
        self.outputs = {'signal': 'signal'}
        
        self.sample_rate = sample_rate
        self.time = 0.0
        
        # Inspired by MNE channel names
        self.channels = ["Fp1", "Fp2", "C3", "C4", "O1", "O2", "T7", "T8"]
        self.selected_channel = self.channels[0]
        
        # Internal state for each channel's oscillators
        self.channel_state = {}
        for ch in self.channels:
            self.channel_state[ch] = {
                'phase': np.random.rand(4) * 2 * np.pi,
                'freqs': np.array([
                    np.random.uniform(2, 4),    # Delta
                    np.random.uniform(5, 8),    # Theta
                    np.random.uniform(9, 12),   # Alpha
                    np.random.uniform(15, 25)   # Beta
                ]),
                'amps': np.array([
                    np.random.uniform(0.5, 1.0),
                    np.random.uniform(0.2, 0.5),
                    np.random.uniform(0.1, 0.8), # Alpha can be strong
                    np.random.uniform(0.05, 0.2)
                ]) * 0.2 # Scale down
            }
        
        self.output_value = 0.0
        self.history = np.zeros(64) # For display

    def step(self):
        dt = 1.0 / self.sample_rate
        self.time += dt
        
        # Get the state for the selected channel
        state = self.channel_state[self.selected_channel]
        
        # Update phases
        state['phase'] += state['freqs'] * dt * 2 * np.pi
        
        # Compute sines
        sines = np.sin(state['phase'])
        
        # Modulate alpha rhythm (make it bursty)
        alpha_mod = (np.sin(self.time * 0.2 * 2 * np.pi) + 1.0) / 2.0 # Slow modulation
        sines[2] *= alpha_mod
        
        # Sum oscillators
        signal = np.dot(sines, state['amps'])
        
        # Add noise
        noise = (np.random.rand() - 0.5) * 0.1
        self.output_value = signal + noise
        
        # Update display history
        self.history[:-1] = self.history[1:]
        self.history[-1] = self.output_value

    def get_output(self, port_name):
        if port_name == 'signal':
            return self.output_value
        return None
        
    def get_display_image(self):
        w, h = 64, 64
        img = np.zeros((h, w), dtype=np.uint8)
        
        # Normalize history from [-1, 1] to [0, h-1]
        vis_data = (self.history + 1.0) / 2.0 * (h - 1)
        
        for i in range(w - 1):
            y1 = int(np.clip(vis_data[i], 0, h - 1))
            y2 = int(np.clip(vis_data[i+1], 0, h - 1))
            # Draw line segment
            img = cv2.line(img, (i, y1), (i+1, y2), (255, 255, 255), 1)

        img = np.ascontiguousarray(img)
        return QtGui.QImage(img.data, w, h, w, QtGui.QImage.Format.Format_Grayscale8)

    def get_config_options(self):
        # Create channel options for the dropdown menu
        channel_options = [(ch, ch) for ch in self.channels]
        
        return [
            ("Channel", "selected_channel", self.selected_channel, channel_options)
        ]


=== FILE: FreqToMidiNode.py ===

"""
Frequency to MIDI Node - Converts a raw frequency signal into quantized
MIDI note number and velocity based on the 12-tone equal temperament scale.

Place this file in the 'nodes' folder
"""

import numpy as np
from PyQt6 import QtGui
import cv2
import math

import __main__
BaseNode = __main__.BaseNode
PA_INSTANCE = getattr(__main__, "PA_INSTANCE", None)
QtGui = __main__.QtGui

# Reference Frequency: A4 = 440 Hz (MIDI note 69)
A4_FREQ = 440.0
A4_MIDI = 69
# MIDI Note formula: N = 69 + 12 * log2(f / 440)

class FreqToMidiNode(BaseNode):
    NODE_CATEGORY = "Transform"
    NODE_COLOR = QtGui.QColor(150, 50, 200) # Musical Purple
    
    def __init__(self, midi_offset=0):
        super().__init__()
        self.node_title = "Freq to MIDI"
        
        self.inputs = {
            'frequency_in': 'signal',
            'amplitude_in': 'signal'
        }
        self.outputs = {
            'midi_note': 'signal',
            'velocity': 'signal'
        }
        
        self.midi_offset = int(midi_offset) # Shifts the output keyboard range
        self.output_note = 0.0
        self.output_velocity = 0.0

    def _freq_to_midi(self, frequency):
        """Converts frequency (Hz) to the nearest integer MIDI note number."""
        if frequency <= 0:
            return 0 # Off note
        
        try:
            # N = 69 + 12 * log2(f / 440)
            midi_note_float = A4_MIDI + 12 * np.log2(frequency / A4_FREQ)
            
            # Round to the nearest integer note
            midi_note = int(round(midi_note_float))
            
            # Apply offset and clamp to MIDI range [0, 127]
            return np.clip(midi_note + self.midi_offset, 0, 127)
            
        except ValueError:
            return 0

    def step(self):
        # 1. Get raw inputs
        freq_in = self.get_blended_input('frequency_in', 'sum')
        amp_in = self.get_blended_input('amplitude_in', 'sum')
        
        # 2. Process Frequency
        # Map input signal [-1, 1] to an audible range (e.g., 50 Hz to 2000 Hz)
        if freq_in is not None:
            # We assume the input signal is normalized (e.g., from SpectrumAnalyzer)
            # Map [-1, 1] to [50, 2000] Hz
            target_freq = (freq_in + 1.0) / 2.0 * 1950.0 + 50.0
            self.output_note = float(self._freq_to_midi(target_freq))
        
        # 3. Process Amplitude
        if amp_in is not None:
            # Map signal [0, 1] (or [-1, 1]) to normalized velocity [0.0, 1.0]
            # Use abs() to treat negative signals as volume
            velocity_norm = np.clip(np.abs(amp_in), 0.0, 1.0)
            self.output_velocity = float(velocity_norm)
        else:
            self.output_velocity = 0.0

    def get_output(self, port_name):
        if port_name == 'midi_note':
            # Only output the note if the velocity is above a threshold
            return self.output_note if self.output_velocity > 0.05 else 0.0
        elif port_name == 'velocity':
            return self.output_velocity
        return None
        
    def get_display_image(self):
        w, h = 96, 48
        img = np.zeros((h, w, 3), dtype=np.uint8)
        
        # Draw piano key visualization
        note = int(self.output_note)
        
        # Calculate Octave and Note Name
        note_name_map = ["C", "C#", "D", "D#", "E", "F", "F#", "G", "G#", "A", "A#", "B"]
        note_name = note_name_map[note % 12]
        octave = note // 12 - 1
        
        # Color based on velocity
        vel_norm = self.output_velocity
        color_val = int(vel_norm * 255)
        
        if vel_norm > 0.05:
            # Draw an active key (white or black key color based on sharp/flat)
            is_sharp = ('#' in note_name)
            fill_color = (255, 0, color_val) if is_sharp else (color_val, color_val, color_val) # Red/Magenta for sharps
            text_color = (0, 0, 0) if not is_sharp else (255, 255, 255)

            cv2.rectangle(img, (0, 0), (w, h), fill_color, -1)
        else:
            text_color = (100, 100, 100)

        # Draw Note Label
        label = f"{note_name}{octave}"
        cv2.putText(img, label, (w//4, h//2), cv2.FONT_HERSHEY_SIMPLEX, 0.6, text_color, 2, cv2.LINE_AA)
        
        # Draw MIDI number
        cv2.putText(img, f"MIDI: {note}", (w//4, h//2 + 18), cv2.FONT_HERSHEY_SIMPLEX, 0.4, text_color, 1, cv2.LINE_AA)
            
        img = np.ascontiguousarray(img)
        return QtGui.QImage(img.data, w, h, 3*w, QtGui.QImage.Format.Format_BGR888)

    def get_config_options(self):
        return [
            ("Keyboard Offset (semitones)", "midi_offset", self.midi_offset, None),
        ]

=== FILE: HSLpatternnode.py ===

"""
H/S/L Fractal Pattern Node - Generates a generative fractal
structure based on H (Hub), S (State), and L (Loop) inputs.

Ported from hslcity.html
"""

import numpy as np
from PyQt6 import QtGui
import cv2
import sys
import os

import __main__
BaseNode = __main__.BaseNode
PA_INSTANCE = getattr(__main__, "PA_INSTANCE", None)
QtGui = __main__.QtGui

# --- Core Simulation Classes (from hslcity.html) ---

class HSLPattern:
    def __init__(self, x, y, angle, scale, depth, patternType='h'):
        self.x = x
        self.y = y
        self.angle = angle
        self.scale = scale
        self.depth = depth
        self.patternType = patternType
        self.phase = np.random.rand() * np.pi * 2
        self.children = []
        self.age = 0
        self.time = 0.0
        
        if depth > 0:
            self.generateChildren()
            
    def generateChildren(self):
        branchAngle = 45.0 * np.pi / 180
        childScale = self.scale * 0.6
        childDepth = self.depth - 1
        
        if self.patternType == 'h': # Hubs branch into states
            for i in range(3):
                childAngle = self.angle + (i - 1) * branchAngle
                childType = ['s', 'l', 's'][i]
                self.children.append(HSLPattern(
                    self.x + np.cos(childAngle) * self.scale * 40,
                    self.y + np.sin(childAngle) * self.scale * 40,
                    childAngle, childScale, childDepth, childType
                ))
        elif self.patternType == 'l': # Loops create circular patterns
            for i in range(4):
                childAngle = self.angle + i * np.pi / 2
                childType = 'l'
                self.children.append(HSLPattern(
                    self.x + np.cos(childAngle) * self.scale * 30,
                    self.y + np.sin(childAngle) * self.scale * 30,
                    childAngle, childScale, childDepth, childType
                ))
        else: # 's' states transition
            childType = 'l' if np.random.rand() > 0.5 else 'h'
            self.children.append(HSLPattern(
                self.x + np.cos(self.angle) * self.scale * 50,
                self.y + np.sin(self.angle) * self.scale * 50,
                self.angle + (np.random.rand() - 0.5) * branchAngle,
                childScale, childDepth, childType
            ))
            
    def update(self, dt, global_time):
        self.age += dt
        self.time = global_time
        for child in self.children:
            child.update(dt, global_time)
            
    def draw(self, ctx_img, pulse_intensity):
        # Calculate pulsation
        pulse = 1.0
        if self.patternType == 'h':
            pulse = 1 + np.sin(self.time * 3 + self.phase) * pulse_intensity * 0.5
        elif self.patternType == 'l':
            pulse = 1 + np.sin(self.time + self.phase) * pulse_intensity * 0.2
        else:
            pulse = 1 + np.sin(self.time * 2 + self.phase) * pulse_intensity * 0.3
        
        # Set color (BGR)
        color = (0,0,0)
        if self.patternType == 'h': color = (100, 100, 255) # Red
        elif self.patternType == 'l': color = (100, 255, 100) # Green
        else: color = (255, 100, 100) # Blue
        
        radius = int(self.scale * 15 * pulse)
        if radius < 1: radius = 1
        
        # Draw the node
        pt = (int(self.x), int(self.y))
        cv2.circle(ctx_img, pt, radius, color, -1, cv2.LINE_AA)
        
        # Draw connections
        for child in self.children:
            child_pt = (int(child.x), int(child.y))
            cv2.line(ctx_img, pt, child_pt, (100, 100, 100), 1, cv2.LINE_AA)
            child.draw(ctx_img, pulse_intensity)

# --- The Main Node Class ---

class HSLPatternNode(BaseNode):
    NODE_CATEGORY = "Source"
    NODE_COLOR = QtGui.QColor(100, 200, 250) # Crystalline blue
    
    def __init__(self, size=128, speed=1.0, pulse=0.8, depth=4):
        super().__init__()
        self.node_title = "HSL Pattern (MTX)"
        
        self.inputs = {
            'H_in': 'signal', # Hub trigger
            'S_in': 'signal', # State trigger
            'L_in': 'signal'  # Loop trigger
        }
        self.outputs = {'image': 'image'}
        
        self.size = int(size)
        self.speed = float(speed)
        self.pulse = float(pulse)
        self.depth = int(depth)
        
        self.time = 0.0
        self.root_patterns = []
        self.output_image = np.zeros((self.size, self.size, 3), dtype=np.uint8)
        
        # Last trigger values
        self.last_h = 0.0
        self.last_s = 0.0
        self.last_l = 0.0
        
        # Initialize
        self._add_seed(self.size // 2, self.size // 2, 'h')

    def _add_seed(self, x, y, pattern_type):
        """Adds a new root pattern to the simulation."""
        new_pattern = HSLPattern(
            x, y, 
            np.random.rand() * 2 * np.pi, 
            scale=1.0, 
            depth=self.depth, 
            patternType=pattern_type
        )
        self.root_patterns.append(new_pattern)
        # Limit total patterns
        if len(self.root_patterns) > 20:
            self.root_patterns.pop(0)

    def step(self):
        # 1. Handle Inputs (check for rising edge)
        h_in = self.get_blended_input('H_in', 'sum') or 0.0
        s_in = self.get_blended_input('S_in', 'sum') or 0.0
        l_in = self.get_blended_input('L_in', 'sum') or 0.0
        
        rand_x = np.random.randint(self.size * 0.2, self.size * 0.8)
        rand_y = np.random.randint(self.size * 0.2, self.size * 0.8)
        
        if h_in > 0.5 and self.last_h <= 0.5: self._add_seed(rand_x, rand_y, 'h')
        if s_in > 0.5 and self.last_s <= 0.5: self._add_seed(rand_x, rand_y, 's')
        if l_in > 0.5 and self.last_l <= 0.5: self._add_seed(rand_x, rand_y, 'l')
        
        self.last_h, self.last_s, self.last_l = h_in, s_in, l_in
        
        # 2. Update time and simulation
        self.time += self.speed * 0.02
        
        # 3. Draw
        # Fade the background
        self.output_image = (self.output_image * 0.9).astype(np.uint8)
        
        for pattern in self.root_patterns:
            pattern.update(0.016, self.time)
            pattern.draw(self.output_image, self.pulse)

    def get_output(self, port_name):
        if port_name == 'image':
            return self.output_image.astype(np.float32) / 255.0
        return None
        
    def get_display_image(self):
        img_rgb = np.ascontiguousarray(self.output_image)
        h, w = img_rgb.shape[:2]
        return QtGui.QImage(img_rgb.data, w, h, 3*w, QtGui.QImage.Format.Format_BGR888)

    def get_config_options(self):
        return [
            ("Resolution", "size", self.size, None),
            ("Speed", "speed", self.speed, None),
            ("Pulsation", "pulse", self.pulse, None),
            ("Recursion Depth", "depth", self.depth, None),
        ]

=== FILE: MTXneuronnode.py ===

"""
MTX Neuron Node - A realistic spiking neuron with H-S-L token emission.
Combines Izhikevich spiking, synaptic dynamics, and dendritic plateaus.
Outputs H/S/L tokens as signal pulses.

Ported from mtxneuron.py
Requires: pip install numpy
"""

import numpy as np
from PyQt6 import QtGui
import cv2
from collections import deque
import sys
import os

import __main__
BaseNode = __main__.BaseNode
PA_INSTANCE = getattr(__main__, "PA_INSTANCE", None)
QtGui = __main__.QtGui

rng = np.random.default_rng(42)

# --- Core Simulation Classes (from mtxneuron.py) ---

class MtxPort:
    def __init__(self, win_ms=300.0, step_ms=0.1):
        self.win_ms = float(win_ms)
        self.step_ms = float(step_ms)
        self.spike_times = deque(maxlen=4000)
        self.voltage_buf = deque(maxlen=int(win_ms/step_ms))
        self.prev_plateau = False
        self.persist_l = 0

    def update(self, voltage, spike, plateau_active, t_ms):
        self.voltage_buf.append(voltage)
        if spike:
            self.spike_times.append(t_ms)

        if len(self.voltage_buf) < 20:
            return None, 0.0, 0.0

        W = self.win_ms
        recent = [s for s in self.spike_times if t_ms - s <= W]
        rate_hz = len(recent) / (W/1000.0)

        if len(recent) >= 4 and np.mean(np.diff(recent)) > 0:
            isis = np.diff(recent)
            cv = np.std(isis) / np.mean(isis)
            coherence = float(np.clip(1.0 - cv, 0.0, 1.0))
        else:
            coherence = 0.0

        v = np.array(self.voltage_buf)
        dv = np.abs(np.diff(v[-20:])).mean()
        novelty = float(np.clip(dv/20.0 + (rate_hz/50.0), 0.0, 1.0))

        token = None
        burst = len(recent) >= 3 and (recent[-1] - recent[-3]) <= 50.0
        plateau_onset = plateau_active and not self.prev_plateau
        if burst or plateau_onset:
            token = 'h'
            self.persist_l = 0
        elif 5.0 <= rate_hz <= 25.0 and coherence > 0.5:
            self.persist_l += 1
            if self.persist_l * self.step_ms >= 200.0:
                token = 'l'
        else:
            self.persist_l = 0

        if token is None and (novelty > 0.25 or spike):
            token = 's'

        self.prev_plateau = plateau_active
        return token, novelty, coherence

class Synapse:
    def __init__(self, syn_type='AMPA', weight=1.0):
        self.type = syn_type
        self.weight = weight
        self.g = 0.0
        self.x = 1.0
        self.u = 0.3 if syn_type == 'AMPA' else 0.1
        if syn_type == 'AMPA': self.tau, self.E_rev = 2.0, 0.0
        elif syn_type == 'NMDA': self.tau, self.E_rev = 50.0, 0.0
        elif syn_type == 'GABAA': self.tau, self.E_rev = 10.0, -70.0
        elif syn_type == 'GABAB': self.tau, self.E_rev = 100.0, -90.0

    def update(self, dt, voltage=0.0):
        self.g *= np.exp(-dt / self.tau)
        if self.type == 'NMDA':
            mg_block = 1.0 / (1.0 + 0.28 * np.exp(-0.062 * voltage))
            return self.g * mg_block
        return self.g

    def receive_spike(self):
        release = self.u * self.x
        self.x = min(1.0, self.x - release + 0.02)
        self.g += self.weight * release

class Dendrite:
    def __init__(self):
        self.voltage = -65.0
        self.calcium = 0.0
        self.plateau_active = False
        self.synapses = []

    def add_synapse(self, syn): self.synapses.append(syn)
    def update(self, dt, soma_v):
        total_I, nmda_I = 0.0, 0.0
        for syn in self.synapses:
            g = syn.update(dt, self.voltage)
            I = g * (syn.E_rev - self.voltage)
            total_I += I
            if syn.type == 'NMDA': nmda_I += I
        self.voltage += dt * (-(self.voltage - soma_v) / 10.0 + total_I / 50.0)
        ca_influx = max(0.0, nmda_I * 0.1)
        self.calcium += dt * (ca_influx - self.calcium / 20.0)
        self.plateau_active = (self.calcium > 0.25 and self.voltage > -55.0)
        return self.plateau_active

class BioNeuron:
    def __init__(self, step_ms=0.1):
        self.a, self.b, self.c, self.d = 0.02, 0.2, -65.0, 8.0
        self.v, self.u = -65.0, self.b * -65.0
        self.spike = False
        self.m_current = 0.0
        self.adaptation = 0.0
        self.atp = 1.0
        self.ampa = [Synapse('AMPA', 0.5) for _ in range(10)]
        self.nmda = [Synapse('NMDA', 0.3) for _ in range(5)]
        self.gabaa = [Synapse('GABAA', 0.7) for _ in range(3)]
        self.gabab = [Synapse('GABAB', 0.4) for _ in range(2)]
        self.dend = Dendrite()
        for s in self.nmda: self.dend.add_synapse(s)
        self.pre, self.post = 0.0, 0.0
        self.DA, self.ACh, self.NE = 0.5, 0.3, 0.2
        self.mtx = MtxPort(win_ms=300.0, step_ms=step_ms)
        self.v_history = deque(maxlen=128) # For display

    def receive_input(self, typ='AMPA'):
        syn_list = {'AMPA': self.ampa, 'NMDA': self.nmda, 'GABAA': self.gabaa, 'GABAB': self.gabab}.get(typ)
        if syn_list: rng.choice(syn_list).receive_spike()

    def _neuromods(self, novelty, coherence):
        if hasattr(self, "_last_nov"):
            if self._last_nov > 0.5 and novelty < 0.3: self.DA = min(1.0, self.DA + 0.05)
            else: self.DA *= 0.99
        self._last_nov = novelty
        self.ACh = 0.8 * (1 - coherence) + 0.2 * self.ACh
        self.NE = 0.7 * novelty + 0.3 * self.NE

    def _stdp(self, dt):
        self.pre *= np.exp(-dt/20.0); self.post *= np.exp(-dt/20.0)
        if self.spike:
            self.post += 1.0
            if self.DA > 0.4:
                for syn in (self.ampa + self.nmda):
                    if syn.x < 0.8: syn.weight = min(2.0, syn.weight + 0.001 * self.pre * self.DA)

    def step(self, dt, t_ms, ext_I=0.0):
        plateau = self.dend.update(dt, self.v)
        I_syn = 0.0
        for s in self.ampa: I_syn += s.update(dt, self.v) * (s.E_rev - self.v)
        nmda_I = 0.0
        for s in self.nmda:
            g = s.update(dt, self.v); I = g * (s.E_rev - self.v)
            I_syn += 0.3 * I; nmda_I += I
        for s in self.gabaa + self.gabab: I_syn += s.update(dt, self.v) * (s.E_rev - self.v)
        
        self.m_current += dt * ((self.v + 35.0)/10.0 - self.m_current) / 100.0
        I_adapt = -5.0 * self.m_current
        noise_gain = 1.0 + 2.0 * self.ACh; gain = 1.0 + 1.5 * self.NE
        I_total = I_syn + I_adapt + ext_I * gain + rng.normal(0.0, 2.0*noise_gain)

        if abs(I_total) > 10: self.atp -= 0.001
        self.atp = min(1.0, self.atp + 0.0005)
        if self.atp < 0.5: I_total *= 0.7

        self.spike = False
        if self.v >= 30.0:
            self.spike = True; self.v = self.c; self.u += self.d; self.adaptation += 0.2
        else:
            dv = 0.04*self.v**2 + 5*self.v + 140 - self.u + I_total
            du = self.a*(self.b*self.v - self.u)
            self.v += dt * dv; self.u += dt * du
        
        self.adaptation *= np.exp(-dt/50.0)
        self.v -= 2.0 * self.adaptation
        self.v_history.append(self.v)
        
        token, novelty, coherence = self.mtx.update(self.v, self.spike, plateau, t_ms)
        self._neuromods(novelty, coherence)
        self._stdp(dt)
        return token, novelty, coherence, plateau

# --- The Main Node Class ---

class MTXNeuronNode(BaseNode):
    NODE_CATEGORY = "Source"
    NODE_COLOR = QtGui.QColor(220, 120, 40) # Neural orange
    
    def __init__(self, step_ms=1.0, steps_per_frame=10):
        super().__init__()
        self.node_title = "BioNeuron (MTX)"
        
        # H=Hub/Burst, S=State/Novelty, L=Loop/Rhythm
        self.outputs = {
            'H_out': 'signal',
            'S_out': 'signal',
            'L_out': 'signal',
            'voltage': 'signal',
            'novelty': 'signal',
            'coherence': 'signal'
        }
        
        self.dt = float(step_ms)
        self.steps_per_frame = int(steps_per_frame)
        self.neuron = BioNeuron(step_ms=self.dt)
        self.time_ms = 0.0
        
        # Internal state for pulses
        self.h_pulse = 0.0
        self.s_pulse = 0.0
        self.l_pulse = 0.0
        self.novelty = 0.0
        self.coherence = 0.0

    def step(self):
        # Reset pulses
        self.h_pulse, self.s_pulse, self.l_pulse = 0.0, 0.0, 0.0
        
        for _ in range(self.steps_per_frame):
            self.time_ms += self.dt
            
            # --- Internal Stimulation (from mtxneuron.py) ---
            ext_I = 0.0
            if rng.random() < 0.05: self.neuron.receive_input('AMPA')
            if rng.random() < 0.02: self.neuron.receive_input('NMDA')
            if rng.random() < 0.03: self.neuron.receive_input('GABAA')
            if rng.random() < 0.002: # Plateau trigger
                for _ in range(6): self.neuron.receive_input('NMDA')
            # ------------------------------------------------
            
            token, nov, coh, plat = self.neuron.step(self.dt, self.time_ms, ext_I)
            
            if token == 'h': self.h_pulse = 1.0
            if token == 's': self.s_pulse = 1.0
            if token == 'l': self.l_pulse = 1.0
            
            self.novelty = nov
            self.coherence = coh

    def get_output(self, port_name):
        if port_name == 'H_out': return self.h_pulse
        if port_name == 'S_out': return self.s_pulse
        if port_name == 'L_out': return self.l_pulse
        if port_name == 'voltage': return (self.neuron.v + 65.0) / 95.0 # Normalize
        if port_name == 'novelty': return self.novelty
        if port_name == 'coherence': return self.coherence
        return None
        
    def get_display_image(self):
        w, h = 128, 64
        img = np.zeros((h, w, 3), dtype=np.uint8)
        
        # Draw voltage trace
        v_hist = np.array(list(self.neuron.v_history))
        if len(v_hist) > 1:
            v_norm = (v_hist - v_hist.min()) / (v_hist.max() - v_hist.min() + 1e-9)
            v_scaled = (v_norm * (h - 10) + 5).astype(int)
            
            for i in range(len(v_scaled) - 1):
                x1 = int(i / len(v_scaled) * w)
                x2 = int((i + 1) / len(v_scaled) * w)
                y1 = h - v_scaled[i]
                y2 = h - v_scaled[i+1]
                cv2.line(img, (x1, y1), (x2, y2), (255, 255, 255), 1)
        
        # Draw token indicators
        if self.h_pulse: cv2.circle(img, (w-10, 10), 5, (0, 0, 255), -1) # H = Red
        if self.s_pulse: cv2.circle(img, (w-10, 25), 5, (0, 255, 0), -1) # S = Green
        if self.l_pulse: cv2.circle(img, (w-10, 40), 5, (255, 0, 0), -1) # L = Blue
            
        img = np.ascontiguousarray(img)
        return QtGui.QImage(img.data, w, h, 3*w, QtGui.QImage.Format.Format_BGR888)

    def get_config_options(self):
        return [
            ("Time Step (ms)", "dt", self.dt, None),
            ("Steps / Frame", "steps_per_frame", self.steps_per_frame, None),
        ]

=== FILE: MidiToFreq.py ===

"""
MIDI to Frequency Node - Converts a standard MIDI note number and velocity
into a usable frequency (Hz) and amplitude signal.

Place this file in the 'nodes' folder
"""

import numpy as np
from PyQt6 import QtGui
import cv2
import math

import __main__
BaseNode = __main__.BaseNode
PA_INSTANCE = getattr(__main__, "PA_INSTANCE", None)
QtGui = __main__.QtGui

# Reference Frequency: A4 = 440 Hz (MIDI note 69)
A4_FREQ = 440.0
A4_MIDI = 69
# MIDI Note formula: f = 440 * 2^((N - 69)/12)

class MidiToFreqNode(BaseNode):
    NODE_CATEGORY = "Transform"
    NODE_COLOR = QtGui.QColor(150, 50, 200) # Musical Purple
    
    def __init__(self):
        super().__init__()
        self.node_title = "MIDI to Freq (Hz)"
        
        self.inputs = {
            'midi_note_in': 'signal',   # MIDI note number (0-127)
            'velocity_in': 'signal'     # MIDI velocity (0.0 to 1.0)
        }
        self.outputs = {
            'frequency_out': 'signal',
            'amplitude_out': 'signal'
        }
        
        self.output_freq = 0.0
        self.output_amp = 0.0
        self.current_note = 0

        self.midi_offset = 0

    def _midi_to_freq(self, midi_note):
        """Converts integer MIDI note number to frequency (Hz)."""
        if midi_note <= 0:
            return 0.0
        
        # Clamp to reasonable range for calculation
        midi_note = np.clip(midi_note, 0, 127)
        
        # f = 440 * 2^((N - 69)/12)
        exponent = (midi_note - A4_MIDI) / 12.0
        return float(A4_FREQ * math.pow(2, exponent))

    def _get_note_name(self, midi_note):
        """Helper to get note name and octave for display."""
        note_name_map = ["C", "C#", "D", "D#", "E", "F", "F#", "G", "G#", "A", "A#", "B"]
        note = int(midi_note)
        note_name = note_name_map[note % 12]
        octave = note // 12 - 1
        return f"{note_name}{octave}"

    def step(self):
        # 1. Get raw inputs
        note_in = self.get_blended_input('midi_note_in', 'sum') or 0.0
        amp_in = self.get_blended_input('velocity_in', 'sum') or 0.0
        
        # 2. Quantize Note Input
        # Note numbers are integers; anything less than 0.5 is treated as 'off'
        if amp_in > 0.05 and note_in >= 0:
            self.current_note = int(round(note_in))
        else:
            self.current_note = 0
        
        # 3. Calculate Frequency
        self.output_freq = self._midi_to_freq(self.current_note)
        
        # 4. Calculate Amplitude
        # Amp is just the velocity signal, clamped and smoothed
        self.output_amp = np.clip(amp_in, 0.0, 1.0)

    def get_output(self, port_name):
        if port_name == 'frequency_out':
            # Output frequency only if amplitude is high enough
            return self.output_freq if self.output_amp > 0.05 else 0.0
        elif port_name == 'amplitude_out':
            return self.output_amp
        return None
        
    def get_display_image(self):
        w, h = 96, 48
        img = np.zeros((h, w, 3), dtype=np.uint8)
        
        note = self.current_note
        freq = self.output_freq
        amp = self.output_amp
        
        # Color based on activity
        if freq > 0.0:
            fill_color = (0, 150, 255) # Active Cyan
            text_color = (0, 0, 0)
            note_label = self._get_note_name(note)
        else:
            fill_color = (50, 50, 50)
            text_color = (150, 150, 150)
            note_label = "OFF"
        
        cv2.rectangle(img, (0, 0), (w, h), fill_color, -1)

        # Draw Note Label
        cv2.putText(img, note_label, (w//4, h//3), cv2.FONT_HERSHEY_SIMPLEX, 0.6, text_color, 2, cv2.LINE_AA)
        
        # Draw Frequency
        cv2.putText(img, f"{freq:.1f} Hz", (w//4, h//3 + 18), cv2.FONT_HERSHEY_SIMPLEX, 0.4, text_color, 1, cv2.LINE_AA)
        
        # Draw Amplitude Bar
        bar_w = int(amp * (w - 10))
        cv2.rectangle(img, (5, h - 10), (5 + bar_w, h - 5), (255, 255, 255), -1)
            
        img = np.ascontiguousarray(img)
        return QtGui.QImage(img.data, w, h, 3*w, QtGui.QImage.Format.Format_BGR888)

    def get_config_options(self):
        return [
            ("MIDI Note Offset", "midi_offset", self.midi_offset, None),
        ]

=== FILE: MoireInterferenceNode.py ===

"""
MoirÃ© Interference Node - Generates a 2D moirÃ© pattern by interfering
two perpendicular sine waves. The frequencies of the waves are
controlled by the signal inputs.

Ported from moire_microscope.html
"""

import numpy as np
from PyQt6 import QtGui
import cv2

import __main__
BaseNode = __main__.BaseNode
PA_INSTANCE = getattr(__main__, "PA_INSTANCE", None)
QtGui = __main__.QtGui

class MoireInterferenceNode(BaseNode):
    NODE_CATEGORY = "Transform"
    NODE_COLOR = QtGui.QColor(100, 180, 180) # MoirÃ© Teal
    
    def __init__(self, size=128, base_phase_1=0.0, base_phase_2=0.0):
        super().__init__()
        self.node_title = "MoirÃ© Interference"
        self.size = int(size)
        self.base_phase_1 = float(base_phase_1)
        self.base_phase_2 = float(base_phase_2)
        
        self.inputs = {
            'freq_1': 'signal', # Controls frequency of horizontal wave
            'freq_2': 'signal'  # Controls frequency of vertical wave
        }
        self.outputs = {'image': 'image'}
        
        # Pre-calculate coordinate grids
        self._init_grids()
        self.output_image = np.zeros((self.size, self.size), dtype=np.float32)

    def _init_grids(self):
        """Creates normalized coordinate grids [0, 1]"""
        if self.size == 0: self.size = 1 # Avoid division by zero
        u_vec = np.linspace(0, 1, self.size, dtype=np.float32)
        v_vec = np.linspace(0, 1, self.size, dtype=np.float32)
        # V (rows, 0->1), U (cols, 0->1)
        self.U, self.V = np.meshgrid(u_vec, v_vec) 
        self.output_image = np.zeros((self.size, self.size), dtype=np.float32)

    def step(self):
        # Check if size changed from config
        if self.U.shape[0] != self.size:
            self._init_grids()
            
        # 1. Get frequency inputs
        # We map the input signal (range -1 to 1) to a k-value (frequency)
        # e.g., mapping to a range of [5, 45]
        k1 = ((self.get_blended_input('freq_1', 'sum') or 0.0) + 1.0) * 20.0 + 5.0
        k2 = ((self.get_blended_input('freq_2', 'sum') or 0.0) + 1.0) * 20.0 + 5.0
        
        # 2. Port the core math from moire_microscope.html
        # const field1 = Math.sin(u * 20 * Math.PI + phase1);
        # const field2 = Math.cos(v * 20 * Math.PI + phase2);
        # const moireValue = Math.cos(field1 * Math.PI - field2 * Math.PI);
        
        # We use U (horizontal grid) for field 1 and V (vertical grid) for field 2
        field1 = np.sin(self.U * k1 * np.pi + self.base_phase_1)
        field2 = np.cos(self.V * k2 * np.pi + self.base_phase_2)
        
        # The interference pattern
        moire_value = np.cos(field1 * np.pi - field2 * np.pi)
        
        # 3. Normalize [-1, 1] to [0, 1] for image output
        self.output_image = (moire_value + 1.0) / 2.0

    def get_output(self, port_name):
        if port_name == 'image':
            return self.output_image
        return None
        
    def get_display_image(self):
        img_u8 = (np.clip(self.output_image, 0, 1) * 255).astype(np.uint8)
        img_u8 = np.ascontiguousarray(img_u8)
        return QtGui.QImage(img_u8.data, self.size, self.size, self.size, QtGui.QImage.Format.Format_Grayscale8)

    def get_config_options(self):
        return [
            ("Resolution", "size", self.size, None),
            ("Base Phase 1", "base_phase_1", self.base_phase_1, None),
            ("Base Phase 2", "base_phase_2", self.base_phase_2, None),
        ]

=== FILE: PCAautoexplorernode.py ===

"""
Auto-Explorer Node - Automatically animates through PC space
Creates smooth explorations of the learned manifold
"""

import numpy as np
from PyQt6 import QtGui
import cv2

import __main__
BaseNode = __main__.BaseNode
QtGui = __main__.QtGui

class AutoExplorerNode(BaseNode):
    """
    Automatically explores PCA latent space with smooth animations.
    Multiple modes: sequential, random walk, circular, spiral
    """
    NODE_CATEGORY = "AI / Physics"
    NODE_COLOR = QtGui.QColor(100, 220, 180)
    
    def __init__(self, mode='sequential'):
        super().__init__()
        self.node_title = "Auto-Explorer"
        
        self.inputs = {
            'latent_in': 'spectrum',
            'speed': 'signal',
            'amplitude': 'signal',
            'chaos': 'signal'  # Randomness amount
        }
        self.outputs = {
            'latent_out': 'spectrum',
            'current_pc': 'signal',
            'phase': 'signal'  # 0-1 oscillation
        }
        
        self.mode = mode  # 'sequential', 'random_walk', 'circular', 'spiral'
        
        # State
        self.base_latent = None
        self.current_latent = None
        self.phase = 0.0
        self.current_pc = 0
        self.random_state = np.random.randn(8)  # For random walk
        
    def step(self):
        latent_in = self.get_blended_input('latent_in', 'first')
        speed = self.get_blended_input('speed', 'sum') or 0.05
        amplitude = self.get_blended_input('amplitude', 'sum') or 2.0
        chaos = self.get_blended_input('chaos', 'sum') or 0.0
        
        if latent_in is not None:
            if self.base_latent is None:
                self.base_latent = latent_in.copy()
            self.current_latent = self.base_latent.copy()
            
            # Advance phase
            self.phase += speed
            
            if self.mode == 'sequential':
                self._sequential_mode(amplitude)
            elif self.mode == 'random_walk':
                self._random_walk_mode(amplitude, chaos)
            elif self.mode == 'circular':
                self._circular_mode(amplitude)
            elif self.mode == 'spiral':
                self._spiral_mode(amplitude)
                
    def _sequential_mode(self, amplitude):
        """Oscillate through PCs one at a time"""
        latent_dim = len(self.base_latent)
        
        # Current PC index (cycles through all)
        self.current_pc = int(self.phase / (2*np.pi)) % latent_dim
        
        # Oscillate that PC
        modulation = np.sin(self.phase) * amplitude
        self.current_latent[self.current_pc] += modulation
        
    def _random_walk_mode(self, amplitude, chaos):
        """Brownian motion in latent space"""
        latent_dim = len(self.base_latent)
        
        # Update random state
        self.random_state += np.random.randn(latent_dim) * chaos * 0.1
        
        # Apply damping
        self.random_state *= 0.98
        
        # Add to latent
        for i in range(min(latent_dim, len(self.random_state))):
            self.current_latent[i] += self.random_state[i] * amplitude
            
    def _circular_mode(self, amplitude):
        """Rotate in PC0-PC1 plane"""
        if len(self.base_latent) >= 2:
            self.current_latent[0] += np.cos(self.phase) * amplitude
            self.current_latent[1] += np.sin(self.phase) * amplitude
            self.current_pc = 0  # Indicate using PC0-PC1
            
    def _spiral_mode(self, amplitude):
        """Spiral outward in PC0-PC1 plane while oscillating PC2"""
        if len(self.base_latent) >= 3:
            # Expanding spiral
            radius = (self.phase / (2*np.pi)) % 5.0  # Expand over 5 cycles
            
            self.current_latent[0] += np.cos(self.phase) * radius * amplitude * 0.3
            self.current_latent[1] += np.sin(self.phase) * radius * amplitude * 0.3
            self.current_latent[2] += np.sin(self.phase * 2) * amplitude * 0.5
            
            self.current_pc = 2  # Indicate complex motion
            
    def get_output(self, port_name):
        if port_name == 'latent_out':
            return self.current_latent
        elif port_name == 'current_pc':
            return float(self.current_pc)
        elif port_name == 'phase':
            return (self.phase % (2*np.pi)) / (2*np.pi)  # Normalized 0-1
        return None
        
    def get_display_image(self):
        """Show current exploration trajectory"""
        img = np.zeros((256, 256, 3), dtype=np.uint8)
        
        if self.current_latent is None:
            cv2.putText(img, "Waiting for input...", (10, 128),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255,255,255), 1)
            return QtGui.QImage(img.data, 256, 256, 256*3, QtGui.QImage.Format.Format_RGB888)
            
        # Draw mode and state
        mode_text = f"Mode: {self.mode}"
        pc_text = f"PC: {self.current_pc}"
        phase_text = f"Phase: {self.phase:.2f}"
        
        cv2.putText(img, mode_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255,255,255), 1)
        cv2.putText(img, pc_text, (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,255,255), 1)
        cv2.putText(img, phase_text, (10, 90), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (200,200,200), 1)
        
        # Visualize current latent code as bars
        latent_dim = len(self.current_latent)
        bar_width = max(1, 256 // latent_dim)
        
        delta = self.current_latent - self.base_latent
        delta_max = np.abs(delta).max()
        if delta_max > 1e-6:
            delta_norm = delta / delta_max
        else:
            delta_norm = delta
            
        for i, val in enumerate(delta_norm):
            x = i * bar_width
            h = int(abs(val) * 80)
            y_base = 200
            
            if val >= 0:
                color = (0, 255, 0)
                y_start = y_base - h
                y_end = y_base
            else:
                color = (0, 0, 255)
                y_start = y_base
                y_end = y_base + h
                
            # Highlight current PC
            if i == self.current_pc:
                color = (255, 255, 0)
                
            cv2.rectangle(img, (x, y_start), (x+bar_width-1, y_end), color, -1)
            
        # Draw baseline
        cv2.line(img, (0, 200), (256, 200), (100, 100, 100), 1)
        
        return QtGui.QImage(img.data, 256, 256, 256*3, QtGui.QImage.Format.Format_RGB888)
        
    def get_config_options(self):
        return [
            ("Mode", "mode", self.mode, None)
        ]

=== FILE: PCAscannernode.py ===

"""
PC Scanner Node - Automatically scans through all principal components
Creates a contact sheet showing what each PC controls
"""

import numpy as np
from PyQt6 import QtGui
import cv2

import __main__
BaseNode = __main__.BaseNode
QtGui = __main__.QtGui

class PCScannerNode(BaseNode):
    """
    Systematically scans through all PCs to visualize their effects.
    Creates a grid showing: [PC0-, PC0+, PC1-, PC1+, ...]
    """
    NODE_CATEGORY = "AI / Physics"
    NODE_COLOR = QtGui.QColor(220, 180, 100)
    
    def __init__(self, scan_amplitude=2.0, grid_cols=4):
        super().__init__()
        self.node_title = "PC Scanner"
        
        self.inputs = {
            'latent_in': 'spectrum',
            'reconstructed_image': 'image',  # Feedback from iFFT
            'scan_speed': 'signal',  # How fast to scan
            'trigger': 'signal',  # Start scan
            'amplitude': 'signal'  # How much to modify each PC
        }
        self.outputs = {
            'latent_out': 'spectrum',  # Modified latent for current scan
            'contact_sheet': 'image',  # The full grid
            'current_pc': 'signal',  # Which PC we're scanning
            'progress': 'signal'  # 0-1 scan progress
        }
        
        self.scan_amplitude = float(scan_amplitude)
        self.grid_cols = int(grid_cols)
        
        # Scanning state
        self.is_scanning = False
        self.scan_index = 0  # Which PC we're currently scanning
        self.scan_direction = 1  # 1 for +, -1 for -
        self.frame_counter = 0
        self.frames_per_scan = 30  # How many frames to wait per PC
        
        # Storage
        self.base_latent = None
        self.current_latent = None
        self.captured_images = {}  # {(pc_idx, direction): image}
        self.contact_sheet = None
        
        # Dimensions
        self.cell_size = 64
        
    def step(self):
        # Get inputs
        latent_in = self.get_blended_input('latent_in', 'first')
        reconstructed = self.get_blended_input('reconstructed_image', 'mean')
        trigger = self.get_blended_input('trigger', 'sum') or 0.0
        scan_speed = self.get_blended_input('scan_speed', 'sum')
        amplitude_signal = self.get_blended_input('amplitude', 'sum')
        
        if amplitude_signal is not None:
            amplitude = amplitude_signal * 5.0
        else:
            amplitude = self.scan_amplitude
            
        if scan_speed is not None:
            self.frames_per_scan = max(5, int(30 / (scan_speed + 0.1)))
        
        # Store base latent
        if latent_in is not None and self.base_latent is None:
            self.base_latent = latent_in.copy()
            self.current_latent = latent_in.copy()
            
        # Trigger scan
        if trigger > 0.5 and not self.is_scanning:
            self.start_scan()
            
        # Scanning logic
        if self.is_scanning and self.base_latent is not None:
            self.frame_counter += 1
            
            # Capture reconstructed image
            if reconstructed is not None and self.frame_counter > 5:  # Wait a few frames for stabilization
                key = (self.scan_index, self.scan_direction)
                if key not in self.captured_images:
                    # Resize and store
                    img_resized = cv2.resize(reconstructed, (self.cell_size, self.cell_size))
                    self.captured_images[key] = img_resized.copy()
                    
            # Time to move to next scan?
            if self.frame_counter >= self.frames_per_scan:
                self.advance_scan()
                
            # Generate current modified latent
            self.current_latent = self.base_latent.copy()
            if self.scan_index < len(self.base_latent):
                self.current_latent[self.scan_index] += amplitude * self.scan_direction
                
        # Build contact sheet
        if len(self.captured_images) > 0:
            self.build_contact_sheet()
            
    def start_scan(self):
        """Start a new scan"""
        self.is_scanning = True
        self.scan_index = 0
        self.scan_direction = -1  # Start with negative
        self.frame_counter = 0
        self.captured_images = {}
        print("PC Scanner: Starting scan...")
        
    def advance_scan(self):
        """Move to next PC/direction"""
        self.frame_counter = 0
        
        if self.scan_direction == -1:
            # Switch to positive
            self.scan_direction = 1
        else:
            # Move to next PC
            self.scan_direction = -1
            self.scan_index += 1
            
            # Check if scan complete
            if self.scan_index >= len(self.base_latent):
                self.is_scanning = False
                self.current_latent = self.base_latent.copy()
                print(f"PC Scanner: Scan complete! Captured {len(self.captured_images)} images.")
                
    def build_contact_sheet(self):
        """Build the grid visualization"""
        num_pcs = len(self.base_latent) if self.base_latent is not None else 8
        
        # Calculate grid dimensions
        # Each PC gets 2 cells (- and +)
        total_cells = num_pcs * 2
        rows = (total_cells + self.grid_cols - 1) // self.grid_cols
        
        # Create canvas
        canvas_width = self.grid_cols * self.cell_size
        canvas_height = rows * self.cell_size
        canvas = np.zeros((canvas_height, canvas_width, 3), dtype=np.float32)
        
        # Fill grid
        cell_idx = 0
        for pc_idx in range(num_pcs):
            for direction in [-1, 1]:
                key = (pc_idx, direction)
                
                row = cell_idx // self.grid_cols
                col = cell_idx % self.grid_cols
                
                y_start = row * self.cell_size
                x_start = col * self.cell_size
                
                if key in self.captured_images:
                    img = self.captured_images[key]
                    
                    # Ensure correct shape
                    if img.ndim == 2:
                        img = np.stack([img, img, img], axis=-1)
                    elif img.shape[2] == 1:
                        img = np.repeat(img, 3, axis=2)
                        
                    canvas[y_start:y_start+self.cell_size, 
                           x_start:x_start+self.cell_size] = img
                else:
                    # Empty cell - draw placeholder
                    cv2.rectangle(canvas, (x_start, y_start), 
                                (x_start+self.cell_size-1, y_start+self.cell_size-1),
                                (0.2, 0.2, 0.2), 1)
                
                # Label
                label = f"PC{pc_idx}" + ("-" if direction == -1 else "+")
                cv2.putText(canvas, label, (x_start+2, y_start+12),
                           cv2.FONT_HERSHEY_SIMPLEX, 0.3, (1, 1, 1), 1)
                
                # Highlight current scan position
                if self.is_scanning and pc_idx == self.scan_index and direction == self.scan_direction:
                    cv2.rectangle(canvas, (x_start, y_start),
                                (x_start+self.cell_size-1, y_start+self.cell_size-1),
                                (0, 1, 0), 2)
                
                cell_idx += 1
                
        self.contact_sheet = canvas
        
    def get_output(self, port_name):
        if port_name == 'latent_out':
            return self.current_latent
        elif port_name == 'contact_sheet':
            return self.contact_sheet
        elif port_name == 'current_pc':
            return float(self.scan_index) if self.is_scanning else -1.0
        elif port_name == 'progress':
            if self.base_latent is not None and self.is_scanning:
                total = len(self.base_latent) * 2
                current = self.scan_index * 2 + (0 if self.scan_direction == -1 else 1)
                return current / total
            return 0.0
        return None
        
    def get_display_image(self):
        if self.contact_sheet is not None:
            # Display the contact sheet
            img = (np.clip(self.contact_sheet, 0, 1) * 255).astype(np.uint8)
            h, w = img.shape[:2]
            return QtGui.QImage(img.data, w, h, w*3, QtGui.QImage.Format.Format_RGB888)
        else:
            # Show status
            img = np.zeros((256, 256, 3), dtype=np.uint8)
            if self.is_scanning:
                status = f"Scanning PC{self.scan_index}{'-' if self.scan_direction == -1 else '+'}"
                progress = int(self.get_output('progress') * 100)
                cv2.putText(img, status, (10, 128), cv2.FONT_HERSHEY_SIMPLEX, 
                           0.6, (0,255,0), 2)
                cv2.putText(img, f"{progress}%", (10, 160), cv2.FONT_HERSHEY_SIMPLEX,
                           0.5, (255,255,255), 1)
                
                # Progress bar
                bar_width = int(256 * self.get_output('progress'))
                cv2.rectangle(img, (0, 240), (bar_width, 256), (0,255,0), -1)
            else:
                cv2.putText(img, "Ready to scan", (10, 128), cv2.FONT_HERSHEY_SIMPLEX,
                           0.6, (255,255,255), 1)
                cv2.putText(img, "Send trigger signal", (10, 160), cv2.FONT_HERSHEY_SIMPLEX,
                           0.4, (200,200,200), 1)
                
            return QtGui.QImage(img.data, 256, 256, 256*3, QtGui.QImage.Format.Format_RGB888)
            
    def get_config_options(self):
        return [
            ("Scan Amplitude", "scan_amplitude", self.scan_amplitude, None),
            ("Grid Columns", "grid_cols", self.grid_cols, None)
        ]

=== FILE: PCAvisualizernode.py ===

"""
PC Visualizer Node - Visualize what each principal component controls
Shows the "eigenfaces" of your frequency space
"""

import numpy as np
from PyQt6 import QtGui
import cv2

import __main__
BaseNode = __main__.BaseNode
QtGui = __main__.QtGui

class PCVisualizerNode(BaseNode):
    """
    Visualizes individual principal components as images.
    Connect to SpectralPCA to see what each PC represents.
    """
    NODE_CATEGORY = "AI / Physics"
    NODE_COLOR = QtGui.QColor(180, 220, 120)
    
    def __init__(self, pc_index=0, amplitude=3.0):
        super().__init__()
        self.node_title = f"PC Visualizer (PC{pc_index})"
        
        self.inputs = {
            'pca_node': 'node_reference',  # Reference to SpectralPCA node
            'amplitude': 'signal'  # How much to amplify
        }
        self.outputs = {
            'image': 'image',
            'complex_spectrum': 'complex_spectrum',
            'variance_explained': 'signal'
        }
        
        self.pc_index = int(pc_index)
        self.amplitude = float(amplitude)
        
        # Visualization
        self.pc_image = np.zeros((128, 128, 3), dtype=np.uint8)
        self.pc_spectrum = None
        self.variance_explained = 0.0
        
    def step(self):
        # Get amplitude modulation
        amp_signal = self.get_blended_input('amplitude', 'sum')
        if amp_signal is not None:
            amplitude = amp_signal * 10.0  # Scale up for visibility
        else:
            amplitude = self.amplitude
            
        # Get reference to PCA node (this is a bit of a hack)
        # In practice, you'd connect SpectralPCA's outputs here
        # For now, we'll create a synthetic visualization
        
        # Create a spectrum with just this PC activated
        # This would come from: mean_spectrum + pc_component * amplitude
        
        # Placeholder: create a synthetic pattern
        size = 64
        freq = self.pc_index + 1
        
        # Each PC might represent a different frequency pattern
        y, x = np.ogrid[0:size, 0:size]
        pattern = np.sin(2*np.pi*freq*x/size) * np.cos(2*np.pi*freq*y/size)
        pattern = pattern * amplitude
        
        # Visualize
        pattern_norm = (pattern - pattern.min()) / (pattern.max() - pattern.min() + 1e-9)
        self.pc_image = cv2.applyColorMap((pattern_norm * 255).astype(np.uint8), 
                                          cv2.COLORMAP_VIRIDIS)
        
        # Create complex spectrum (simplified)
        self.pc_spectrum = np.fft.fft2(pattern)
        
        # Variance explained (would come from PCA node)
        self.variance_explained = 1.0 / (self.pc_index + 1)  # Decreasing
        
    def get_output(self, port_name):
        if port_name == 'image':
            return self.pc_image.astype(np.float32) / 255.0
        elif port_name == 'complex_spectrum':
            return self.pc_spectrum
        elif port_name == 'variance_explained':
            return self.variance_explained
        return None
        
    def get_display_image(self):
        img = self.pc_image.copy()
        
        # Add label
        label = f"PC{self.pc_index}: {self.variance_explained:.1%}"
        cv2.putText(img, label, (5, 15), cv2.FONT_HERSHEY_SIMPLEX, 
                   0.4, (255,255,255), 1)
        
        return QtGui.QImage(img.data, 128, 128, 128*3, QtGui.QImage.Format.Format_RGB888)
        
    def get_config_options(self):
        return [
            ("PC Index", "pc_index", self.pc_index, None),
            ("Amplitude", "amplitude", self.amplitude, None)
        ]


=== FILE: ReactiveSpaceNode.py ===

"""
Reactive Space Node - A simplified, audio-reactive version of the
earth19.py particle simulation.
Does not use Pygame, Torch, or OpenGL.
"""

import numpy as np
from PyQt6 import QtGui
import cv2

import sys
import os
# --- This is the new, correct block ---
import __main__
BaseNode = __main__.BaseNode
PA_INSTANCE = getattr(__main__, "PA_INSTANCE", None)
QtGui = __main__.QtGui
# ------------------------------------

# --- Color Map Dictionary ---
# Maps string names to OpenCV colormap constants
CMAP_DICT = {
    "gray": None, # Special case for no colormap
    "plasma": cv2.COLORMAP_PLASMA,
    "viridis": cv2.COLORMAP_VIRIDIS,
    "inferno": cv2.COLORMAP_INFERNO,
    "magma": cv2.COLORMAP_MAGMA,
    "hot": cv2.COLORMAP_HOT,
    "jet": cv2.COLORMAP_JET
}


class ReactiveSpaceNode(BaseNode):
    NODE_CATEGORY = "Source"
    NODE_COLOR = QtGui.QColor(50, 80, 160) # Deep space blue
    
    def __init__(self, particle_count=200, width=160, height=120, color_scheme='plasma'):
        super().__init__()
        self.node_title = "Reactive Space"
        
        # --- Inputs for audio-reactivity ---
        self.inputs = {
            'bass_in': 'signal',  # Controls Sun/Attractor
            'highs_in': 'signal'  # Controls Stars/Particles
        }
        self.outputs = {'image': 'image', 'signal': 'signal'}
        
        self.w, self.h = width, height
        self.particle_count = int(particle_count)
        
        # --- Color scheme ---
        self.color_scheme = str(color_scheme)
        
        # Particle state
        self.positions = np.random.rand(self.particle_count, 2).astype(np.float32) * [self.w, self.h]
        self.velocities = (np.random.rand(self.particle_count, 2).astype(np.float32) - 0.5) * 2.0
        
        # The "density" image
        self.space = np.zeros((self.h, self.w), dtype=np.float32)
        self.display_img = np.zeros((self.h, self.w), dtype=np.float32)
        
        # Track last dimensions to detect resizing (NEW)
        self._last_w = self.w
        self._last_h = self.h
        
        self.time = 0.0

    def _check_and_resize_arrays(self):
        """Reinitialize arrays if dimensions changed (NEW HELPER)"""
        if self.w != self._last_w or self.h != self._last_h:
            # Dimensions changed - reinitialize arrays
            old_space = self.space
            
            # Create new arrays
            self.space = np.zeros((self.h, self.w), dtype=np.float32)
            self.display_img = np.zeros((self.h, self.w), dtype=np.float32)
            
            # Try to preserve old content (resize it)
            try:
                # Resize old_space content to fit the new dimensions
                self.space = cv2.resize(old_space, (self.w, self.h), interpolation=cv2.INTER_LINEAR)
            except Exception:
                # If resize fails (e.g., old_space was empty or invalid), just use zeros
                pass 
            
            # Clamp all particle positions to new bounds
            self.positions[:, 0] = np.clip(self.positions[:, 0], 0, self.w - 1)
            self.positions[:, 1] = np.clip(self.positions[:, 1], 0, self.h - 1)
            
            # Update tracking
            self._last_w = self.w
            self._last_h = self.h
            

    def step(self):
        # FIX: Check if node was resized and update arrays
        self._check_and_resize_arrays()
        
        self.time += 0.01
        
        # --- Get audio-reactive signals ---
        bass_energy = self.get_blended_input('bass_in', 'sum') or 0.0
        highs_energy = self.get_blended_input('highs_in', 'sum') or 0.0

        # Central attractor
        attractor_pos = np.array([
            self.w / 2 + np.sin(self.time * 0.5) * self.w * 0.3,
            self.h / 2 + np.cos(self.time * 0.3) * self.h * 0.3
        ])
        
        # Calculate forces (simple gravity)
        to_attractor = attractor_pos - self.positions
        dist_sq = np.sum(to_attractor**2, axis=1, keepdims=True) + 1e-3
        
        base_gravity = 5.0
        sun_pulse_strength = 1.0 + (bass_energy * 5.0)
        force = to_attractor / dist_sq * (base_gravity * sun_pulse_strength)
        
        # Update velocities
        self.velocities += force * 0.1
        
        star_jiggle = (np.random.rand(self.particle_count, 2) - 0.5) * (highs_energy * 0.5)
        self.velocities += star_jiggle
        
        self.velocities *= 0.98
        
        # Update positions
        self.positions += self.velocities
        
        # Clamp positions to valid range
        self.positions[:, 0] = np.clip(self.positions[:, 0], 0, self.w - 1)
        self.positions[:, 1] = np.clip(self.positions[:, 1], 0, self.h - 1)
        
        # Bounce velocities when hitting walls
        mask_x_low = self.positions[:, 0] <= 0
        mask_x_high = self.positions[:, 0] >= self.w - 1
        mask_y_low = self.positions[:, 1] <= 0
        mask_y_high = self.positions[:, 1] >= self.h - 1
        
        self.velocities[mask_x_low | mask_x_high, 0] *= -0.5
        self.velocities[mask_y_low | mask_y_high, 1] *= -0.5

        # Update the density image
        self.space *= 0.9
        
        # Get integer positions
        int_pos = self.positions.astype(int)
        
        # Validate positions
        valid = (int_pos[:, 0] >= 0) & (int_pos[:, 0] < self.w) & \
                (int_pos[:, 1] >= 0) & (int_pos[:, 1] < self.h)
        
        valid_pos = int_pos[valid]
        
        # "Splat" particles onto the image
        if valid_pos.shape[0] > 0:
            y_coords = np.clip(valid_pos[:, 1], 0, self.h - 1)
            x_coords = np.clip(valid_pos[:, 0], 0, self.w - 1)
            # Use assignment to set the density at particle locations
            self.space[y_coords, x_coords] = 1.0
        
        # Blur to make it look like a density field
        self.display_img = cv2.GaussianBlur(self.space, (5, 5), 0)

    def get_output(self, port_name):
        if port_name == 'image':
            return self.display_img
        elif port_name == 'signal':
            # Output mean velocity as a signal
            return np.mean(np.linalg.norm(self.velocities, axis=1))
        return None
        
    def get_display_image(self):
        # FIX: Use the actual current dimensions of the arrays for QImage creation.
        img_u8 = (np.clip(self.display_img, 0, 1) * 255).astype(np.uint8)
        
        cmap_cv2 = CMAP_DICT.get(self.color_scheme)
        
        if cmap_cv2 is not None:
            # Apply CV2 colormap
            img_color = cv2.applyColorMap(img_u8, cmap_cv2)
            img_color = np.ascontiguousarray(img_color)
            h, w = img_color.shape[:2]
            return QtGui.QImage(img_color.data, w, h, 3*w, QtGui.QImage.Format.Format_BGR888)
        else:
            # Just return grayscale at ACTUAL size
            img_u8 = np.ascontiguousarray(img_u8)
            h, w = img_u8.shape
            return QtGui.QImage(img_u8.data, w, h, w, QtGui.QImage.Format.Format_Grayscale8)

    def get_config_options(self):
        # Create color scheme options for the dropdown
        color_options = [(name.title(), name) for name in CMAP_DICT.keys()]
        
        return [
            ("Particle Count", "particle_count", self.particle_count, None),
            ("Color Scheme", "color_scheme", self.color_scheme, color_options),
        ]


=== FILE: U1.py ===

"""
U1FieldNode (Electromagnetism Metaphor)

Simulates a U(1) gauge force, like electromagnetism.
It takes a grayscale "charge density" map and calculates
the resulting force field (like an E-field).

[FIXED] Initialized self.potential in __init__ and
saved potential to self.potential in step().
"""

import numpy as np
import cv2

# --- Magic import block ---
import __main__
BaseNode = __main__.BaseNode
QtGui = __main__.QtGui
# --------------------------

class U1FieldNode(BaseNode):
    """
    Generates a U(1) force field from a charge density map.
    """
    NODE_CATEGORY = "Generator"
    NODE_COLOR = QtGui.QColor(100, 150, 220) # Blue

    def __init__(self, size=128):
        super().__init__()
        self.node_title = "U(1) Field (E/M)"
        
        self.inputs = {
            'charge_in': 'image',    # Grayscale image (0-1)
            'strength': 'signal'     # 0-1, force strength
        }
        self.outputs = {
            'potential_out': 'image', # The scalar potential (blurred charge)
            'field_viz': 'image'      # Vector field visualization
        }
        
        self.size = int(size)
        
        # --- START FIX ---
        # Initialize the output variables to prevent AttributeError
        self.viz = np.zeros((self.size, self.size, 3), dtype=np.float32)
        self.potential = np.zeros((self.size, self.size), dtype=np.float32)
        # --- END FIX ---

    def _prepare_image(self, img):
        if img is None:
            return np.full((self.size, self.size), 0.5, dtype=np.float32)
        
        if img.dtype != np.float32: img = img.astype(np.float32)
        if img.max() > 1.0: img /= 255.0
            
        img_resized = cv2.resize(img, (self.size, self.size), 
                                 interpolation=cv2.INTER_LINEAR)
        
        if img_resized.ndim == 3:
            return cv2.cvtColor(img_resized, cv2.COLOR_RGB2GRAY)
        return img_resized

    def step(self):
        # --- 1. Get Charge Density ---
        # Map input [0, 1] to charge [-1, 1]
        charge_density = (self._prepare_image(
            self.get_blended_input('charge_in', 'first')
        ) * 2.0) - 1.0
        
        strength = self.get_blended_input('strength', 'sum') or 1.0
        
        # --- 2. Calculate Potential ---
        # Simulate long-range 1/r potential by blurring
        # A large blur kernel simulates the 1/r falloff
        ksize = self.size // 4 * 2 + 1 # Must be odd
        
        self.potential = cv2.GaussianBlur(charge_density, (ksize, ksize), 0)
        
        # --- 3. Calculate Force Field (E-Field) ---
        # E = -âV (Force is the negative gradient of potential)
        grad_x = -cv2.Sobel(self.potential, cv2.CV_32F, 1, 0, ksize=3) * strength
        grad_y = -cv2.Sobel(self.potential, cv2.CV_32F, 0, 1, ksize=3) * strength
        
        # --- 4. Create Visualization ---
        self.viz = np.zeros((self.size, self.size, 3), dtype=np.float32)
        step = 8 # Draw an arrow every 8 pixels
        for y in range(0, self.size, step):
            for x in range(0, self.size, step):
                vx = grad_x[y, x] * 20 # Scale for viz
                vy = grad_y[y, x] * 20
                
                pt1 = (x, y)
                pt2 = (int(np.clip(x + vx, 0, self.size-1)), 
                       int(np.clip(y + vy, 0, self.size-1)))
                
                # Color based on direction
                angle = np.arctan2(vy, vx) + np.pi
                hue = int(angle / (2 * np.pi) * 179) # 0-179 for OpenCV HSV
                color_hsv = np.uint8([[[hue, 255, 255]]])
                color_rgb = cv2.cvtColor(color_hsv, cv2.COLOR_HSV2RGB)[0][0]
                color_float = color_rgb.astype(np.float32) / 255.0
                
                # --- START FIX ---
                # Convert numpy.float32 to standard Python floats for OpenCV
                color_tuple = (float(color_float[0]), float(color_float[1]), float(color_float[2]))
                cv2.arrowedLine(self.viz, pt1, pt2, color_tuple, 1, cv2.LINE_AA)
                # --- END FIX ---

    def get_output(self, port_name):
        if port_name == 'potential_out':
            # Normalize potential [-max, +max] to [0, 1]
            p_max = np.max(np.abs(self.potential))
            if p_max == 0: return np.full((self.size, self.size), 0.5, dtype=np.float32)
            return (self.potential / (2 * p_max)) + 0.5
            
        elif port_name == 'field_viz':
            return self.viz
        return None

    def get_display_image(self):
        return self.viz

=== FILE: anttis_crystalmaker.py ===

"""
Antti's CrystalMaker Node - A 3D polyrhythmic field generator
Based on the PolyrhythmicSea class from crystal_kingdom.py
Place this file in the 'nodes' folder
"""

import numpy as np
from PyQt6 import QtGui
import cv2

import sys
import os
# --- This is the new, correct block ---
import __main__
BaseNode = __main__.BaseNode
PA_INSTANCE = getattr(__main__, "PA_INSTANCE", None)
# ------------------------------------
try:
    # --- FIX: Change import to ndimage.convolve for periodic boundaries ---
    from scipy.ndimage import convolve 
    SCIPY_AVAILABLE = True
except ImportError:
    SCIPY_AVAILABLE = False
    print("Warning: CrystalMakerNode requires 'scipy'.")
    print("Please run: pip install scipy")

class CrystalMakerNode(BaseNode):
    NODE_CATEGORY = "Source"
    NODE_COLOR = QtGui.QColor(100, 200, 250) # Crystalline blue
    
    def __init__(self, grid_size=32, num_fields=10):
        super().__init__()
        self.node_title = "Antti's CrystalMaker"
        
        self.inputs = {
            'tension': 'signal',
            'damping': 'signal',
            'nonlinearity_a': 'signal',
            'nonlinearity_b': 'signal'
        }
        self.outputs = {
            'field_slice': 'image', # 2D slice of the 3D field
            'total_energy': 'signal'
        }
        
        self.N = int(grid_size)
        self.num_fields = int(num_fields)
        
        # --- Physics Parameters from crystal_kingdom.py ---
        self.dt = 0.05
        self.polyrhythm_coupling = 0.1
        self.nonlinearity_A = 1.0
        self.nonlinearity_B = 1.0
        self.damping_factor = 0.005
        self.tension = 5.0
        self.base_frequencies_min = 0.5
        self.base_frequencies_max = 2.5
        self.diffusion_coeffs_min = 0.05
        self.diffusion_coeffs_max = 0.1
        
        self.total_energy = 0.0
        
        # --- Internal 3D State ---
        self._initialize_fields_and_params()
        
        # 3D Laplacian Kernel
        self.kern = np.zeros((3,3,3), np.float32)
        self.kern[1,1,1] = -6
        for dx,dy,dz in [(1,1,0),(1,1,2),(1,0,1),(1,2,1),(0,1,1),(2,1,1)]:
            self.kern[dx,dy,dz] = 1
            
        if not SCIPY_AVAILABLE:
            self.node_title = "CrystalMaker (No SciPy!)"

    def _initialize_fields_and_params(self):
        """Initializes or re-initializes fields."""
        shape = (self.N, self.N, self.N)
        self.phi_fields = [(np.random.rand(*shape).astype(np.float32) - 0.5) * 0.5
                           for _ in range(self.num_fields)]
        self.phi_o_fields = [np.copy(phi) for phi in self.phi_fields]
        
        self.base_frequencies = np.linspace(self.base_frequencies_min, self.base_frequencies_max, self.num_fields)
        self.diffusion_coeffs = np.linspace(self.diffusion_coeffs_max, self.diffusion_coeffs_min, self.num_fields)
        self.field_phases = np.random.uniform(0, 2 * np.pi, self.num_fields)

        self.phi = np.zeros(shape, dtype=np.float32)
        self.phi_o = np.zeros(shape, dtype=np.float32)
        self._update_summed_fields()

    def _update_summed_fields(self):
        """Update the main summed field from individual phi fields"""
        self.phi = np.sum(self.phi_fields, axis=0) / max(1, len(self.phi_fields))
        self.phi_o = np.sum(self.phi_o_fields, axis=0) / max(1, len(self.phi_fields))

    def _potential_deriv(self, field_k):
        """Calculate the derivative of the potential function for a field"""
        return -self.nonlinearity_A * field_k + self.nonlinearity_B * (field_k**3)
        
    def _laplacian(self, f):
        """3D Laplacian using convolution with periodic boundary ('wrap')"""
        if not SCIPY_AVAILABLE:
            return np.zeros_like(f)
            
        # --- FIX: Use mode='wrap' with scipy.ndimage.convolve ---
        return convolve(f, self.kern, mode='wrap')
        # --- END FIX ---

    def step(self):
        if not SCIPY_AVAILABLE:
            return
            
        # --- Update parameters from inputs ---
        # Map signals [-1, 1] to a useful range
        self.tension = (self.get_blended_input('tension', 'sum') or 0.0) * 10.0 + 10.0 # Range [0, 20]
        self.damping_factor = (self.get_blended_input('damping', 'sum') or 0.0) * 0.02 + 0.02 # Range [0, 0.04]
        self.nonlinearity_A = (self.get_blended_input('nonlinearity_a', 'sum') or 0.0) + 1.0 # Range [0, 2]
        self.nonlinearity_B = (self.get_blended_input('nonlinearity_b', 'sum') or 0.0) + 1.0 # Range [0, 2]

        # --- Run simulation step (from crystal_kingdom.py) ---
        new_phi_list = []

        self.field_phases += self.base_frequencies * self.dt
        self.field_phases %= (2 * np.pi)

        for k in range(self.num_fields):
            phi_k = self.phi_fields[k]
            phi_o_k = self.phi_o_fields[k]

            vel_k = phi_k - phi_o_k
            lap_k = self._laplacian(phi_k)
            potential_deriv_k = self._potential_deriv(phi_k)

            other_fields_sum = (np.sum(self.phi_fields, axis=0) - phi_k)
            coupling_force = self.polyrhythm_coupling * other_fields_sum / max(1, self.num_fields - 1)

            driving_force_k = 0.005 * np.sin(self.field_phases[k])
            c2 = 1.0 / (1.0 + self.tension * phi_k**2 + 1e-6)

            acc = (c2 * self.diffusion_coeffs[k] * lap_k -
                   potential_deriv_k +
                   coupling_force +
                   driving_force_k)

            new_phi_k = phi_k + (1 - self.damping_factor * self.dt) * vel_k + self.dt**2 * acc
            new_phi_list.append(new_phi_k)

        # Update fields
        # Note: phi_o_fields update logic (phi_o_k = phi_k) seems missing from the original source step,
        # but the physics uses phi_o_k to compute vel_k, so we need to update it here.
        self.phi_o_fields = self.phi_fields # Save current as previous for the next step
        self.phi_fields = new_phi_list
        self._update_summed_fields()
        
        # Calculate total energy (simplified)
        self.total_energy = np.mean(self.phi**2)

    def get_output(self, port_name):
        if port_name == 'field_slice':
            # Output the middle slice
            z_mid = self.N // 2
            field_slice = self.phi[z_mid, :, :]
            
            # Normalize field for output
            vmax = np.abs(field_slice).max() + 1e-9
            return (field_slice / (2 * vmax)) + 0.5 # map [-v, v] to [0, 1]
            
        elif port_name == 'total_energy':
            return self.total_energy
        return None
        
    def get_display_image(self):
        # Get the middle slice for the node's display
        z_mid = self.N // 2
        field_slice = self.phi[z_mid, :, :]
        
        # Normalize field for display
        vmax = np.abs(field_slice).max() + 1e-9
        img_norm = np.clip((field_slice / (2 * vmax)) + 0.5, 0.0, 1.0)
        
        img_u8 = (img_norm * 255).astype(np.uint8)
        
        # Apply a colormap
        img_color = cv2.applyColorMap(img_u8, cv2.COLORMAP_JET)
        
        img_color = np.ascontiguousarray(img_color)
        h, w = img_color.shape[:2]
        return QtGui.QImage(img_color.data, w, h, 3*w, QtGui.QImage.Format.Format_BGR888)

    def get_config_options(self):
        return [
            ("Grid Size (3D)", "N", self.N, None),
            ("Num Fields", "num_fields", self.num_fields, None),
        ]


=== FILE: anttis_ifft.py ===

"""
iFFT Cochlea Node - Reconstructs an image from a complex spectrum.
Based on the hardwired iFFTCochleaNode from anttis_perception_laboratory.py
Requires: pip install scipy
Place this file in the 'nodes' folder
"""

import numpy as np
from PyQt6 import QtGui
import cv2

# --- !! CRITICAL IMPORT BLOCK !! ---
import __main__
BaseNode = __main__.BaseNode
QtGui = __main__.QtGui
PA_INSTANCE = getattr(__main__, "PA_INSTANCE", None)
# -----------------------------------

try:
    from scipy.fft import irfft
    SCIPY_AVAILABLE = True
except ImportError:
    SCIPY_AVAILABLE = False
    print("Warning: iFFTCochleaNode requires 'scipy'.")
    print("Please run: pip install scipy")


class iFFTCochleaNode(BaseNode):
    """
    Performs an Inverse Real FFT on a complex spectrum (from FFTCochleaNode)
    to reconstruct a 2D image.
    """
    NODE_CATEGORY = "Transform"
    NODE_COLOR = QtGui.QColor(200, 100, 60)
    
    def __init__(self, height=120, width=160):
        super().__init__()
        self.node_title = "iFFT Cochlea"
        self.inputs = {'complex_spectrum': 'complex_spectrum'}
        self.outputs = {'image': 'image'}
        
        self.h, self.w = height, width
        self.reconstructed_img = np.zeros((self.h, self.w), dtype=np.float32)

    def step(self):
        if not SCIPY_AVAILABLE:
            return

        complex_spec = self.get_blended_input('complex_spectrum', 'mean')
        
        if complex_spec is not None and complex_spec.ndim == 2:
            try:
                # Perform inverse real FFT
                img = irfft(complex_spec, axis=1).astype(np.float32)
                
                # Resize to target output size (just in case)
                self.reconstructed_img = cv2.resize(img, (self.w, self.h))
                
                # Normalize for viewing (0-1)
                min_v, max_v = np.min(self.reconstructed_img), np.max(self.reconstructed_img)
                if (max_v - min_v) > 1e-6:
                    self.reconstructed_img = (self.reconstructed_img - min_v) / (max_v - min_v)
                else:
                    self.reconstructed_img.fill(0.5)
                    
            except Exception as e:
                print(f"iFFT Error: {e}")
                self.reconstructed_img.fill(0.0)
        else:
            # Fade to black if no input
            self.reconstructed_img *= 0.9 
            
    def get_output(self, port_name):
        if port_name == 'image':
            return self.reconstructed_img
        return None
        
    def get_display_image(self):
        img_u8 = (np.clip(self.reconstructed_img, 0, 1) * 255).astype(np.uint8)
        img_u8 = np.ascontiguousarray(img_u8)
        return QtGui.QImage(img_u8.data, self.w, self.h, self.w, QtGui.QImage.Format.Format_Grayscale8)

    def get_config_options(self):
        return [
            ("Height", "height", self.h, None),
            ("Width", "width", self.w, None)
        ]

=== FILE: anttis_phiworld.py ===

"""
Antti's PhiWorld Node - A TADS-like particle field simulation
Driven by an energy signal and perturbed by an image.
Based on the physics from phiworld2.py.
Requires: pip install scipy
Place this file in the 'nodes' folder
"""

import numpy as np
from PyQt6 import QtGui
import cv2

import sys
import os
# --- This is the new, correct block ---
import __main__
BaseNode = __main__.BaseNode
PA_INSTANCE = getattr(__main__, "PA_INSTANCE", None)
# ------------------------------------

try:
    from scipy.signal import convolve2d
    from scipy.ndimage import maximum_filter
    SCIPY_AVAILABLE = True
except ImportError:
    SCIPY_AVAILABLE = False
    print("Warning: PhiWorldNode requires 'scipy'.")
    print("Please run: pip install scipy")

class PhiWorldNode(BaseNode):
    NODE_CATEGORY = "Transform"
    NODE_COLOR = QtGui.QColor(220, 120, 40) # Transform Orange
    
    def __init__(self, grid_size=96):
        super().__init__()
        self.node_title = "Antti's PhiWorld"
        
        self.inputs = {
            'energy_in': 'signal', # Drives the simulation
            'perturb_in': 'image'  # Pushes the field
        }
        self.outputs = {
            'field': 'image',       # The raw phi field
            'particles': 'image',   # Just the detected particles
            'count': 'signal'       # Number of particles
        }
        
        self.grid_size = int(grid_size)
        
        # --- Parameters from phiworld2.py ---
        self.dt = 0.08
        self.damping = 0.005 # Increased damping for stability in node
        self.base_c_sq = 1.0
        self.tension_factor = 5.0
        self.potential_lin = 1.0
        self.potential_cub = 0.2
        self.biharmonic_gamma = 0.02
        self.particle_threshold = 0.5
        
        # --- Internal State ---
        self.phi = np.zeros((self.grid_size, self.grid_size), dtype=np.float64)
        self.phi_old = np.zeros_like(self.phi)
        
        # Optimized Laplacian Kernel
        self.laplacian_kernel = np.array([[0, 1, 0], [1, -4, 1]], dtype=np.float64)
        
        # Outputs
        self.particle_image = np.zeros_like(self.phi, dtype=np.float32)
        self.particle_count = 0.0

        if not SCIPY_AVAILABLE:
            self.node_title = "PhiWorld (No SciPy!)"

    # --- Physics methods adapted from phiworld2.py ---
    
    def _laplacian(self, f):
        # Using np.roll is faster than convolve2d for this kernel
        lap_x = np.roll(f, -1, axis=1) - 2 * f + np.roll(f, 1, axis=1)
        lap_y = np.roll(f, -1, axis=0) - 2 * f + np.roll(f, 1, axis=0)
        return lap_x + lap_y

    def _biharmonic(self, f):
        lap_f = self._laplacian(f)
        return self._laplacian(lap_f) # Laplacian of the Laplacian

    def _potential_deriv(self, phi):
        return (-self.potential_lin * phi
                + self.potential_cub * (phi**3))

    def _local_speed_sq(self, phi):
        intensity = phi**2
        return self.base_c_sq / (1.0 + self.tension_factor * intensity + 1e-9)

    def _track_particles(self, field):
        """Optimized particle tracking using scipy.ndimage."""
        # Find local maxima using a 3x3 filter
        maxima_mask = (field == maximum_filter(field, size=3))
        # Find points above threshold
        threshold_mask = (field > self.particle_threshold)
        
        # Combine masks
        particle_mask = (maxima_mask & threshold_mask)
        
        # Update outputs
        self.particle_image = particle_mask.astype(np.float32)
        self.particle_count = np.sum(particle_mask)

    def step(self):
        if not SCIPY_AVAILABLE:
            return

        # Get inputs
        energy = self.get_blended_input('energy_in', 'sum') or 0.0
        perturb_img = self.get_blended_input('perturb_in', 'mean')
        
        if energy <= 0.01:
            # If no energy, dampen the field
            self.phi *= (1.0 - (self.damping * 10)) # Faster damping
            self.phi_old = self.phi.copy()
            self.particle_image *= 0.9
            self.particle_count = 0
            return

        # --- Run simulation step (from phiworld2.py) ---
        
        # Calculate forces
        lap_phi = self._laplacian(self.phi)
        biharm_phi = self._biharmonic(self.phi)
        c2 = self._local_speed_sq(self.phi)
        V_prime = self._potential_deriv(self.phi)
        
        # Scale acceleration by energy input
        acceleration = energy * ( (c2 * lap_phi) - V_prime - (self.biharmonic_gamma * biharm_phi) )

        # Update field (Verlet integration)
        velocity = self.phi - self.phi_old
        phi_new = self.phi + (1.0 - self.damping * self.dt) * velocity + (self.dt**2) * acceleration

        # --- Add Image Perturbation ---
        if perturb_img is not None:
            # Resize image to grid
            img_resized = cv2.resize(perturb_img, (self.grid_size, self.grid_size),
                                     interpolation=cv2.INTER_AREA)
            # "Push" the field with the image, scaled by energy
            phi_new += (img_resized - 0.5) * 0.1 * energy # (Image is 0-1, so map to -0.5 to 0.5)

        self.phi_old = self.phi.copy()
        self.phi = phi_new
        
        # Clamp to prevent instability
        self.phi = np.clip(self.phi, -10.0, 10.0)

        # Track particles on the new field
        self._track_particles(np.abs(self.phi))

    def get_output(self, port_name):
        if port_name == 'field':
            # Normalize field for output [-2, 2] -> [0, 1]
            return np.clip(self.phi * 0.25 + 0.5, 0.0, 1.0)
        elif port_name == 'particles':
            return self.particle_image
        elif port_name == 'count':
            return self.particle_count
        return None
        
    def get_display_image(self):
        # Normalize field for display
        img_norm = np.clip(self.phi * 0.25 + 0.5, 0.0, 1.0)
        
        img_u8 = (img_norm * 255).astype(np.uint8)
        
        # Apply a colormap
        img_color = cv2.applyColorMap(img_u8, cv2.COLORMAP_VIRIDIS)
        
        # Overlay particles in bright red
        img_color[self.particle_image > 0] = (0, 0, 255) # BGR for red
        
        img_color = np.ascontiguousarray(img_color)
        h, w = img_color.shape[:2]
        return QtGui.QImage(img_color.data, w, h, 3*w, QtGui.QImage.Format.Format_BGR888)

    def get_config_options(self):
        return [
            ("Grid Size", "grid_size", self.grid_size, None),
            ("Particle Thresh", "particle_threshold", self.particle_threshold, None),
            ("Damping", "damping", self.damping, None),
            ("Tension", "tension_factor", self.tension_factor, None),
            ("Linear Pot.", "potential_lin", self.potential_lin, None),
            ("Cubic Pot.", "potential_cub", self.potential_cub, None),
            ("Biharmonic (g)", "biharmonic_gamma", self.biharmonic_gamma, None),
        ]

=== FILE: anttis_phiworld3d.py ===

"""
Antti's PhiWorld 3D Node - A 3D particle field simulation
Driven by an energy signal and perturbed by an image slice.
Physics adapted from phiworld2.py.
3D logic inspired by best.py.
Requires: pip install scipy
Place this file in the 'nodes' folder
"""

import numpy as np
from PyQt6 import QtGui
import cv2

import sys
import os
# --- This is the new, correct block ---
import __main__
BaseNode = __main__.BaseNode
PA_INSTANCE = getattr(__main__, "PA_INSTANCE", None)
# ------------------------------------

try:
    from scipy.ndimage import maximum_filter
    SCIPY_AVAILABLE = True
except ImportError:
    SCIPY_AVAILABLE = False
    print("Warning: PhiWorld3DNode requires 'scipy'.")
    print("Please run: pip install scipy")

class PhiWorld3DNode(BaseNode):
    NODE_CATEGORY = "Transform"
    NODE_COLOR = QtGui.QColor(220, 120, 40) # Transform Orange
    
    def __init__(self, grid_size=48):
        super().__init__()
        self.node_title = "Antti's PhiWorld 3D"
        
        self.inputs = {
            'energy_in': 'signal', # Drives the simulation
            'perturb_in': 'image', # 2D image to "push" the field
            'z_slice': 'signal'    # Controls which Z-slice to push (range -1 to 1)
        }
        self.outputs = {
            'field_slice': 'image',   # A 2D slice of the 3D field (for display)
            'particles_slice': 'image', # A 2D slice of detected particles
            'count': 'signal'         # Total 3D particle count
        }
        
        self.grid_size = int(grid_size)
        
        # --- Parameters from phiworld2.py ---
        self.dt = 0.08
        self.damping = 0.005
        self.base_c_sq = 1.0
        self.tension_factor = 5.0
        self.potential_lin = 1.0
        self.potential_cub = 0.2
        self.biharmonic_gamma = 0.02
        self.particle_threshold = 0.5
        
        # --- Internal 3D State ---
        shape = (self.grid_size, self.grid_size, self.grid_size)
        self.phi = np.zeros(shape, dtype=np.float64)
        self.phi_old = np.zeros_like(self.phi)
        
        # Outputs
        self.particle_image = np.zeros_like(self.phi, dtype=np.float32)
        self.particle_count = 0.0

        if not SCIPY_AVAILABLE:
            self.node_title = "PhiWorld 3D (No SciPy!)"

    # --- 3D Physics methods adapted from phiworld2.py ---
    
    def _laplacian_3d(self, f):
        """A 3D Laplacian using numpy.roll (inspired by 2D version)"""
        lap_x = np.roll(f, -1, axis=0) - 2 * f + np.roll(f, 1, axis=0)
        lap_y = np.roll(f, -1, axis=1) - 2 * f + np.roll(f, 1, axis=1)
        lap_z = np.roll(f, -1, axis=2) - 2 * f + np.roll(f, 1, axis=2)
        return lap_x + lap_y + lap_z

    def _biharmonic(self, f):
        """3D Biharmonic is the Laplacian of the Laplacian"""
        lap_f = self._laplacian_3d(f)
        return self._laplacian_3d(lap_f)

    def _potential_deriv(self, phi):
        """Element-wise potential, works in 3D"""
        return (-self.potential_lin * phi
                + self.potential_cub * (phi**3))

    def _local_speed_sq(self, phi):
        """Element-wise speed, works in 3D"""
        intensity = phi**2
        return self.base_c_sq / (1.0 + self.tension_factor * intensity + 1e-9)

    def _track_particles(self, field):
        """3D particle tracking using scipy.ndimage.maximum_filter"""
        # Find local maxima using a 3x3x3 filter
        maxima_mask = (field == maximum_filter(field, size=(3, 3, 3)))
        # Find points above threshold
        threshold_mask = (field > self.particle_threshold)
        
        # Combine masks
        particle_mask = (maxima_mask & threshold_mask)
        
        # Update outputs
        self.particle_image = particle_mask.astype(np.float32)
        self.particle_count = np.sum(particle_mask)

    def step(self):
        if not SCIPY_AVAILABLE:
            return

        # Get inputs
        energy = self.get_blended_input('energy_in', 'sum') or 0.0
        perturb_img = self.get_blended_input('perturb_in', 'mean')
        z_slice_signal = self.get_blended_input('z_slice', 'sum') or 0.0
        
        if energy <= 0.01:
            # If no energy, dampen the field
            self.phi *= (1.0 - (self.damping * 10)) # Faster damping
            self.phi_old = self.phi.copy()
            self.particle_image *= 0.9
            self.particle_count = 0
            return

        # --- Run 3D simulation step (adapted from phiworld2.py) ---
        
        # Calculate 3D forces
        lap_phi = self._laplacian_3d(self.phi)
        biharm_phi = self._biharmonic(self.phi)
        c2 = self._local_speed_sq(self.phi)
        V_prime = self._potential_deriv(self.phi)
        
        # Scale acceleration by energy input
        acceleration = energy * ( (c2 * lap_phi) - V_prime - (self.biharmonic_gamma * biharm_phi) )

        # Update field (Verlet integration)
        velocity = self.phi - self.phi_old
        phi_new = self.phi + (1.0 - self.damping * self.dt) * velocity + (self.dt**2) * acceleration

        # --- Add Image Perturbation ---
        if perturb_img is not None:
            # Determine which Z-slice to push
            # Map signal [-1, 1] to [0, grid_size-1]
            z_index = int(np.clip((z_slice_signal + 1.0) / 2.0 * (self.grid_size - 1), 0, self.grid_size - 1))
            
            # Resize image to grid slice
            img_resized = cv2.resize(perturb_img, (self.grid_size, self.grid_size),
                                     interpolation=cv2.INTER_AREA)
                                     
            # "Push" the field at that slice
            push_force = (img_resized - 0.5) * 0.1 * energy # Map [0,1] to [-0.05, 0.05] * energy
            phi_new[z_index, :, :] += push_force

        self.phi_old = self.phi.copy()
        self.phi = phi_new
        
        # Clamp to prevent instability
        self.phi = np.clip(self.phi, -10.0, 10.0)

        # Track particles on the new 3D field
        self._track_particles(np.abs(self.phi))

    def get_output(self, port_name):
        # Output the middle slice for visualization
        z_mid = self.grid_size // 2
        
        if port_name == 'field_slice':
            # Normalize field slice for output [-2, 2] -> [0, 1]
            field_slice = self.phi[z_mid, :, :]
            return np.clip(field_slice * 0.25 + 0.5, 0.0, 1.0)
        
        elif port_name == 'particles_slice':
            return self.particle_image[z_mid, :, :]
            
        elif port_name == 'count':
            # Output the total 3D particle count
            return self.particle_count
        return None
        
    def get_display_image(self):
        # Get the middle slice for the node's display
        z_mid = self.grid_size // 2
        field_slice = self.phi[z_mid, :, :]
        particles_slice = self.particle_image[z_mid, :, :]
        
        # Normalize field for display
        img_norm = np.clip(field_slice * 0.25 + 0.5, 0.0, 1.0)
        img_u8 = (img_norm * 255).astype(np.uint8)
        
        # Apply a colormap
        img_color = cv2.applyColorMap(img_u8, cv2.COLORMAP_VIRIDIS)
        
        # Overlay particles in bright red
        img_color[particles_slice > 0] = (0, 0, 255) # BGR for red
        
        img_color = np.ascontiguousarray(img_color)
        h, w = img_color.shape[:2]
        return QtGui.QImage(img_color.data, w, h, 3*w, QtGui.QImage.Format.Format_BGR888)

    def get_config_options(self):
        return [
            ("Grid Size (3D)", "grid_size", self.grid_size, None),
            ("Particle Thresh", "particle_threshold", self.particle_threshold, None),
            ("Damping", "damping", self.damping, None),
            ("Tension", "tension_factor", self.tension_factor, None),
            ("Linear Pot.", "potential_lin", self.potential_lin, None),
            ("Cubic Pot.", "potential_cub", self.potential_cub, None),
            ("Biharmonic (g)", "biharmonic_gamma", self.biharmonic_gamma, None),
        ]

=== FILE: anttis_signal_attractor.py ===

"""
Signal Attractor Node - Generates a 2D chaotic pattern from two signals
Place this file in the 'nodes' folder
"""

import numpy as np
from PyQt6 import QtGui
import cv2

import sys
import os
# --- This is the new, correct block ---
import __main__
BaseNode = __main__.BaseNode
PA_INSTANCE = getattr(__main__, "PA_INSTANCE", None)
# ------------------------------------

class SignalAttractorNode(BaseNode):
    NODE_CATEGORY = "Transform"
    NODE_COLOR = QtGui.QColor(180, 80, 180) # Attractor Purple
    
    def __init__(self, width=128, height=128, param_c=1.0, param_d=0.7):
        super().__init__()
        self.node_title = "Signal Attractor"
        self.inputs = {
            'signal_a': 'signal',
            'signal_b': 'signal'
        }
        self.outputs = {'image': 'image', 'x_out': 'signal', 'y_out': 'signal'}
        
        self.w, self.h = int(width), int(height)
        
        # Attractor state
        self.x, self.y = 0.1, 0.1
        
        # Parameters (a & b are controlled by input, c & d are configurable)
        self.param_c = float(param_c)
        self.param_d = float(param_d)
        
        # For visualization
        self.points = np.zeros((self.h, self.w), dtype=np.float32)
        self.img = np.zeros((self.h, self.w), dtype=np.float32)

    def step(self):
        # Get signals, map from [-1, 1] to [-2, 2]
        param_a = (self.get_blended_input('signal_a', 'sum') or 0.0) * 2.0
        param_b = (self.get_blended_input('signal_b', 'sum') or 0.0) * 2.0
        
        # Iterate the attractor equations 500 times per frame
        for _ in range(500):
            # Clifford Attractor equations
            x_new = np.sin(param_a * self.y) + self.param_c * np.cos(param_a * self.x)
            y_new = np.sin(param_b * self.x) + self.param_d * np.cos(param_b * self.y)
            
            self.x, self.y = x_new, y_new
            
            # Scale from [-2, 2] range to image coordinates
            px = int((self.x + 2.0) / 4.0 * self.w)
            py = int((self.y + 2.0) / 4.0 * self.h)
            
            if 0 <= px < self.w and 0 <= py < self.h:
                self.points[py, px] += 0.1 # Add energy
        
        # Apply decay to the image so it fades
        self.points *= 0.97
        self.points = np.clip(self.points, 0, 1.0)
        
        # Blur for a "glowing" effect
        self.img = cv2.GaussianBlur(self.points, (3, 3), 0)
        
    def get_output(self, port_name):
        if port_name == 'image':
            return self.img
        elif port_name == 'x_out':
            return self.x / 2.0 # Normalize to [-1, 1]
        elif port_name == 'y_out':
            return self.y / 2.0 # Normalize to [-1, 1]
        return None
        
    def get_display_image(self):
        img_u8 = (np.clip(self.img, 0, 1) * 255).astype(np.uint8)
        img_u8 = np.ascontiguousarray(img_u8)
        return QtGui.QImage(img_u8.data, self.w, self.h, self.w, QtGui.QImage.Format.Format_Grayscale8)

    def get_config_options(self):
        return [
            ("Param C", "param_c", self.param_c, None),
            ("Param D", "param_d", self.param_d, None),
            ("Width", "w", self.w, None),
            ("Height", "h", self.h, None),
        ]

=== FILE: anttis_spiking_neuron.py ===

"""
Antti's Spiking Neuron - A Leaky Integrate-and-Fire (LIF) neuron
Transforms input signals into spikes. Can be chained.
Place this file in the 'nodes' folder
"""

import numpy as np
from PyQt6 import QtGui
import cv2

import sys
import os
# --- This is the new, correct block ---
import __main__
BaseNode = __main__.BaseNode
PA_INSTANCE = getattr(__main__, "PA_INSTANCE", None)
# ------------------------------------

class SpikingNeuronNode(BaseNode):
    NODE_CATEGORY = "Transform"
    NODE_COLOR = QtGui.QColor(220, 120, 40) # Neural orange
    
    def __init__(self, threshold=1.0, tau_m=0.1, resistance=5.0, refractory_ms=0.05):
        super().__init__()
        self.node_title = "Spiking Neuron (LIF)"
        
        self.inputs = {'signal_in': 'signal'}
        self.outputs = {'spike_out': 'signal'}
        
        # --- Neuron Parameters ---
        # These are configurable (see get_config_options)
        self.V_rest = 0.0
        self.V_threshold = float(threshold)
        self.V_reset = 0.0
        self.tau_m = float(tau_m)             # Membrane time constant (sec)
        self.R_m = float(resistance)          # Membrane resistance (scales input)
        self.refractory_period = float(refractory_ms) # Refractory period (sec)
        
        # --- Neuron State ---
        self.V_m = self.V_rest                # Current membrane potential
        self.refractory_timer = 0.0           # Countdown timer for refractory period
        self.output_signal = 0.0              # Output spike
        self.dt = 1.0 / 30.0                  # Assume ~30 FPS step rate

    def step(self):
        # 1. Reset output
        self.output_signal = 0.0
        
        # 2. Check refractory period
        if self.refractory_timer > 0:
            self.refractory_timer -= self.dt
            self.V_m = self.V_reset # Keep potential at reset
            return

        # 3. Get total input current (crucially, using 'sum' blend mode)
        # This allows multiple neurons to connect and sum their inputs
        I_in = self.get_blended_input('signal_in', 'sum') or 0.0
        
        # 4. Leaky Integrate-and-Fire (LIF) equation
        # tau_m * dV/dt = (V_rest - V) + R_m * I_in
        # dV = [ (V_rest - V_m) + (R_m * I_in) ] / tau_m * dt
        dV = (((self.V_rest - self.V_m) + self.R_m * I_in) / self.tau_m) * self.dt
        
        self.V_m += dV
        
        # 5. Check for spike
        if self.V_m >= self.V_threshold:
            self.output_signal = 1.0          # Fire!
            self.V_m = self.V_reset           # Reset potential
            self.refractory_timer = self.refractory_period # Start refractory timer

    def get_output(self, port_name):
        if port_name == 'spike_out':
            return self.output_signal
        return None
        
    def get_display_image(self):
        w, h = 64, 64
        img = np.zeros((h, w, 3), dtype=np.uint8)
        
        # Max voltage to display (to see threshold)
        max_viz_v = self.V_threshold * 1.2
        
        # Draw threshold line (Red)
        thresh_y = h - int(np.clip(self.V_threshold / max_viz_v, 0, 1) * h)
        cv2.line(img, (0, thresh_y), (w, thresh_y), (0, 0, 255), 1)

        # Draw resting line (Gray)
        rest_y = h - int(np.clip(self.V_rest / max_viz_v, 0, 1) * h)
        cv2.line(img, (0, rest_y), (w, rest_y), (100, 100, 100), 1)

        # Draw membrane potential bar
        vm_y = h - int(np.clip(self.V_m / max_viz_v, 0, 1) * h)
        
        if self.output_signal == 1.0:
            bar_color = (0, 255, 255) # Yellow
        elif self.refractory_timer > 0:
            bar_color = (255, 100, 0) # Blue
        else:
            bar_color = (0, 255, 0) # Green
            
        cv2.rectangle(img, (w//2 - 5, vm_y), (w//2 + 5, h), bar_color, -1)
        
        img = np.ascontiguousarray(img)
        return QtGui.QImage(img.data, w, h, 3*w, QtGui.QImage.Format.Format_BGR888)

    def get_config_options(self):
        return [
            ("Threshold", "V_threshold", self.V_threshold, None),
            ("Leak (tau_m)", "tau_m", self.tau_m, None),
            ("Input (R_m)", "R_m", self.R_m, None),
            ("Refractory (sec)", "refractory_period", self.refractory_period, None),
        ]

=== FILE: anttis_superfluid.py ===

"""
Antti's Superfluid Node - Simulates a 1D complex field with knots
Physics based on the 1D NLSE from knotiverse_interactive_viewer.py
Requires: pip install scipy
Place this file in the 'nodes' folder
"""

import numpy as np
from PyQt6 import QtGui
import cv2

import sys
import os
# --- This is the new, correct block ---
import __main__
BaseNode = __main__.BaseNode
PA_INSTANCE = getattr(__main__, "PA_INSTANCE", None)
# ------------------------------------

try:
    from scipy.signal import hilbert
    SCIPY_AVAILABLE = True
except ImportError:
    SCIPY_AVAILABLE = False
    print("Warning: AnttiSuperfluidNode requires 'scipy'.")
    print("Please run: pip install scipy")

class AnttiSuperfluidNode(BaseNode):
    NODE_CATEGORY = "Transform"
    NODE_COLOR = QtGui.QColor(180, 80, 180) # Superfluid purple
    
    def __init__(self, grid_size=512, coupling=0.5, nonlinear=0.8, damping=0.005):
        super().__init__()
        self.node_title = "Antti's Superfluid"
        
        self.inputs = {
            'signal_in': 'signal',
            'coupling': 'signal',
            'nonlinearity': 'signal',
            'damping': 'signal'
        }
        self.outputs = {
            'field_image': 'image',
            'angular_momentum': 'signal',
            'knot_count': 'signal'
        }
        
        # --- Parameters from knotiverse_interactive_viewer.py ---
        self.L = int(grid_size)
        self.dt = 0.05
        self.detect_threshold = 0.5
        self.saturation_threshold = 2.0
        self.max_amplitude_clip = 1e3
        
        # Default physics values (will be overridden by signals)
        self.coupling = coupling
        self.nonlinear = nonlinear
        self.damping = damping
        
        # --- Internal State ---
        rng = np.random.default_rng()
        self.psi = (rng.standard_normal(self.L) + 1j * rng.standard_normal(self.L)) * 0.01
        
        # Seed with a pulse
        x = np.arange(self.L)
        p = self.L // 2
        gauss = 1.0 * np.exp(-((x - p)**2) / (2 * 4**2))
        self.psi += gauss * np.exp(1j * 2.0 * np.pi * rng.random())
        
        self.knots = np.array([], dtype=int)
        self.angular_momentum_out = 0.0
        
        if not SCIPY_AVAILABLE:
            self.node_title = "Superfluid (No SciPy!)"

    def laplacian_1d(self, arr):
        """Discrete laplacian with periodic boundary."""
        return np.roll(arr, -1) - 2*arr + np.roll(arr, 1)

    def step(self):
        if not SCIPY_AVAILABLE:
            return

        # --- Get inputs ---
        signal_in = self.get_blended_input('signal_in', 'sum') or 0.0
        coupling = self.get_blended_input('coupling', 'sum')
        nonlinear = self.get_blended_input('nonlinearity', 'sum')
        damping = self.get_blended_input('damping', 'sum')
        
        # Use signal if connected, else use internal value
        c = coupling if coupling is not None else self.coupling
        n = nonlinear if nonlinear is not None else self.nonlinear
        d = damping if damping is not None else self.damping

        # --- Physics Step (from knotiverse_interactive_viewer.py) ---
        lap = self.laplacian_1d(self.psi)
        coupling_term = 1j * c * lap
        
        amp = np.abs(self.psi)
        sat = np.tanh(amp / self.saturation_threshold)
        nonlin_term = -1j * n * (sat**2) * self.psi
        
        damping_term = -d * self.psi
        
        self.psi = self.psi + self.dt * (coupling_term + nonlin_term + damping_term)
        
        # --- Resonance from input signal ---
        # "Pluck" the center of the string
        self.psi[self.L // 2] += signal_in * 0.5 # Scale input
        
        # Stability checks
        self.psi = np.nan_to_num(self.psi, nan=0.0, posinf=0.0, neginf=0.0)
        amp_new = np.abs(self.psi)
        over = amp_new > self.max_amplitude_clip
        if np.any(over):
            self.psi[over] = self.psi[over] * (self.max_amplitude_clip / amp_new[over])
        
        amp_now = np.abs(self.psi)
        
        # --- Knot Detection ---
        left = np.roll(amp_now, 1)
        right = np.roll(amp_now, -1)
        mask_thresh = amp_now > self.detect_threshold
        mask_local_max = (amp_now >= left) & (amp_now >= right)
        self.knots = np.where(mask_thresh & mask_local_max)[0]
        self.knot_count_out = len(self.knots)
        
        # --- Angular Momentum ---
        grad_psi = np.roll(self.psi, -1) - np.roll(self.psi, 1)
        moment_density = np.imag(np.conj(self.psi) * grad_psi)
        self.angular_momentum_out = float(np.sum(moment_density))

    def get_output(self, port_name):
        if port_name == 'field_image':
            return self._draw_field_image(as_float=True)
        elif port_name == 'angular_momentum':
            return self.angular_momentum_out
        elif port_name == 'knot_count':
            return self.knot_count_out
        return None
        
    def _draw_field_image(self, as_float=False):
        h, w = 64, self.L
        img_color = np.zeros((h, w, 3), dtype=np.uint8)
        
        # Get field data
        amp_now = np.abs(self.psi)
        phase_now = np.angle(hilbert(self.psi.real))
        
        # Normalize
        amp_norm = np.clip(amp_now / self.saturation_threshold, 0, 1)
        phase_norm = (phase_now + np.pi) / (2 * np.pi)
        
        # Draw amplitude (top half) and phase (bottom half)
        h_half = h // 2
        for x in range(w):
            # Amplitude (Cyan)
            y_amp = int((h_half - 1) - amp_norm[x] * (h_half - 1))
            img_color[y_amp, x] = (255, 255, 0) # BGR for Cyan
            
            # Phase (Magenta)
            y_phase = int(h_half + (h_half - 1) - phase_norm[x] * (h_half - 1))
            img_color[y_phase, x] = (255, 0, 255) # BGR for Magenta
            
        # Draw center line
        cv2.line(img_color, (0, h // 2), (w, h // 2), (50, 50, 50), 1)
        
        # Draw knots (Red)
        for kx in self.knots:
            ky = int((h_half - 1) - amp_norm[kx] * (h_half - 1))
            cv2.circle(img_color, (kx, ky), 3, (0, 0, 255), -1) # BGR for Red
            
        if as_float:
            return img_color.astype(np.float32) / 255.0
            
        img_color = np.ascontiguousarray(img_color)
        return QtGui.QImage(img_color.data, w, h, 3*w, QtGui.QImage.Format.Format_BGR888)
        
    def get_display_image(self):
        return self._draw_field_image(as_float=False)

    def get_config_options(self):
        return [
            ("Grid Size", "L", self.L, None),
            ("Knot Threshold", "detect_threshold", self.detect_threshold, None),
            ("Coupling", "coupling", self.coupling, None),
            ("Nonlinearity", "nonlinear", self.nonlinear, None),
            ("Damping", "damping", self.damping, None),
        ]

=== FILE: anttis_wave_mirror.py ===

"""
Antti's Wave Mirror - Learns an image, then evolves it.
Inspired by mirror.py
Place this file in the 'nodes' folder
"""

import numpy as np
from PyQt6 import QtGui
import cv2
import time

import sys
import os
# --- This is the new, correct block ---
import __main__
BaseNode = __main__.BaseNode
PA_INSTANCE = getattr(__main__, "PA_INSTANCE", None)
# ------------------------------------

class WaveNeuron:
    """Simplified WaveNeuron class from mirror.py"""
    def __init__(self, w, h):
        # WaveNeuron is designed for grayscale/single-channel data (w, h)
        self.frequency = np.random.uniform(0.1, 1.0, (h, w)).astype(np.float32)
        self.amplitude = np.random.uniform(0.5, 1.0, (h, w)).astype(np.float32)
        self.phase = np.random.uniform(0, 2 * np.pi, (h, w)).astype(np.float32)
        
    def activate(self, input_signal, t):
        # Vectorized activation
        return self.amplitude * np.sin(2 * np.pi * self.frequency * t + self.phase) + input_signal
        
    def train(self, target, t, learning_rate):
        # Target must be (h, w) shape to match output
        output = self.activate(0, t) # Get internal activation
        error = target - output
        
        sin_term = np.sin(2 * np.pi * self.frequency * t + self.phase)
        cos_term = np.cos(2 * np.pi * self.frequency * t + self.phase)
        
        self.amplitude += learning_rate * error * sin_term
        self.phase += learning_rate * error * self.amplitude * cos_term
        self.frequency += learning_rate * error * self.amplitude * (2 * np.pi * t) * cos_term
        
        # Clamp values to reasonable ranges
        self.amplitude = np.clip(self.amplitude, 0.1, 2.0)
        self.frequency = np.clip(self.frequency, 0.01, 2.0)

class WaveMirrorNode(BaseNode):
    NODE_CATEGORY = "Transform"
    NODE_COLOR = QtGui.QColor(60, 180, 160) # A teal/aqua color
    
    def __init__(self, width=80, height=60, training_duration=300):
        super().__init__()
        self.node_title = "Antti's Mirror"
        
        self.inputs = {'image_in': 'image'}
        self.outputs = {'image_out': 'image'}
        
        self.w, self.h = int(width), int(height)
        self.training_duration = int(training_duration)
        self.learning_rate = 0.01
        
        # Internal state
        self.wnn = WaveNeuron(self.w, self.h)
        self.output_image = np.zeros((self.h, self.w), dtype=np.float32)
        self.training_counter = 0
        self.start_time = time.time()
        self.is_trained = False

    def step(self):
        t = time.time() - self.start_time
        input_image = self.get_blended_input('image_in', 'mean')
        
        if input_image is None:
            input_image = np.zeros((self.h, self.w), dtype=np.float32)
        else:
            # 1. Resize the input
            input_image = cv2.resize(input_image, (self.w, self.h), interpolation=cv2.INTER_AREA)

            # 2. FIX: Convert to Grayscale if the input is color (ndim == 3)
            if input_image.ndim == 3:
                # Convert BGR/RGB to Grayscale (assuming input is float 0-1)
                input_image = cv2.cvtColor(input_image.astype(np.float32), cv2.COLOR_BGR2GRAY)
            
        if self.training_counter < self.training_duration:
            # --- Training Phase ---
            self.wnn.train(input_image, t, self.learning_rate)
            self.training_counter += 1
            # Show the input image while training
            self.output_image = input_image
            self.is_trained = False
        else:
            # --- Evolution Phase ---
            if not self.is_trained:
                self.is_trained = True
                print("WaveMirror: Training complete. Entering evolution phase.")
                
            # "Lives its own life" by using 0 as input
            input_signal = np.zeros((self.h, self.w), dtype=np.float32)
            self.output_image = self.wnn.activate(input_signal, t)

    def get_output(self, port_name):
        if port_name == 'image_out':
            # Normalize for output
            out = self.output_image - np.min(self.output_image)
            out_max = np.max(out)
            if out_max > 1e-6:
                out = out / out_max
            return out
        return None
        
    def get_display_image(self):
        # Display internal state
        out_img = self.get_output('image_out')
        if out_img is None:
            out_img = np.zeros((self.h, self.w), dtype=np.float32)
            
        img_u8 = (np.clip(out_img, 0, 1) * 255).astype(np.uint8)
        
        # Add status bar
        if not self.is_trained:
            status_color = (0, 255, 0) # Green for training
            progress = int((self.training_counter / self.training_duration) * self.w)
            cv2.rectangle(img_u8, (0, self.h - 5), (progress, self.h - 1), status_color, -1)
        else:
            status_color = (0, 0, 255) # Red for evolving
            cv2.rectangle(img_u8, (0, self.h - 5), (self.w - 1, self.h - 1), status_color, -1)

        img_u8 = np.ascontiguousarray(img_u8)
        return QtGui.QImage(img_u8.data, self.w, self.h, self.w, QtGui.QImage.Format.Format_Grayscale8)

    def get_config_options(self):
        return [
            ("Training Frames", "training_duration", self.training_duration, None)
        ]

=== FILE: box-a-count.py ===

"""
FilamentBoxcountNode

Extracts bright "filaments" from an image via thresholding,
displays them, and calculates their fractal dimension using
a box-counting algorithm.
"""

import numpy as np
import cv2

# --- Magic import block ---
import __main__
BaseNode = __main__.BaseNode
QtGui = __main__.QtGui
# --------------------------

class FilamentBoxcountNode(BaseNode):
    """
    Analyzes the fractal dimension of filaments in an image.
    """
    NODE_CATEGORY = "Analyzer"
    NODE_COLOR = QtGui.QColor(220, 180, 100) # Gold

    def __init__(self, size=128):
        super().__init__()
        self.node_title = "Filament Boxcounter"
        
        self.inputs = {
            'image_in': 'image',
            'threshold': 'signal' # 0-1, controls filament detection
        }
        self.outputs = {
            'image': 'image',         # The binary filament image
            'fractal_dim': 'signal',  # The calculated fractal dimension (1.0 - 2.0)
            'density': 'signal'       # How many pixels are "on" (0-1)
        }
        
        # Box counting is SLOW on large images.
        # We process a downscaled version for speed.
        self.size = int(size) 
        
        self.display_image = np.zeros((self.size, self.size, 3), dtype=np.float32)
        self.fractal_dim = 1.0
        self.density = 0.0

    def _box_count(self, binary_img):
        """
        Performs a box-counting algorithm on a binary image.
        Uses a fast method optimized for sparse pixels.
        """
        # Find the coordinates of all "on" pixels
        pixels = np.argwhere(binary_img > 0)
        
        if len(pixels) == 0:
            return 1.0 # No dimension if no pixels

        # Use 8 scales, from 2 up to size/2
        max_log = np.log2(self.size // 2)
        scales = np.logspace(1.0, max_log, num=8, base=2)
        scales = np.unique(np.round(scales).astype(int))
        
        counts = []
        valid_scales = []
        
        for scale in scales:
            if scale < 2: continue
            
            # Use a set to store unique box indices
            # This is much faster than iterating over a full grid
            box_indices = set()
            for y, x in pixels:
                box_indices.add( (y // scale, x // scale) )
            
            # We must have at least one box to count
            if len(box_indices) > 0:
                counts.append(len(box_indices))
                valid_scales.append(scale)
        
        if len(counts) < 2:
            return 1.0 # Not enough data to fit a line

        # Fit a line to log(counts) vs log(scales)
        # The fractal dimension D is the *negative* slope.
        # N(s) â s^(-D)  =>  log(N) = -D * log(s) + C
        try:
            coeffs = np.polyfit(np.log(valid_scales), np.log(counts), 1)
            dimension = -coeffs[0]
        except np.linalg.LinAlgError:
            dimension = 1.0 # Fitting failed
        
        # A 2D fractal dimension must be between 1 (a line) and 2 (a filled plane)
        return np.clip(dimension, 1.0, 2.0)

    def step(self):
        # --- 1. Get and Prepare Image ---
        img = self.get_blended_input('image_in', 'first')
        if img is None:
            return # Do nothing if no image

        # Resize for performance
        img_resized = cv2.resize(img, (self.size, self.size), 
                                 interpolation=cv2.INTER_LINEAR)
        
        # Convert to grayscale
        if img_resized.ndim == 3:
            img_gray = cv2.cvtColor(img_resized, cv2.COLOR_RGB2GRAY)
        else:
            img_gray = img_resized
            
        # Ensure 0-1 float
        if img_gray.max() > 1.0:
            img_gray = img_gray.astype(np.float32) / 255.0
        
        # --- 2. Extract Filaments ---
        threshold = self.get_blended_input('threshold', 'sum') or 0.5
        
        # Apply threshold to get the binary image
        _ , binary_img = cv2.threshold(
            (img_gray * 255).astype(np.uint8), 
            int(threshold * 255), 
            255, 
            cv2.THRESH_BINARY
        )
        
        # --- 3. Analyze ---
        self.fractal_dim = self._box_count(binary_img)
        self.density = np.sum(binary_img > 0) / binary_img.size
        
        # --- 4. Prepare Display ---
        # Convert the B/W filament image to color for display
        self.display_image = cv2.cvtColor(binary_img, cv2.COLOR_GRAY2RGB)
        self.display_image = self.display_image.astype(np.float32) / 255.0

    def get_output(self, port_name):
        if port_name == 'image':
            return self.display_image
        elif port_name == 'fractal_dim':
            return self.fractal_dim
        elif port_name == 'density':
            return self.density
        return None

=== FILE: brainlobesnode.py ===

"""
Brain Lobes Node - Phase-lobes hypothesis demonstration
Shows frequency separation across brain regions

Place this in nodes/ folder
"""

import numpy as np
from PyQt6 import QtGui
import cv2

import __main__
BaseNode = __main__.BaseNode
PA_INSTANCE = getattr(__main__, "PA_INSTANCE", None)
QtGui = __main__.QtGui

try:
    from scipy.fft import rfft, irfft, rfftfreq
    SCIPY_AVAILABLE = True
except ImportError:
    SCIPY_AVAILABLE = False

class BrainLobesNode(BaseNode):
    NODE_CATEGORY = "Analysis"
    NODE_COLOR = QtGui.QColor(180, 100, 200)
    
    def __init__(self, field_size=512, damage_lobe='None'):
        super().__init__()
        self.node_title = "Brain Lobes"
        
        self.inputs = {
            'external_field': 'signal',
            'damage_amount': 'signal',
        }
        
        self.outputs = {
            'frontal_output': 'signal',
            'parietal_output': 'signal',
            'temporal_output': 'signal',
            'occipital_output': 'signal',
            'integrated_experience': 'signal',
            'cross_frequency_leakage': 'signal',
            'lobe_spectrum_image': 'image',
        }
        
        if not SCIPY_AVAILABLE:
            self.node_title = "Brain Lobes (No SciPy!)"
            return
        
        self.field_size = int(field_size)
        self.damage_lobe = damage_lobe
        self.fs = 1000.0
        
        self.history = np.zeros(field_size, dtype=np.float32)
        self.W_lobes = {}
        self._init_filters()
        
        self.lobe_outputs = {'frontal': 0.0, 'parietal': 0.0, 'temporal': 0.0, 'occipital': 0.0}
        self.integrated_output = 0.0
        self.leakage_metric = 0.0
        self.last_spectra = {lobe: None for lobe in self.lobe_outputs.keys()}
        
    def _init_filters(self):
        freqs = rfftfreq(self.field_size, 1.0/self.fs)
        
        # Frontal: Theta (4-8 Hz)
        W_frontal = np.zeros_like(freqs)
        mask = (freqs >= 4.0) & (freqs <= 8.0)
        W_frontal[mask] = 1.0
        self.W_lobes['frontal'] = self._smooth(W_frontal, freqs, 4.0, 8.0)
        
        # Parietal: Alpha (8-13 Hz)
        W_parietal = np.zeros_like(freqs)
        mask = (freqs >= 8.0) & (freqs <= 13.0)
        W_parietal[mask] = 1.0
        self.W_lobes['parietal'] = self._smooth(W_parietal, freqs, 8.0, 13.0)
        
        # Temporal: Gamma (30-100 Hz)
        W_temporal = np.zeros_like(freqs)
        mask = (freqs >= 30.0) & (freqs <= 100.0)
        W_temporal[mask] = 1.0
        self.W_lobes['temporal'] = self._smooth(W_temporal, freqs, 30.0, 100.0)
        
        # Occipital: Beta-Gamma (13-100 Hz)
        W_occipital = np.zeros_like(freqs)
        mask = (freqs >= 13.0) & (freqs <= 100.0)
        W_occipital[mask] = 1.0
        self.W_lobes['occipital'] = self._smooth(W_occipital, freqs, 13.0, 100.0)
        
    def _smooth(self, W, freqs, low, high, width=3.0):
        for i, f in enumerate(freqs):
            if f < low:
                W[i] = np.exp(-((low - f)**2) / (2 * width**2))
            elif f > high:
                W[i] = np.exp(-((f - high)**2) / (2 * width**2))
        return W
    
    def _filter_lobe(self, signal, lobe_name, damage=0.0):
        F = rfft(signal)
        W = self.W_lobes[lobe_name].copy()
        
        if damage > 0.0:
            noise = np.random.randn(len(W)) * damage * 0.3
            W = W * (1.0 - damage * 0.5) + np.abs(noise)
            W = np.clip(W, 0, 1)
        
        W = W[:len(F)]
        F_filtered = F * W
        signal_filtered = irfft(F_filtered, n=len(signal))
        
        return signal_filtered, F, F_filtered
    
    def _compute_leakage(self):
        if self.last_spectra['frontal'] is None:
            return 0.0
        
        freqs = rfftfreq(self.field_size, 1.0/self.fs)
        frontal_spectrum = np.abs(self.last_spectra['frontal'])
        high_freq_mask = freqs > 20.0
        
        if len(frontal_spectrum) >= len(high_freq_mask):
            high_freq_mask = high_freq_mask[:len(frontal_spectrum)]
            contamination = np.sum(frontal_spectrum * high_freq_mask)
            total = np.sum(frontal_spectrum) + 1e-9
            leakage = contamination / total
        else:
            leakage = 0.0
        
        return float(np.clip(leakage, 0, 1))
    
    def step(self):
        if not SCIPY_AVAILABLE:
            return
        
        external = self.get_blended_input('external_field', 'sum') or 0.0
        damage_signal = self.get_blended_input('damage_amount', 'sum') or 0.0
        damage_amount = np.clip((damage_signal + 1.0) / 2.0, 0, 1)
        
        self.history[:-1] = self.history[1:]
        self.history[-1] = external
        
        for lobe_name in ['frontal', 'parietal', 'temporal', 'occipital']:
            lobe_damage = damage_amount if self.damage_lobe == lobe_name else 0.0
            filtered, F_orig, F_filtered = self._filter_lobe(self.history, lobe_name, lobe_damage)
            self.lobe_outputs[lobe_name] = filtered[-1]
            self.last_spectra[lobe_name] = F_filtered
        
        self.integrated_output = (
            self.lobe_outputs['frontal'] * 0.3 +
            self.lobe_outputs['parietal'] * 0.25 +
            self.lobe_outputs['temporal'] * 0.25 +
            self.lobe_outputs['occipital'] * 0.2
        )
        
        self.leakage_metric = self._compute_leakage()
    
    def get_output(self, port_name):
        if port_name == 'frontal_output':
            return self.lobe_outputs['frontal']
        elif port_name == 'parietal_output':
            return self.lobe_outputs['parietal']
        elif port_name == 'temporal_output':
            return self.lobe_outputs['temporal']
        elif port_name == 'occipital_output':
            return self.lobe_outputs['occipital']
        elif port_name == 'integrated_experience':
            return self.integrated_output
        elif port_name == 'cross_frequency_leakage':
            return self.leakage_metric
        elif port_name == 'lobe_spectrum_image':
            return self._gen_spectrum_image()
        return None
    
    def _gen_spectrum_image(self):
        h, w = 128, 256
        img = np.zeros((h, w), dtype=np.float32)
        
        if self.last_spectra['frontal'] is None:
            return img
        
        band_h = h // 4
        lobe_names = ['frontal', 'parietal', 'temporal', 'occipital']
        colors = [0.3, 0.5, 0.7, 0.9]
        
        for i, lobe_name in enumerate(lobe_names):
            spectrum = np.abs(self.last_spectra[lobe_name])
            max_val = np.max(spectrum) + 1e-9
            spectrum_norm = spectrum / max_val
            
            if len(spectrum_norm) > w:
                indices = np.linspace(0, len(spectrum_norm)-1, w).astype(int)
                spectrum_norm = spectrum_norm[indices]
            
            y_start = i * band_h
            y_end = (i + 1) * band_h
            
            for x in range(min(len(spectrum_norm), w)):
                height = int(spectrum_norm[x] * band_h * 0.8)
                if height > 0:
                    y_bottom = y_end - 2
                    y_top = max(y_start, y_bottom - height)
                    img[y_top:y_bottom, x] = colors[i]
            
            if i < 3:
                img[y_end-1:y_end+1, :] = 0.2
        
        return img
    
    def get_display_image(self):
        if not SCIPY_AVAILABLE:
            return None
        
        # Create larger, more graphical display
        h, w = 256, 384
        img = np.zeros((h, w, 3), dtype=np.uint8)
        
        # Define lobe colors (RGB) - distinct brain regions
        lobe_colors = {
            'frontal': (180, 100, 255),    # Purple
            'parietal': (100, 180, 255),   # Blue
            'temporal': (100, 255, 180),   # Cyan/Green
            'occipital': (255, 180, 100)   # Orange
        }
        
        lobe_names = ['frontal', 'parietal', 'temporal', 'occipital']
        labels = ['FRONTAL', 'PARIETAL', 'TEMPORAL', 'OCCIPITAL']
        freq_ranges = ['4-8 Hz', '8-13 Hz', '30-100 Hz', '13-100 Hz']
        
        band_h = h // 4
        
        # Draw each lobe as a colored region
        for i, lobe_name in enumerate(lobe_names):
            y_start = i * band_h
            y_end = (i + 1) * band_h
            
            base_color = lobe_colors[lobe_name]
            
            # Check if damaged
            if self.damage_lobe == lobe_name:
                # Damaged: red tint + noise pattern
                base_color = (80, 80, 200)  # Reddish
                # Add damage pattern
                noise = np.random.randint(0, 30, (band_h, w, 3), dtype=np.uint8)
                img[y_start:y_end] = noise
                # Add red overlay
                overlay = np.zeros((band_h, w, 3), dtype=np.uint8)
                overlay[:, :] = (0, 0, 180)
                img[y_start:y_end] = cv2.addWeighted(img[y_start:y_end], 0.5, overlay, 0.5, 0)
            else:
                # Healthy: solid color with activity pattern
                img[y_start:y_end] = base_color
            
            # Get lobe activity (spectrum energy)
            if self.last_spectra[lobe_name] is not None:
                spectrum = np.abs(self.last_spectra[lobe_name])
                energy = np.sum(spectrum) / len(spectrum)
                energy = np.clip(energy * 100, 0, 1)
                
                # Activity bar on left side
                bar_width = 20
                bar_height = int(energy * (band_h - 10))
                if bar_height > 0:
                    cv2.rectangle(img, 
                                (5, y_end - 5 - bar_height), 
                                (5 + bar_width, y_end - 5),
                                (255, 255, 0), -1)  # Yellow activity bar
            
            # Draw border
            cv2.rectangle(img, (0, y_start), (w-1, y_end-1), (60, 60, 60), 2)
            
            # Draw labels
            font = cv2.FONT_HERSHEY_SIMPLEX
            label_y = y_start + band_h // 2 - 5
            
            # Lobe name (large)
            label_text = labels[i]
            if self.damage_lobe == lobe_name:
                label_text += " [DMG]"
                text_color = (0, 0, 255)  # Red
            else:
                text_color = (255, 255, 255)  # White
            
            # Draw text with shadow
            cv2.putText(img, label_text, (35, label_y), font, 0.5, (0, 0, 0), 3, cv2.LINE_AA)
            cv2.putText(img, label_text, (35, label_y), font, 0.5, text_color, 1, cv2.LINE_AA)
            
            # Frequency range (small)
            cv2.putText(img, freq_ranges[i], (35, label_y + 20), font, 0.35, (200, 200, 200), 1, cv2.LINE_AA)
            
            # Draw mini spectrum visualization on right side
            if self.last_spectra[lobe_name] is not None:
                spectrum = np.abs(self.last_spectra[lobe_name])
                spectrum_norm = spectrum / (np.max(spectrum) + 1e-9)
                
                # Draw small spectrum graph
                spec_w = 100
                spec_h = band_h - 20
                spec_x = w - spec_w - 10
                spec_y = y_start + 10
                
                # Downsample spectrum
                if len(spectrum_norm) > spec_w:
                    indices = np.linspace(0, len(spectrum_norm)-1, spec_w).astype(int)
                    spectrum_norm = spectrum_norm[indices]
                
                # Draw spectrum as bars
                for x in range(min(len(spectrum_norm), spec_w)):
                    bar_h = int(spectrum_norm[x] * spec_h)
                    if bar_h > 0:
                        cv2.line(img, 
                               (spec_x + x, spec_y + spec_h), 
                               (spec_x + x, spec_y + spec_h - bar_h),
                               (255, 255, 100), 1)
        
        # Draw cross-frequency leakage indicator
        if self.leakage_metric > 0.05:
            # Big red warning in top-right
            warning_text = f"LEAKAGE: {self.leakage_metric*100:.1f}%"
            cv2.putText(img, warning_text, (w - 180, 20), font, 0.4, (0, 0, 0), 3, cv2.LINE_AA)
            cv2.putText(img, warning_text, (w - 180, 20), font, 0.4, (0, 0, 255), 1, cv2.LINE_AA)
            
            # Draw connecting lines showing leakage between lobes
            if self.leakage_metric > 0.2:
                # Draw red connecting line from temporal to frontal
                cv2.line(img, (w//2, band_h * 2 + band_h//2), (w//2, band_h//2), 
                        (0, 0, 255), int(self.leakage_metric * 5), cv2.LINE_AA)
        
        img = np.ascontiguousarray(img)
        return QtGui.QImage(img.data, w, h, 3*w, QtGui.QImage.Format.Format_RGB888)
    
    def get_config_options(self):
        return [
            ("Damage Lobe", "damage_lobe", self.damage_lobe, [
                ("None (Healthy)", "None"),
                ("Frontal (Theta)", "frontal"),
                ("Parietal (Alpha)", "parietal"),
                ("Temporal (Gamma)", "temporal"),
                ("Occipital (Beta-Gamma)", "occipital")
            ]),
            ("Field Size", "field_size", self.field_size, None),
        ]

=== FILE: chaoticcontrolnode.py ===

"""
Chaotic Control Node - Simulates the Lorenz Attractor, a classic chaotic system.
It includes an input port ('control_nudge') to subtly influence the chaotic evolution,
testing if external signals can control the attractor's trajectory.
Ported from chaos_control_simulator (1).html.
"""

import numpy as np
from PyQt6 import QtGui
import cv2

import __main__
BaseNode = __main__.BaseNode
PA_INSTANCE = getattr(__main__, "PA_INSTANCE", None)
QtGui = __main__.QtGui

class ChaoticControlNode(BaseNode):
    NODE_CATEGORY = "Source"
    NODE_COLOR = QtGui.QColor(150, 50, 50) # Chaotic Red
    
    def __init__(self, dt=0.01):
        super().__init__()
        self.node_title = "Chaotic Control (Lorenz)"
        
        self.inputs = {
            'control_nudge': 'signal',   # Input signal to influence the system
            'reset': 'signal'
        }
        self.outputs = {
            'chaos_x': 'signal',
            'chaos_y': 'signal',
            'phase_image': 'image',
        }
        
        # Lorenz Attractor parameters (standard values)
        self.sigma = 10.0
        self.rho = 28.0
        self.beta = 8/3
        self.dt = float(dt)
        
        # System state (X, Y, Z)
        self.state = np.array([1.0, 1.0, 1.0], dtype=np.float64)
        
        # History for Phase Space Plot (X vs Y)
        self.history_len = 1000
        self.history_x = np.zeros(self.history_len, dtype=np.float64)
        self.history_y = np.zeros(self.history_len, dtype=np.float64)
        
        self.output_x = 0.0
        self.output_y = 0.0

    def _lorenz_derivative(self, state, nudge):
        """Lorenz system derivative with external nudge applied to dx/dt"""
        x, y, z = state
        sigma, rho, beta = self.sigma, self.rho, self.beta
        
        dx_dt = sigma * (y - x) + nudge # <--- CONTROL POINT
        dy_dt = x * (rho - z) - y
        dz_dt = x * y - beta * z
        
        return np.array([dx_dt, dy_dt, dz_dt])

    def _runge_kutta_4(self, state, nudge):
        """Standard RK4 numerical integration for the Lorenz system"""
        
        k1 = self._lorenz_derivative(state, nudge)
        
        state2 = state + 0.5 * self.dt * k1
        k2 = self._lorenz_derivative(state2, nudge)
        
        state3 = state + 0.5 * self.dt * k2
        k3 = self._lorenz_derivative(state3, nudge)
        
        state4 = state + self.dt * k3
        k4 = self._lorenz_derivative(state4, nudge)
        
        return state + (self.dt / 6) * (k1 + 2*k2 + 2*k3 + k4)

    def randomize(self):
        """Reset the system state to initial chaotic values"""
        self.state = np.array([1.0, 1.0, 1.0], dtype=np.float64)
        self.history_x.fill(0.0)
        self.history_y.fill(0.0)
        
    def step(self):
        # 1. Get inputs
        control_nudge_in = self.get_blended_input('control_nudge', 'sum') or 0.0
        reset_sig = self.get_blended_input('reset', 'sum')
        
        if reset_sig is not None and reset_sig > 0.5:
            self.randomize()
            return

        # Map input signal [-1, 1] to a subtle control range [-0.5, 0.5]
        nudge_force = control_nudge_in * 0.5 
        
        # 2. Integrate the system
        self.state = self._runge_kutta_4(self.state, nudge_force)
        
        # 3. Update outputs and history
        self.output_x = self.state[0]
        self.output_y = self.state[1]
        
        self.history_x[:-1] = self.history_x[1:]
        self.history_x[-1] = self.output_x
        
        self.history_y[:-1] = self.history_y[1:]
        self.history_y[-1] = self.output_y

    def get_output(self, port_name):
        if port_name == 'chaos_x':
            return self.output_x
        elif port_name == 'chaos_y':
            return self.output_y
        elif port_name == 'phase_image':
            # This output is generated in get_display_image for efficiency
            # We return a dummy value so the port is active, or use get_display_image directly
            return np.zeros((64, 64), dtype=np.float32) 
        return None
        
    def get_display_image(self):
        w, h = 96, 96
        img = np.zeros((h, w), dtype=np.uint8)
        
        if self.history_x.max() == 0:
            return QtGui.QImage(img.data, w, h, w, QtGui.QImage.Format.Format_Grayscale8)

        # 1. Normalize and Scale History for Plotting
        # Find bounds for scaling
        min_val_x, max_val_x = self.history_x.min(), self.history_x.max()
        range_x = max_val_x - min_val_x
        
        min_val_y, max_val_y = self.history_y.min(), self.history_y.max()
        range_y = max_val_y - min_val_y

        # Define plotting area margins
        margin = 8
        scale_x = (w - 2 * margin) / (range_x + 1e-9)
        scale_y = (h - 2 * margin) / (range_y + 1e-9)

        # Map trajectory points to screen coordinates
        x_coords = ((self.history_x - min_val_x) * scale_x + margin).astype(int)
        # Flip Y-axis (top is 0)
        y_coords = (h - margin - (self.history_y - min_val_y) * scale_y).astype(int)
        
        # 2. Draw Trajectory (X vs Y Phase Space)
        for i in range(1, self.history_len):
            pt1 = (x_coords[i-1], y_coords[i-1])
            pt2 = (x_coords[i], y_coords[i])
            
            # Draw faded line
            color = 50 + int(i / self.history_len * 200)
            cv2.line(img, pt1, pt2, color, 1)

        # 3. Draw current point (Attractor)
        if self.history_len > 0:
            current_pt = (x_coords[-1], y_coords[-1])
            cv2.circle(img, current_pt, 2, 255, -1)
            
        img = np.ascontiguousarray(img)
        return QtGui.QImage(img.data, w, h, w, QtGui.QImage.Format.Format_Grayscale8)

    def get_config_options(self):
        return [
            ("Integration dt", "dt", self.dt, None),
            ("Sigma (Ï)", "sigma", self.sigma, None),
            ("Rho (Ï)", "rho", self.rho, None),
            ("Beta (Î²)", "beta", self.beta, None),
        ]

=== FILE: checkerboardnode.py ===

"""
CheckerboardNode

Generates a simple checkerboard texture.
"""

import numpy as np
import cv2

# --- Magic import block ---
import __main__
BaseNode = __main__.BaseNode
QtGui = __main__.QtGui
# --------------------------

class CheckerboardNode(BaseNode):
    """
    Generates a checkerboard texture.
    """
    NODE_CATEGORY = "Generator"
    NODE_COLOR = QtGui.QColor(200, 200, 200) # Gray

    def __init__(self, size=256):
        super().__init__()
        self.node_title = "Checkerboard"
        
        self.inputs = {
            'square_size': 'signal' # 0-1, size of the squares
        }
        self.outputs = {'image': 'image'}
        
        self.size = int(size)
        self.display_image = np.zeros((self.size, self.size, 3), dtype=np.float32)

    def step(self):
        # 1. Get Controls
        size_in = self.get_blended_input('square_size', 'sum') or 0.1
        square_size = int(5 + size_in * 50) # 5px to 55px
        
        # 2. Generate Grid
        y, x = np.mgrid[0:self.size, 0:self.size]
        
        # 3. Create Checkerboard
        check_pattern = ((x // square_size) + (y // square_size)) % 2
        
        self.display_image = np.stack([check_pattern] * 3, axis=-1).astype(np.float32)
        
    def get_output(self, port_name):
        if port_name == 'image':
            return self.display_image
        return None

=== FILE: circulationfieldnode.py ===

"""
CirculationFieldNode

Generates the "Circulation medium" (spacetime) as a
2D vector field based on Perlin noise.

[FIXED-v2] Replaced buggy .repeat() logic with cv2.resize()
to fix broadcasting error when size is not divisible by res.
"""

import numpy as np
import cv2

# --- Magic import block ---
import __main__
BaseNode = __main__.BaseNode
QtGui = __main__.QtGui
# --------------------------

class CirculationFieldNode(BaseNode):
    """
    Generates a 2D vector field representing the "Circulation medium"
    """
    NODE_CATEGORY = "Generator"
    NODE_COLOR = QtGui.QColor(100, 150, 220) # Blue

    def __init__(self, size=128):
        super().__init__()
        self.node_title = "Circulation Field"
        
        self.inputs = {
            'speed': 'signal',   # How fast the field evolves
            'scale': 'signal',   # Zoom level of the field
            'strength': 'signal' # Magnitude of the vectors
        }
        self.outputs = {
            'vector_field': 'image',  # Raw [vx, vy, 0] data
            'field_viz': 'image'      # Human-readable visualization
        }
        
        self.size = int(size)
        self.z_offset = 0.0 # Time dimension for 3D noise
        
        # We need two noise fields, one for X and one for Y
        self.noise_res = (8, 8)
        self.noise_seed_x = np.random.rand(self.noise_res[0]+1, self.noise_res[1]+1)
        self.noise_seed_y = np.random.rand(self.noise_res[0]+1, self.noise_res[1]+1)
        
        # Initialize output arrays to prevent AttributeError on first frame
        self.vx = np.zeros((self.size, self.size), dtype=np.float32)
        self.vy = np.zeros((self.size, self.size), dtype=np.float32)
        self.viz = np.zeros((self.size, self.size, 3), dtype=np.float32)

    def _generate_noise_slice(self, seed):
        """
        Generates a 2D slice of Perlin-like noise.
        [FIXED] This version uses cv2.resize for robust interpolation.
        """
        # --- Smooth interpolation function ---
        def f(t):
            return 6*t**5 - 15*t**4 + 10*t**3

        # --- 1. Get base parameters ---
        res = self.noise_res
        shape = (self.size, self.size)
        
        # --- 2. Create gradient angles ---
        # (Using z_offset for 3D time-varying noise)
        angles = 2*np.pi * (seed + self.z_offset)
        gradients = np.dstack((np.cos(angles), np.sin(angles)))
        
        # --- 3. Create coordinate grid ---
        # This grid is (size, size, 2) and goes from [0, res]
        delta = (res[0] / shape[0], res[1] / shape[1])
        grid = np.mgrid[0:res[0]:delta[0], 0:res[1]:delta[1]].transpose(1, 2, 0) % 1
        
        # --- 4. Get corner gradients ---
        # [FIX] Use cv2.resize(..., interpolation=cv2.INTER_NEAREST)
        # This replaces the buggy .repeat(d[0], 0).repeat(d[1], 1) logic
        # dsize is (w, h), which corresponds to (shape[1], shape[0])
        dsize = (shape[1], shape[0]) 
        
        g00 = cv2.resize(gradients[0:-1, 0:-1], dsize, interpolation=cv2.INTER_NEAREST)
        g10 = cv2.resize(gradients[1:  , 0:-1], dsize, interpolation=cv2.INTER_NEAREST)
        g01 = cv2.resize(gradients[0:-1, 1:  ], dsize, interpolation=cv2.INTER_NEAREST)
        g11 = cv2.resize(gradients[1:  , 1:  ], dsize, interpolation=cv2.INTER_NEAREST)

        # --- 5. Calculate dot products (ramps) ---
        # All arrays (grid, g00, g10, g01, g11) are now guaranteed
        # to be (size, size, 2), so this math is safe.
        n00 = np.sum(np.dstack((grid[:,:,0]  , grid[:,:,1]  )) * g00, 2)
        n10 = np.sum(np.dstack((grid[:,:,0]-1, grid[:,:,1]  )) * g10, 2)
        n01 = np.sum(np.dstack((grid[:,:,0]  , grid[:,:,1]-1)) * g01, 2)
        n11 = np.sum(np.dstack((grid[:,:,0]-1, grid[:,:,1]-1)) * g11, 2)
        
        # --- 6. Interpolate ---
        t = f(grid) # Apply smoothstep to the grid
        
        n0 = n00*(1-t[:,:,0]) + t[:,:,0]*n10
        n1 = n01*(1-t[:,:,0]) + t[:,:,0]*n11
        
        # Final result is (size, size)
        return np.sqrt(2)*((1-t[:,:,1])*n0 + t[:,:,1]*n1)

    def step(self):
        # --- 1. Get Controls ---
        speed = self.get_blended_input('speed', 'sum') or 0.1
        scale = self.get_blended_input('scale', 'sum') or 1.0
        strength = self.get_blended_input('strength', 'sum') or 1.0
        
        self.z_offset += speed * 0.05
        
        # --- 2. Generate Vector Field ---
        # Map scale to noise resolution
        res_val = int(4 + scale * 12)
        self.noise_res = (res_val, res_val)
        
        # Ensure seeds match new resolution
        if self.noise_seed_x.shape[0] != self.noise_res[0] + 1:
            self.noise_seed_x = np.random.rand(self.noise_res[0]+1, self.noise_res[1]+1)
            self.noise_seed_y = np.random.rand(self.noise_res[0]+1, self.noise_res[1]+1)

        # Generate noise maps for X and Y velocities
        # Result is in [-1, 1] range
        self.vx = self._generate_noise_slice(self.noise_seed_x) * strength
        self.vy = self._generate_noise_slice(self.noise_seed_y) * strength
        
        # --- 3. Create Visualization ---
        self.viz = np.zeros((self.size, self.size, 3), dtype=np.float32)
        step = 10
        for y in range(0, self.size, step):
            for x in range(0, self.size, step):
                vx = self.vx[y, x] * 5 # Scale for viz
                vy = self.vy[y, x] * 5
                
                pt1 = (x, y)
                pt2 = (int(x + vx), int(y + vy))
                
                # Clip points to be inside the image
                pt1 = (np.clip(pt1[0], 0, self.size-1), np.clip(pt1[1], 0, self.size-1))
                pt2 = (np.clip(pt2[0], 0, self.size-1), np.clip(pt2[1], 0, self.size-1))
                
                cv2.arrowedLine(self.viz, pt1, pt2, (1,1,1), 1, cv2.LINE_AA)

    def get_output(self, port_name):
        if port_name == 'vector_field':
            # Output as [vx, vy, 0] image in [-1, 1] range
            # We map this to [0, 1] for image compatibility
            # R = (vx+1)/2, G = (vy+1)/2, B = 0
            field_img = np.dstack([
                (self.vx + 1.0) / 2.0, 
                (self.vy + 1.0) / 2.0, 
                np.zeros((self.size, self.size))
            ])
            return field_img.astype(np.float32)
            
        elif port_name == 'field_viz':
            return self.viz
            
        return None

    def get_display_image(self):
        # We need to return a QImage, but numpy_to_qimage is in the host
        # A simple fix is to just return the float array and let the host handle it
        return self.viz

=== FILE: circulatoranalyzernode.py ===

"""
CirculationAnalyzerNode

Analyzes the "total circulation cost" of a vector field
by calculating its 2D curl (vorticity).
"""

import numpy as np
import cv2

# --- Magic import block ---
import __main__
BaseNode = __main__.BaseNode
QtGui = __main__.QtGui
# --------------------------

class CirculationAnalyzerNode(BaseNode):
    """
    Calculates the 2D curl (vorticity) of an input vector field.
    """
    NODE_CATEGORY = "Analyzer"
    NODE_COLOR = QtGui.QColor(220, 100, 100) # Red

    def __init__(self, size=64):
        super().__init__()
        self.node_title = "Circulation Analyzer"
        
        self.inputs = {
            'vector_field_in': 'image' # From CirculationFieldNode
        }
        self.outputs = {
            'total_circulation': 'signal', # "Total circulation cost"
            'vorticity_map': 'image'       # Visualization of curl
        }
        
        self.size = int(size)
        
        # Internal state
        self.total_circulation = 0.0
        self.vorticity_map = np.zeros((self.size, self.size, 3), dtype=np.float32)

    def step(self):
        # --- 1. Get and Prepare Field ---
        field = self.get_blended_input('vector_field_in', 'first')
        if field is None:
            return

        # Ensure float32
        if field.dtype != np.float32:
            field = field.astype(np.float32)
        if field.max() > 1.0: # (Assumes 0-255 if not 0-1)
            field = field / 255.0
            
        field_resized = cv2.resize(field, (self.size, self.size), 
                                   interpolation=cv2.INTER_LINEAR)
        
        # Convert from [0, 1] (R,G) to [-1, 1] (vx, vy)
        vx = (field_resized[..., 0] * 2.0) - 1.0
        vy = (field_resized[..., 1] * 2.0) - 1.0
        
        # --- 2. Calculate Vorticity (Curl) ---
        # curl(F) = (dVy/dx - dVx/dy)
        
        # Must use CV_32F to handle negative numbers
        dvx_dy = cv2.Sobel(vx, cv2.CV_32F, 0, 1, ksize=3)
        dvy_dx = cv2.Sobel(vy, cv2.CV_32F, 1, 0, ksize=3)
        
        curl = dvy_dx - dvx_dy
        
        # --- 3. Calculate Outputs ---
        
        # "Total circulation cost" = average absolute vorticity
        self.total_circulation = np.mean(np.abs(curl))
        
        # --- 4. Create Visualization ---
        # Normalize curl from [-max, +max] to [0, 1]
        max_curl = np.max(np.abs(curl))
        if max_curl == 0:
            norm_curl = np.zeros((self.size, self.size), dtype=np.float32)
        else:
            norm_curl = (curl + max_curl) / (2 * max_curl)
        
        img_u8 = (norm_curl * 255).astype(np.uint8)
        self.vorticity_map = cv2.applyColorMap(img_u8, cv2.COLORMAP_BONE)
        self.vorticity_map = self.vorticity_map.astype(np.float32) / 255.0

    def get_output(self, port_name):
        if port_name == 'total_circulation':
            return self.total_circulation
        elif port_name == 'vorticity_map':
            return self.vorticity_map
        return None

    def get_display_image(self):
        return self.vorticity_map

=== FILE: circulatorswarmnode.py ===

"""
CirculatorSwarmNode

Simulates "bits" (Circulators) moving through the
Circulation Field. Implements particle advection and
collision/interaction.
"""

import numpy as np
import cv2

# --- Magic import block ---
import __main__
BaseNode = __main__.BaseNode
QtGui = __main__.QtGui
# --------------------------

class CirculatorSwarmNode(BaseNode):
    """
    Moves particles (Circulators) along an input vector field.
    """
    NODE_CATEGORY = "Generator"
    NODE_COLOR = QtGui.QColor(220, 180, 100) # Gold

    def __init__(self, size=256, particle_count=300):
        super().__init__()
        self.node_title = "Circulator Swarm"
        
        self.inputs = {
            'vector_field_in': 'image', # From CirculationFieldNode
            'repulsion': 'signal',      # 0-1, strength of collisions
            'damping': 'signal'         # 0-1, how much to follow field
        }
        self.outputs = {'image': 'image'}
        
        self.size = int(size)
        self.particle_count = int(particle_count)
        
        # Initialize particles
        self.positions = np.random.rand(self.particle_count, 2) * self.size
        self.velocities = (np.random.rand(self.particle_count, 2) - 0.5) * 2.0
        
        # Fading trail buffer
        self.trail_buffer = np.zeros((self.size, self.size, 3), dtype=np.float32)

    def _prepare_field(self, img):
        """Helper to resize and format the vector field."""
        if img is None:
            return np.zeros((self.size, self.size, 2), dtype=np.float32)
        
        # Ensure float32
        if img.dtype != np.float32:
            img = img.astype(np.float32)
        if img.max() > 1.0: # (Assumes 0-255 if not 0-1)
            img = img / 255.0
            
        img_resized = cv2.resize(img, (self.size, self.size), 
                                 interpolation=cv2.INTER_LINEAR)
        
        # Convert from [0, 1] (R,G) to [-1, 1] (vx, vy)
        vx = (img_resized[..., 0] * 2.0) - 1.0
        vy = (img_resized[..., 1] * 2.0) - 1.0
        
        return np.dstack([vx, vy])

    def step(self):
        # --- 1. Get Inputs ---
        vector_field = self._prepare_field(self.get_blended_input('vector_field_in', 'first'))
        repulsion = (self.get_blended_input('repulsion', 'sum') or 0.1) * 20.0
        damping = 1.0 - (self.get_blended_input('damping', 'sum') or 0.1) # 0.9 to 1.0
        
        # --- 2. Update Particle Velocities ---
        
        # a) Get field velocity at each particle's position
        int_pos = self.positions.astype(int)
        px = np.clip(int_pos[:, 0], 0, self.size - 1)
        py = np.clip(int_pos[:, 1], 0, self.size - 1)
        
        field_velocities = vector_field[py, px] # (N, 2) array
        
        # b) Apply damping (follow the field)
        self.velocities = self.velocities * damping + field_velocities * (1.0 - damping)
        
        # c) Apply collisions ("Interactions")
        if repulsion > 0:
            for i in range(self.particle_count):
                # Vectorized repulsion (broadcasting)
                diffs = self.positions[i] - self.positions
                dists_sq = np.sum(diffs**2, axis=1)
                
                # Avoid self-repulsion and divide-by-zero
                dists_sq[i] = np.inf 
                dists_sq[dists_sq < 1] = 1 # Min distance
                
                # Force = 1/r^2
                repel_force = repulsion * diffs / dists_sq[:, np.newaxis]
                
                # Sum forces from all other particles
                self.velocities[i] += np.sum(repel_force, axis=0)
        
        # Clamp velocity
        self.velocities = np.clip(self.velocities, -5.0, 5.0)
        
        # --- 3. Update Positions ---
        self.positions += self.velocities
        
        # Wrap around edges
        self.positions = self.positions % self.size
        
        # --- 4. Draw ---
        self.trail_buffer *= 0.85 # Fade trails
        
        int_pos = self.positions.astype(int)
        px = int_pos[:, 0]
        py = int_pos[:, 1]
        
        # Draw all particles
        self.trail_buffer[py, px] = 1.0

    def get_output(self, port_name):
        if port_name == 'image':
            return self.trail_buffer
        return None

=== FILE: cognitive_set_analyzer.py ===

"""
Cognitive Set Analyzer Node - Analyzes signal trajectories as "thought patterns"
Place this file in the 'nodes' folder
"""

import numpy as np
from PyQt6 import QtGui
import cv2

import sys
import os
# --- This is the new, correct block ---
import __main__
BaseNode = __main__.BaseNode
PA_INSTANCE = getattr(__main__, "PA_INSTANCE", None)
# ------------------------------------

try:
    from sklearn.cluster import KMeans
    from scipy import stats
    import networkx as nx
    SKLEARN_NX_AVAILABLE = True
except ImportError:
    SKLEARN_NX_AVAILABLE = False
    print("Warning: CognitiveSetAnalyzerNode requires 'scikit-learn' and 'networkx'")
    print("Please run: pip install scikit-learn networkx")

class CognitiveSetAnalyzerNode(BaseNode):
    NODE_CATEGORY = "Transform"
    NODE_COLOR = QtGui.QColor(220, 180, 40) # A golden/analysis color
    
    def __init__(self, trajectory_length=500, num_states=10, display_mode="Radar Plot"):
        super().__init__()
        self.node_title = "Cognitive Set Analyzer"
        
        self.inputs = {
            'signal_1': 'signal', 
            'signal_2': 'signal', 
            'signal_3': 'signal', 
            'signal_4': 'signal'
        }
        self.outputs = {'image': 'image', 'entropy': 'signal'}
        
        self.trajectory_length = int(trajectory_length)
        self.num_states = int(num_states)
        self.display_mode = display_mode
        
        self.trajectory = []
        self.metrics = {}
        self.display_img = np.zeros((128, 128, 3), dtype=np.uint8)

        if not SKLEARN_NX_AVAILABLE:
            self.node_title = "Set Analyzer (Libs Missing!)"

    def step(self):
        if not SKLEARN_NX_AVAILABLE:
            return

        # 1. Collect signal vector
        vec = [
            self.get_blended_input('signal_1', 'sum') or 0.0,
            self.get_blended_input('signal_2', 'sum') or 0.0,
            self.get_blended_input('signal_3', 'sum') or 0.0,
            self.get_blended_input('signal_4', 'sum') or 0.0
        ]
        
        self.trajectory.append(vec)
        if len(self.trajectory) > self.trajectory_length:
            self.trajectory.pop(0)

        # 2. Analyze if we have enough data
        if len(self.trajectory) < 50:
            return
            
        traj_np = np.array(self.trajectory)
        
        if self.display_mode == "Radar Plot":
            # 3. Analyze state dynamics (from brain_set_system.py)
            self.metrics = self._analyze_dynamics(traj_np)
            # 4. Draw Radar Plot
            self.display_img = self._draw_radar_plot(self.metrics)
        
        elif self.display_mode == "Similarity Matrix":
            # 3. Analyze correlation
            corr = np.corrcoef(traj_np.T)
            corr = (corr + 1.0) / 2.0 # Normalize -1..1 to 0..1
            corr_u8 = (corr * 255).astype(np.uint8)
            # 4. Draw Matrix
            img = cv2.resize(corr_u8, (128, 128), interpolation=cv2.INTER_NEAREST)
            self.display_img = cv2.applyColorMap(img, cv2.COLORMAP_VIRIDIS)
            
    def _analyze_dynamics(self, latent_trajectory):
        """Adapted from analyze_state_dynamics in brain_set_system.py"""
        n_states = self.num_states
        if len(latent_trajectory) < n_states:
            return {}
            
        kmeans = KMeans(n_clusters=n_states, random_state=42, n_init='auto')
        state_labels = kmeans.fit_predict(latent_trajectory)
        
        transitions = np.zeros((n_states, n_states))
        for i in range(len(state_labels) - 1):
            transitions[state_labels[i], state_labels[i+1]] += 1
        
        row_sums = transitions.sum(axis=1)
        transition_probs = transitions / row_sums[:, np.newaxis]
        transition_probs[np.isnan(transition_probs)] = 0
        
        metrics = {}
        state_probs = np.bincount(state_labels) / len(state_labels)
        metrics['state_entropy'] = stats.entropy(state_probs[state_probs > 0])
        
        flat_transitions = transition_probs.flatten()
        metrics['transition_entropy'] = stats.entropy(flat_transitions[flat_transitions > 0])
        
        loops = 0
        for i in range(n_states):
            if transition_probs[i, i] > 0.3:
                loops += 1
        metrics['loops'] = loops
        
        try:
            G = nx.from_numpy_array(transitions, create_using=nx.DiGraph)
            communities = list(nx.community.greedy_modularity_communities(G.to_undirected()))
            metrics['modularity'] = nx.community.modularity(G.to_undirected(), communities)
        except Exception:
            metrics['modularity'] = 0
            
        return metrics

    def _draw_radar_plot(self, metrics):
        """Draw a radar plot using numpy and cv2."""
        img = np.zeros((128, 128, 3), dtype=np.uint8)
        center = (64, 64)
        radius = 55
        
        categories = ['State Entropy', 'Trans. Entropy', 'Modularity', 'Loops']
        n_cats = len(categories)
        
        # Get values and normalize
        vals = [
            metrics.get('state_entropy', 0) / 2.3, # Normalize (log(10))
            metrics.get('transition_entropy', 0) / 4.6, # Normalize (log(100))
            metrics.get('modularity', 0),
            metrics.get('loops', 0) / self.num_states
        ]
        vals = np.clip(vals, 0, 1)
        
        # Draw grid
        for i in range(n_cats):
            angle = (i / n_cats) * 2 * np.pi - (np.pi / 2)
            x = int(center[0] + radius * np.cos(angle))
            y = int(center[1] + radius * np.sin(angle))
            cv2.line(img, center, (x, y), (50, 50, 50), 1)
        
        # Draw data shape
        points = []
        for i in range(n_cats):
            angle = (i / n_cats) * 2 * np.pi - (np.pi / 2)
            r = radius * vals[i]
            x = int(center[0] + r * np.cos(angle))
            y = int(center[1] + r * np.sin(angle))
            points.append([x, y])
            
        pts = np.array(points, np.int32).reshape((-1, 1, 2))
        cv2.polylines(img, [pts], isClosed=True, color=(100, 255, 100), thickness=2)
        cv2.fillPoly(img, [pts], color=(50, 120, 50, 0.5))
        
        return img

    def get_output(self, port_name):
        if port_name == 'image':
            return self.display_img.astype(np.float32) / 255.0
        elif port_name == 'entropy':
            return self.metrics.get('state_entropy', 0.0)
        return None
        
    def get_display_image(self):
        rgb = np.ascontiguousarray(self.display_img)
        h, w = rgb.shape[:2]
        return QtGui.QImage(rgb.data, w, h, 3*w, QtGui.QImage.Format.Format_RGB888)

    def get_config_options(self):
        return [
            ("Trajectory Length", "trajectory_length", self.trajectory_length, None),
            ("Number of States", "num_states", self.num_states, None),
            ("Display Mode", "display_mode", self.display_mode, [
                ("Radar Plot", "Radar Plot"), 
                ("Similarity Matrix", "Similarity Matrix")
            ]),
        ]

=== FILE: conscious_galaxy_node.py ===

"""
Conscious Galaxy Node - Audio-reactive consciousness field with agent dynamics
Creates galaxy-like memory patterns from audio and internal agent activity
Requires: pip install torch scipy
Place this file in the 'nodes' folder
"""

import numpy as np
from PyQt6 import QtGui
import cv2
import torch
from scipy.fft import fft, fftfreq
from collections import deque

import __main__
BaseNode = __main__.BaseNode
PA_INSTANCE = getattr(__main__, "PA_INSTANCE", None)
QtGui = __main__.QtGui

try:
    DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    TORCH_AVAILABLE = True
    from scipy.fft import fft, fftfreq
    SCIPY_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False
    SCIPY_AVAILABLE = False
    print("Warning: ConsciousGalaxyNode requires 'torch' and 'scipy'.")


class ConsciousAgent:
    """A field processing agent with emotional resonance"""
    def __init__(self, pos, frequency_range, sensitivity):
        self.pos = np.array(pos, dtype=np.float32)
        self.vel = np.zeros(2, dtype=np.float32)
        self.activation = 0.0
        self.frequency_range = frequency_range
        self.sensitivity = sensitivity
        self.audio_resonance = 0.0
        self.emotion_state = 0.0  # Current emotional activation


class ConsciousGalaxyNode(BaseNode):
    NODE_CATEGORY = "Transform"
    NODE_COLOR = QtGui.QColor(180, 100, 220)  # Purple consciousness
    
    def __init__(self, grid_size=96, num_agents=8):
        super().__init__()
        self.node_title = "Conscious Galaxy"
        
        self.inputs = {
            'audio_signal': 'signal',    # Audio drives emotion/activation
            'emotion_modulator': 'signal',  # External emotion control
            'awareness': 'signal'        # Awareness level (affects memory)
        }
        self.outputs = {
            'consciousness_field': 'image',  # The living field
            'memory_trace': 'image',         # Persistent memories
            'awareness_level': 'signal',     # Current awareness
            'dominant_emotion': 'signal'     # Strongest emotion
        }
        
        if not (TORCH_AVAILABLE and SCIPY_AVAILABLE):
            self.node_title = "Conscious (Missing Libs!)"
            return
            
        self.grid_size = int(grid_size)
        self.num_agents = int(num_agents)
        self.dt = 0.03
        self.time = 0.0
        
        # Field state
        self.psi = torch.zeros((self.grid_size, self.grid_size), 
                               dtype=torch.cfloat, device=DEVICE)
        self.psi_prev = torch.zeros_like(self.psi)
        self.memory = torch.zeros((self.grid_size, self.grid_size), 
                                  dtype=torch.float32, device=DEVICE)
        
        # Laplacian kernel
        self.laplace_kernel = torch.tensor(
            [[0, 1, 0], [1, -4, 1], [0, 1, 0]], 
            dtype=torch.float32, device=DEVICE
        ).unsqueeze(0).unsqueeze(0)
        
        # Create conscious agents
        self.agents = self._create_agents()
        
        # Audio processing
        self.audio_buffer = deque(maxlen=512)
        self.frequency_memory = deque(maxlen=50)
        
        # Emotion system
        self.emotions = {
            'joy': 0.0,
            'sadness': 0.0,
            'anger': 0.0,
            'fear': 0.0,
            'surprise': 0.0,
            'calm': 0.0
        }
        self.awareness_level = 0.12
        
        # Parameters
        self.wave_speed = 1.8
        self.field_damping = 0.05
        self.memory_persistence = 0.995
        
    def _create_agents(self):
        """Create field processing agents positioned around the space"""
        agents = []
        positions = [
            (0.2, 0.2), (0.8, 0.2), (0.2, 0.8), (0.8, 0.8),
            (0.5, 0.3), (0.3, 0.7), (0.7, 0.5), (0.5, 0.5)
        ]
        
        for i in range(min(self.num_agents, len(positions))):
            x, y = positions[i]
            agent = ConsciousAgent(
                pos=[x * self.grid_size, y * self.grid_size],
                frequency_range=(50 + i*200, 250 + i*200),
                sensitivity=0.3 + i * 0.1
            )
            agents.append(agent)
        
        return agents
    
    def _process_audio_spectrum(self, audio_signal):
        """Analyze audio and update agent activations"""
        if audio_signal is None or abs(audio_signal) < 0.01:
            # Decay activations
            for agent in self.agents:
                agent.activation *= 0.95
                agent.audio_resonance *= 0.9
            return
        
        # Add to buffer
        self.audio_buffer.append(audio_signal)
        
        if len(self.audio_buffer) < 256:
            return
        
        # FFT analysis
        recent_audio = np.array(list(self.audio_buffer)[-256:])
        spectrum = fft(recent_audio)
        freqs = fftfreq(len(recent_audio), 1.0/44100)
        power = np.abs(spectrum[:128])
        
        volume = np.sqrt(np.mean(recent_audio**2))
        
        # Store frequency memory
        self.frequency_memory.append({
            'spectrum': power[:50].copy(),
            'volume': volume
        })
        
        # Update agents based on their frequency ranges
        for agent in self.agents:
            f_min, f_max = agent.frequency_range
            freq_mask = (np.abs(freqs[:128]) >= f_min) & (np.abs(freqs[:128]) <= f_max)
            
            if np.any(freq_mask):
                emotional_power = np.mean(power[freq_mask])
                activation_strength = emotional_power * volume * 1000
                
                # Update with momentum
                agent.activation = 0.85 * agent.activation + 0.15 * activation_strength
                agent.activation = np.clip(agent.activation, 0, 2.0)
                
                # Audio resonance
                if self.frequency_memory:
                    recent_spectrum = self.frequency_memory[-1]['spectrum']
                    freq_response = np.mean(recent_spectrum) * agent.sensitivity
                    agent.audio_resonance = 0.8 * agent.audio_resonance + 0.2 * freq_response
    
    def _update_emotions(self):
        """Update emotional state based on agent activations"""
        # Map agent activations to emotions
        if len(self.agents) >= 6:
            self.emotions['joy'] = self.agents[0].activation / 2.0
            self.emotions['sadness'] = self.agents[1].activation / 2.0
            self.emotions['anger'] = self.agents[2].activation / 2.0
            self.emotions['fear'] = self.agents[3].activation / 2.0
            self.emotions['surprise'] = self.agents[4].activation / 2.0
            self.emotions['calm'] = self.agents[5].activation / 2.0
        
        # Decay emotions
        for key in self.emotions:
            self.emotions[key] *= 0.98
            self.emotions[key] = np.clip(self.emotions[key], 0, 1)
    
    def _create_agent_patterns(self):
        """Agents create field patterns based on their activation"""
        Y, X = torch.meshgrid(
            torch.arange(self.grid_size, device=DEVICE), 
            torch.arange(self.grid_size, device=DEVICE), 
            indexing='ij'
        )
        
        field_additions = torch.zeros_like(self.psi)
        
        for i, agent in enumerate(self.agents):
            if agent.activation > 0.1:
                ax, ay = agent.pos
                
                # Distance from agent
                r = torch.sqrt((X - ax)**2 + (Y - ay)**2)
                theta = torch.atan2(Y - ay, X - ax)
                
                # Different pattern types
                if i % 3 == 0:  # Expanding circles
                    pattern = agent.activation * torch.sin(3 * r * 0.1 - self.time * 5)
                    phase = self.time
                    phase_cplx = torch.cos(torch.tensor(phase, device=DEVICE)) + \
                                1j * torch.sin(torch.tensor(phase, device=DEVICE))
                    field_additions += 0.5 * pattern * phase_cplx
                    
                elif i % 3 == 1:  # Spirals
                    pattern = agent.activation * torch.sin(r * 0.1 - theta * 3 - self.time * 2)
                    phase_cplx = torch.cos(theta) + 1j * torch.sin(theta)
                    field_additions += 0.3 * pattern * phase_cplx
                    
                else:  # Ripples
                    pattern = agent.activation * torch.exp(-r / 20) * torch.sin(r * 0.3 - self.time * 4)
                    phase = self.time * 3
                    phase_cplx = torch.cos(torch.tensor(phase, device=DEVICE)) + \
                                1j * torch.sin(torch.tensor(phase, device=DEVICE))
                    field_additions += 0.4 * pattern * phase_cplx
        
        return field_additions
    
    def _laplacian(self, field):
        """Compute Laplacian"""
        real_part = torch.nn.functional.conv2d(
            field.real.unsqueeze(0).unsqueeze(0), 
            self.laplace_kernel, 
            padding=1
        ).squeeze()
        
        imag_part = torch.nn.functional.conv2d(
            field.imag.unsqueeze(0).unsqueeze(0), 
            self.laplace_kernel, 
            padding=1
        ).squeeze()
        
        return real_part + 1j * imag_part
    
    def _update_agents(self):
        """Move agents based on field gradients"""
        field_intensity = torch.abs(self.psi)**2
        field_np = field_intensity.cpu().numpy()
        
        for agent in self.agents:
            x, y = int(agent.pos[0]), int(agent.pos[1])
            x = np.clip(x, 1, self.grid_size - 2)
            y = np.clip(y, 1, self.grid_size - 2)
            
            if agent.activation > 0.2:
                # Follow field gradients
                grad_x = field_np[y, min(x+1, self.grid_size-1)] - \
                        field_np[y, max(x-1, 0)]
                grad_y = field_np[min(y+1, self.grid_size-1), x] - \
                        field_np[max(y-1, 0), x]
                
                agent.vel += np.array([grad_x, grad_y]) * 0.1 * agent.activation
                
                # Add exploration
                agent.vel += np.random.randn(2) * 0.3
            
            # Damping
            agent.vel *= 0.85
            agent.vel = np.clip(agent.vel, -3, 3)
            
            # Update position
            agent.pos += agent.vel * self.dt
            agent.pos = np.clip(agent.pos, 5, self.grid_size - 5)

    def step(self):
        if not (TORCH_AVAILABLE and SCIPY_AVAILABLE):
            return
            
        # Get inputs
        audio = self.get_blended_input('audio_signal', 'sum') or 0.0
        emotion_mod = self.get_blended_input('emotion_modulator', 'sum')
        awareness_in = self.get_blended_input('awareness', 'sum')
        
        # Update awareness
        if awareness_in is not None:
            self.awareness_level = 0.9 * self.awareness_level + 0.1 * abs(awareness_in)
        else:
            self.awareness_level = 0.9 * self.awareness_level + 0.1 * 0.12
        
        # Process audio
        self._process_audio_spectrum(audio)
        
        # Update emotions
        self._update_emotions()
        
        # Apply emotion modulator
        if emotion_mod is not None:
            for agent in self.agents:
                agent.activation *= (1.0 + emotion_mod * 0.2)
        
        # Create agent patterns
        agent_patterns = self._create_agent_patterns()
        self.psi += agent_patterns
        
        # Evolve field
        laplacian = self._laplacian(self.psi)
        psi_new = (2 * self.psi - self.psi_prev + 
                   self.dt**2 * (self.wave_speed * laplacian - 
                                 self.field_damping * self.psi))
        
        # Limit amplitude
        amp = torch.abs(psi_new)
        max_amp = 5.0
        mask = amp > max_amp
        psi_new[mask] = psi_new[mask] / amp[mask] * max_amp
        
        # Update memory with awareness modulation
        field_intensity = torch.abs(self.psi)**2
        memory_rate = self.memory_persistence + (1 - self.memory_persistence) * self.awareness_level
        self.memory = memory_rate * self.memory + (1 - memory_rate) * field_intensity
        
        # Update
        self.psi_prev = self.psi.clone()
        self.psi = psi_new
        
        # Update agents
        self._update_agents()
        
        self.time += self.dt

    def get_output(self, port_name):
        if port_name == 'consciousness_field':
            field_cpu = torch.abs(self.psi).cpu().numpy().astype(np.float32)
            max_val = field_cpu.max()
            if max_val > 1e-9:
                return field_cpu / max_val
            return field_cpu
            
        elif port_name == 'memory_trace':
            memory_cpu = self.memory.cpu().numpy().astype(np.float32)
            max_val = memory_cpu.max()
            if max_val > 1e-9:
                return memory_cpu / max_val
            return memory_cpu
            
        elif port_name == 'awareness_level':
            return float(self.awareness_level)
            
        elif port_name == 'dominant_emotion':
            if self.emotions:
                return float(max(self.emotions.values()))
            return 0.0
            
        return None
        
    def get_display_image(self):
        # Show memory trace with magma colormap
        memory_np = self.memory.cpu().numpy()
        
        max_val = memory_np.max()
        if max_val > 1e-9:
            memory_norm = memory_np / max_val
        else:
            memory_norm = memory_np
            
        img_u8 = (memory_norm * 255).astype(np.uint8)
        img_color = cv2.applyColorMap(img_u8, cv2.COLORMAP_MAGMA)
        
        # Draw agents as dots
        for agent in self.agents:
            x, y = int(agent.pos[0]), int(agent.pos[1])
            if 0 <= x < self.grid_size and 0 <= y < self.grid_size:
                brightness = int(agent.activation * 127 + 128)
                color = (brightness, brightness, 255)
                cv2.circle(img_color, (x, y), 2, color, -1)
        
        img_color = np.ascontiguousarray(img_color)
        h, w = img_color.shape[:2]
        return QtGui.QImage(img_color.data, w, h, 3*w, QtGui.QImage.Format.Format_BGR888)

    def get_config_options(self):
        return [
            ("Grid Size (NxN)", "grid_size", self.grid_size, None),
            ("Num Agents", "num_agents", self.num_agents, None),
        ]
    
    def randomize(self):
        """Reset the consciousness"""
        if TORCH_AVAILABLE:
            self.psi.zero_()
            self.psi_prev.zero_()
            self.memory.zero_()
            for agent in self.agents:
                agent.activation = 0.0
                agent.vel[:] = 0.0

=== FILE: consciousnessfilternode.py ===

"""
Consciousness Filter Node - Models observer-dependent reality projection
Demonstrates "out-of-band content is invisible to the observer" principle.
Implements a trainable W matrix that learns which frequency bands constitute "experience".

Place this file in the 'nodes' folder as 'consciousnessfilter.py'
"""

import numpy as np
from PyQt6 import QtGui
import cv2

import __main__
BaseNode = __main__.BaseNode
PA_INSTANCE = getattr(__main__, "PA_INSTANCE", None)
QtGui = __main__.QtGui

try:
    from scipy.fft import rfft, irfft, rfftfreq
    from scipy.signal import butter, filtfilt
    SCIPY_AVAILABLE = True
except ImportError:
    SCIPY_AVAILABLE = False
    print("Warning: ConsciousnessFilterNode requires scipy")

class ConsciousnessFilterNode(BaseNode):
    NODE_CATEGORY = "Analysis"
    NODE_COLOR = QtGui.QColor(140, 70, 180)  # Deep purple for consciousness
    
    def __init__(self, observer_bandwidth=50.0, field_size=512):
        super().__init__()
        self.node_title = "Consciousness Filter"
        
        self.inputs = {
            'external_field': 'signal',    # The "world out there" 
            'internal_field': 'signal',    # The "thoughts/predictions"
            'attention_shift': 'signal',   # Dynamically shift filter band
            'coherence_demand': 'signal'   # How much to enforce phase lock
        }
        
        self.outputs = {
            'conscious_experience': 'signal',  # What "you" experience
            'invisible_content': 'signal',     # What exists but you can't sense
            'phase_coherence': 'signal',       # How locked internal/external are
            'spectrum_image': 'image',         # Visualization of filter action
            'attractor_strength': 'signal'     # How stable is "you" right now
        }
        
        if not SCIPY_AVAILABLE:
            self.node_title = "Consciousness (No SciPy!)"
            return
        
        self.field_size = int(field_size)
        self.observer_bandwidth = float(observer_bandwidth)  # Hz cutoff
        
        self.fs = 1000.0 # Sample rate for frequency interpretation
        
        # The W matrix: your learned frequency response (what bands you can sense)
        self.W_filter_response = self._initialize_W_filter()
        
        # History for phase coherence tracking
        self.external_history = np.zeros(field_size, dtype=np.float32)
        self.internal_history = np.zeros(field_size, dtype=np.float32)
        
        # Attractor state (are you maintaining coherence?)
        self.attractor_basin_depth = 1.0
        self.coherence_history = []
        
        # --- FIX: Initialize all output variables ---
        self.phase_coherence = 0.0
        self.conscious_experience = 0.0
        self.invisible_content = 0.0
        self.attractor_strength_val = 0.0
        self.last_F_ext = np.zeros(self.field_size // 2 + 1, dtype=np.complex64)
        self.last_F_conscious = np.zeros(self.field_size // 2 + 1, dtype=np.complex64)
        # --- END FIX ---
        
    def _initialize_W_filter(self):
        """
        Initialize the W matrix as a frequency response function.
        This is your "consciousness bandwidth" - what you can sense.
        """
        freqs = rfftfreq(self.field_size, 1.0/self.fs)
        
        low_cutoff = 4.0   # Below theta: unconscious
        high_cutoff = self.observer_bandwidth  # Above this: too fast to integrate
        
        W = np.zeros_like(freqs)
        mask = (freqs >= low_cutoff) & (freqs <= high_cutoff)
        W[mask] = 1.0
        
        transition_width = 5.0
        for i, f in enumerate(freqs):
            if f < low_cutoff:
                W[i] = np.exp(-((low_cutoff - f)**2) / (2 * transition_width**2))
            elif f > high_cutoff:
                W[i] = np.exp(-((f - high_cutoff)**2) / (2 * transition_width**2))
        
        return W
    
    def apply_consciousness_filter(self, signal, attention_shift=0.0):
        """
        Apply the W matrix (consciousness filter) to incoming signal.
        """
        F = rfft(signal)
        freqs = rfftfreq(len(signal), 1.0/self.fs)
        
        shifted_W = np.roll(self.W_filter_response, int(attention_shift * 10))
        shifted_W = shifted_W[:len(F)]  # Match length
        
        F_conscious = F * shifted_W
        F_invisible = F * (1.0 - shifted_W)  # What you CAN'T sense
        
        conscious_signal = irfft(F_conscious, n=len(signal))
        invisible_signal = irfft(F_invisible, n=len(signal))
        
        return conscious_signal, invisible_signal, F, F_conscious
    
    def measure_phase_coherence(self, external, internal):
        """
        Measure how phase-locked external and internal fields are.
        """
        F_ext = rfft(external)
        F_int = rfft(internal)
        
        phase_ext = np.angle(F_ext)
        phase_int = np.angle(F_int)
        phase_diff = np.abs(phase_ext - phase_int)
        
        W_slice = self.W_filter_response[:len(phase_diff)]
        weighted_diff = phase_diff * W_slice
        
        coherence = 1.0 - np.mean(weighted_diff) / np.pi
        coherence = np.clip(coherence, 0, 1)
        
        return coherence
    
    def update_attractor_stability(self, coherence):
        """
        Track attractor stability over time.
        """
        self.coherence_history.append(coherence)
        if len(self.coherence_history) > 100:
            self.coherence_history.pop(0)
        
        if len(self.coherence_history) > 10:
            coherence_variance = np.var(self.coherence_history[-20:])
            self.attractor_basin_depth = 1.0 / (1.0 + coherence_variance * 10)
        
        return self.attractor_basin_depth
    
    def step(self):
        if not SCIPY_AVAILABLE:
            return
        
        external = self.get_blended_input('external_field', 'sum') or 0.0
        internal = self.get_blended_input('internal_field', 'sum') or 0.0
        attention_shift = self.get_blended_input('attention_shift', 'sum') or 0.0
        coherence_demand = self.get_blended_input('coherence_demand', 'sum') or 0.5
        
        self.external_history[:-1] = self.external_history[1:]
        self.external_history[-1] = external
        
        self.internal_history[:-1] = self.internal_history[1:]
        self.internal_history[-1] = internal
        
        conscious_ext, invisible_ext, F_ext, F_conscious = self.apply_consciousness_filter(
            self.external_history, attention_shift
        )
        
        conscious_int, invisible_int, F_int, _ = self.apply_consciousness_filter(
            self.internal_history, attention_shift
        )
        
        coherence = self.measure_phase_coherence(
            self.external_history, 
            self.internal_history
        )
        
        attractor_strength = self.update_attractor_stability(coherence)
        
        blend_ratio = 0.5 + coherence_demand * 0.3
        self.conscious_experience = (
            blend_ratio * conscious_ext[-1] + 
            (1 - blend_ratio) * conscious_int[-1]
        )
        
        self.invisible_content = invisible_ext[-1]
        
        self.phase_coherence = coherence
        self.attractor_strength_val = attractor_strength
        
        self.last_F_ext = F_ext
        self.last_F_conscious = F_conscious
    
    def get_output(self, port_name):
        if port_name == 'conscious_experience':
            return self.conscious_experience
        
        elif port_name == 'invisible_content':
            return self.invisible_content
        
        elif port_name == 'phase_coherence':
            return self.phase_coherence
        
        elif port_name == 'attractor_strength':
            return self.attractor_strength_val
        
        elif port_name == 'spectrum_image':
            return self.generate_spectrum_image()
        
        return None
    
    def generate_spectrum_image(self):
        """
        Visualize what you can/cannot sense.
        """
        if not hasattr(self, 'last_F_ext'):
            return np.zeros((64, 128), dtype=np.float32)
        
        h, w = 64, 128
        img = np.zeros((h, w), dtype=np.float32)
        
        mag_original = np.abs(self.last_F_ext)
        mag_conscious = np.abs(self.last_F_conscious)
        
        norm_max = np.max(mag_original) + 1e-9
        mag_original = mag_original / norm_max
        mag_conscious = mag_conscious / norm_max
        
        n_bins = len(mag_original)
        if n_bins > w:
            indices = np.linspace(0, n_bins-1, w).astype(int)
            mag_original = mag_original[indices]
            mag_conscious = mag_conscious[indices]
        
        for i in range(len(mag_original)):
            if i >= w:
                break
            
            height_orig = int(mag_original[i] * (h // 2 - 1))
            img[h//2 - height_orig:h//2, i] = 0.5
            
            height_cons = int(mag_conscious[i] * (h // 2 - 1))
            img[h//2:h//2 + height_cons, i] = 1.0
        
        img[h//2, :] = 0.3
        
        return img
    
    def get_display_image(self):
        if not SCIPY_AVAILABLE:
            return None
        
        spectrum_img = self.generate_spectrum_image()
        img_u8 = (np.clip(spectrum_img, 0, 1) * 255).astype(np.uint8)
        
        img_color = cv2.applyColorMap(img_u8, cv2.COLORMAP_PLASMA)
        
        h, w = img_color.shape[:2]
        
        font = cv2.FONT_HERSHEY_SIMPLEX
        cv2.putText(img_color, 'EXISTS', (2, 12), font, 0.3, (255, 255, 255), 1)
        cv2.putText(img_color, 'YOU SENSE', (2, h-4), font, 0.3, (255, 255, 255), 1)
        
        bar_width = 8
        bar_height = int(self.phase_coherence * h)
        img_color[-bar_height:, -bar_width:] = [0, 255, 0]  # Green bar
        
        img_color = np.ascontiguousarray(img_color)
        return QtGui.QImage(img_color.data, w, h, 3*w, QtGui.QImage.Format.Format_BGR888)
    
    def get_config_options(self):
        return [
            ("Observer Bandwidth (Hz)", "observer_bandwidth", self.observer_bandwidth, None),
            ("Field Size (samples)", "field_size", self.field_size, None),
        ]

=== FILE: constantsignalnode.py ===

"""
Constant Signal Node - Outputs a fixed, configurable signal value.
Useful for providing stable parameters or triggers.
Place this file in the 'nodes' folder
"""

import numpy as np
from PyQt6 import QtGui
from PIL import Image, ImageDraw, ImageFont
import os

import __main__
BaseNode = __main__.BaseNode
PA_INSTANCE = getattr(__main__, "PA_INSTANCE", None)

class ConstantSignalNode(BaseNode):
    NODE_CATEGORY = "Source"
    NODE_COLOR = QtGui.QColor(40, 120, 80) # Source Green
    
    def __init__(self, value=1.0):
        super().__init__()
        self.node_title = "Constant Signal"
        self.outputs = {'signal': 'signal'}
        self.value = float(value)
        
        # Try to load a font for display
        try:
            self.font = ImageFont.load_default(size=14)
        except IOError:
            self.font = None

    def step(self):
        # Do nothing, the value is constant
        pass
        
    def get_output(self, port_name):
        if port_name == 'signal':
            return self.value
        return None
        
    def get_display_image(self):
        w, h = 64, 32  # Small and wide
        img = np.zeros((h, w, 3), dtype=np.uint8)
        img_pil = Image.fromarray(img)
        draw = ImageDraw.Draw(img_pil)
        
        text = f"{self.value:.2f}"
        text_color = (200, 200, 200)
        
        try:
            bbox = draw.textbbox((0, 0), text, font=self.font)
            text_w = bbox[2] - bbox[0]
            text_h = bbox[3] - bbox[1]
            x = (w - text_w) / 2
            y = (h - text_h) / 2
        except Exception:
            x, y = 5, 5 # Fallback
            
        draw.text((x, y), text, fill=text_color, font=self.font)
        
        img_final = np.array(img_pil)
        img_final = np.ascontiguousarray(img_final)
        return QtGui.QImage(img_final.data, w, h, 3*w, QtGui.QImage.Format.Format_RGB888)

    def get_config_options(self):
        return [
            ("Value", "value", self.value, None)
        ]

=== FILE: contourmomentnode.py ===

"""
ContourMomentNode

Calculates geometric moments from a binary (B&W) image
to extract actionable control signals:
- Center of Mass (x, y)
- Area (how much white)
- Orientation (angle of the main shape)
- Eccentricity (how "stretched" the shape is)
"""

import numpy as np
import cv2

# --- Magic import block ---
import __main__
BaseNode = __main__.BaseNode
QtGui = __main__.QtGui
# --------------------------

class ContourMomentNode(BaseNode):
    """
    Extracts geometric features from a binary image using moments.
    """
    NODE_CATEGORY = "Analyzer"
    NODE_COLOR = QtGui.QColor(220, 200, 100) # Gold

    def __init__(self, size=128):
        super().__init__()
        self.node_title = "Contour Moments"
        
        self.inputs = {
            'image_in': 'image',
            'threshold': 'signal' # To convert grayscale to B&W
        }
        self.outputs = {
            'image': 'image',      # The B&W image + overlay
            'center_x': 'signal',  # Normalized -1 to 1
            'center_y': 'signal',  # Normalized -1 to 1
            'area': 'signal',      # Normalized 0 to 1
            'orientation': 'signal', # Normalized -1 to 1 (-90 to +90 deg)
            'eccentricity': 'signal' # Normalized 0 to 1
        }
        
        # We downscale for performance
        self.size = int(size) 
        
        # Internal state
        self.display_image = np.zeros((self.size, self.size, 3), dtype=np.float32)
        self.center_x = 0.0
        self.center_y = 0.0
        self.area = 0.0
        self.orientation = 0.0
        self.eccentricity = 0.0

    def step(self):
        # --- 1. Get and Prepare Image ---
        img = self.get_blended_input('image_in', 'first')
        
        if img is None:
            # Decay signals if no image
            self.area *= 0.95
            self.eccentricity *= 0.95
            return

        # --- START FIX (for float64 error) ---
        # We must ensure the image is float32 *before* any OpenCV operations
        
        # 1. Convert to float32 if it isn't already
        if img.dtype != np.float32:
             # This will catch float64 (the error) and uint8 (common)
            img = img.astype(np.float32)

        # 2. Normalize to 0-1 if it's in 0-255 range
        if img.max() > 1.0:
            img = img / 255.0
            
        img = np.clip(img, 0, 1) # Ensure range
        # --- END FIX ---
        
        # Resize for performance and consistency
        img_resized = cv2.resize(img, (self.size, self.size), 
                                 interpolation=cv2.INTER_LINEAR)
        
        # Convert to grayscale
        if img_resized.ndim == 3:
            img_gray = cv2.cvtColor(img_resized, cv2.COLOR_RGB2GRAY)
        else:
            img_gray = img_resized
        
        # --- 2. Get Binary Image ---
        threshold = self.get_blended_input('threshold', 'sum') or 0.5
        
        _ , binary = cv2.threshold(
            (img_gray * 255).astype(np.uint8), 
            int(threshold * 255), 
            255, 
            cv2.THRESH_BINARY
        )
        
        # --- 3. Calculate Moments ---
        moments = cv2.moments(binary)
        m00 = moments['m00'] # This is the total area (in pixels)

        if m00 > 0:
            # --- Area ---
            self.area = m00 / (self.size * self.size) # Normalized 0-1
            
            # --- Center of Mass ---
            cx = moments['m10'] / m00
            cy = moments['m01'] / m00
            
            # Normalize -1 to 1
            self.center_x = (cx / self.size) * 2.0 - 1.0
            self.center_y = (cy / self.size) * 2.0 - 1.0
            
            # --- Orientation & Eccentricity ---
            mu20 = moments['mu20']
            mu02 = moments['mu02']
            mu11 = moments['mu11']
            
            term = np.sqrt((mu20 - mu02)**2 + 4 * mu11**2)
            lambda1 = 0.5 * (mu20 + mu02 + term) # Major axis
            lambda2 = 0.5 * (mu20 + mu02 - term) # Minor axis

            angle_rad = 0.5 * np.arctan2(2 * mu11, mu20 - mu02)
            self.orientation = angle_rad / (np.pi / 2.0) # Normalize -1 to 1

            if lambda1 > 0 and lambda2 >= 0:
                self.eccentricity = np.sqrt(1.0 - (lambda2 / lambda1))
            else:
                self.eccentricity = 0.0
            
        else:
            # No contours, set all to 0
            self.area = 0.0
            self.center_x = 0.0
            self.center_y = 0.0
            self.orientation = 0.0
            self.eccentricity = 0.0
            
        # --- 4. Prepare Display Image ---
        self.display_image = cv2.cvtColor(binary, cv2.COLOR_GRAY2RGB)
        self.display_image = self.display_image.astype(np.float32) / 255.0
        
        if m00 > 0:
            # Convert normalized coords back to pixel space
            cx_px = int((self.center_x + 1.0) * 0.5 * self.size)
            cy_px = int((self.center_y + 1.0) * 0.5 * self.size)
            
            # Draw Center of Mass (Green Circle)
            cv2.circle(self.display_image, (cx_px, cy_px), 5, (0, 1, 0), -1) 

            # Draw Orientation Line (Magenta)
            angle_rad = self.orientation * (np.pi / 2.0)
            length = self.eccentricity * (self.size / 4.0) + 10 
            
            dx = np.cos(angle_rad) * length
            dy = np.sin(angle_rad) * length
            
            p1 = (int(cx_px - dx), int(cy_px - dy))
            p2 = (int(cx_px + dx), int(cy_px + dy))
            cv2.line(self.display_image, p1, p2, (1, 0, 1), 2)
            
        self.display_image = np.clip(self.display_image, 0, 1)


    def get_output(self, port_name):
        if port_name == 'image':
            return self.display_image
        elif port_name == 'center_x':
            return self.center_x
        elif port_name == 'center_y':
            return self.center_y
        elif port_name == 'area':
            return self.area
        elif port_name == 'orientation':
            return self.orientation
        elif port_name == 'eccentricity':
            return self.eccentricity
        return None

=== FILE: coordinatenodes.py ===

"""
Particle Attractor Field Node - ULTRA-SAFE EDITION

NO ANTIALIASING - just simple pixel drawing
Absolute bounds protection - cannot possibly go out of range
"""

import numpy as np
import cv2

import __main__
BaseNode = __main__.BaseNode
QtGui = __main__.QtGui

class ParticleAttractorNode(BaseNode):
    """Particle swarm attracted to x/y coordinate position - ULTRA SAFE VERSION"""
    NODE_CATEGORY = "Generator"
    NODE_COLOR = QtGui.QColor(200, 100, 180)
    
    def __init__(self, particle_count=300, size=256, attraction_strength=0.5):
        super().__init__()
        self.node_title = "Particle Attractor (Safe)"
        
        self.inputs = {
            'x_coord': 'signal',
            'y_coord': 'signal',
            'strength': 'signal',
            'chaos': 'signal'
        }
        self.outputs = {
            'image': 'image',
            'density': 'signal'
        }
        
        self.particle_count = int(particle_count)
        self.size = int(size)
        self.attraction_strength = float(attraction_strength)
        
        # Initialize particles in center region only
        margin = self.size * 0.1
        self.positions = np.random.rand(self.particle_count, 2) * (self.size - 2*margin) + margin
        self.velocities = np.zeros((self.particle_count, 2), dtype=np.float32)
        
        # Trail buffer
        self.trail_buffer = np.zeros((self.size, self.size), dtype=np.float32)
        
        # Output
        self.density = 0.0
        
    def step(self):
        # Get inputs
        x_coord = self.get_blended_input('x_coord', 'sum') or 0.0
        y_coord = self.get_blended_input('y_coord', 'sum') or 0.0
        strength = self.get_blended_input('strength', 'sum')
        if strength is None:
            strength = self.attraction_strength
        chaos = self.get_blended_input('chaos', 'sum') or 0.0
        
        # Attractor position
        attractor_x = np.clip((x_coord + 1.0) * 0.5 * self.size, 0, self.size - 1)
        attractor_y = np.clip((y_coord + 1.0) * 0.5 * self.size, 0, self.size - 1)
        attractor = np.array([attractor_x, attractor_y])
        
        # Forces
        to_attractor = attractor - self.positions
        distances = np.linalg.norm(to_attractor, axis=1, keepdims=True)
        distances = np.maximum(distances, 10.0)  # Prevent extreme forces
        
        # Attraction (clamped)
        forces = to_attractor / (distances ** 2) * strength * 50
        forces = np.clip(forces, -20, 20)
        
        # Chaos
        if chaos > 0.01:
            forces += (np.random.rand(self.particle_count, 2) - 0.5) * chaos * 5
        
        # Update
        self.velocities += forces * 0.1
        self.velocities = np.clip(self.velocities, -5, 5)
        self.velocities *= 0.9
        self.positions += self.velocities
        
        # ABSOLUTE HARD CLAMP - cannot escape
        self.positions = np.clip(self.positions, 0, self.size - 1.01)
        
        # Fade
        self.trail_buffer *= 0.92
        
        # Draw - NO ANTIALIASING, just simple pixels
        for i in range(len(self.positions)):
            x = int(self.positions[i, 0])
            y = int(self.positions[i, 1])
            
            # Paranoid bounds check
            if 0 <= x < self.size and 0 <= y < self.size:
                self.trail_buffer[y, x] += 1.0
        
        # Density
        attractor_distances = np.linalg.norm(self.positions - attractor, axis=1)
        close_particles = np.sum(attractor_distances < self.size * 0.15)
        self.density = close_particles / self.particle_count
        
    def get_output(self, port_name):
        if port_name == 'image':
            normalized = np.clip(self.trail_buffer / (np.max(self.trail_buffer) + 1e-9), 0, 1)
            colored = cv2.applyColorMap((normalized * 255).astype(np.uint8), cv2.COLORMAP_HOT)
            return colored.astype(np.float32) / 255.0
        elif port_name == 'density':
            return self.density
        return None


class StrangeAttractorNode(BaseNode):
    """Strange attractor - ULTRA SAFE VERSION"""
    NODE_CATEGORY = "Generator"
    NODE_COLOR = QtGui.QColor(180, 100, 200)
    
    def __init__(self, size=256, attractor_type='lorenz'):
        super().__init__()
        self.node_title = f"Strange Attractor ({attractor_type})"
        
        self.inputs = {
            'param_a': 'signal',
            'param_b': 'signal',
            'speed': 'signal'
        }
        self.outputs = {
            'image': 'image',
            'chaos': 'signal'
        }
        
        self.size = int(size)
        self.attractor_type = attractor_type
        self.state = np.array([0.1, 0.0, 0.0])
        self.trail_buffer = np.zeros((self.size, self.size), dtype=np.float32)
        self.history = []
        self.chaos_measure = 0.0
        
    def step(self):
        param_a = self.get_blended_input('param_a', 'sum') or 0.0
        param_b = self.get_blended_input('param_b', 'sum') or 0.0
        speed = self.get_blended_input('speed', 'sum') or 1.0
        
        if self.attractor_type == 'lorenz':
            sigma = 10.0 + param_a * 5.0
            rho = 28.0 + param_b * 10.0
            beta = 8.0 / 3.0
            
            x, y, z = self.state
            dx = sigma * (y - x)
            dy = x * (rho - z) - y
            dz = x * y - beta * z
            
            dt = 0.01 * speed
            self.state += np.array([dx, dy, dz]) * dt
            
            proj_x = (x / 30.0 + 1.0) * 0.5 * self.size
            proj_y = (z / 50.0) * 0.5 * self.size + self.size * 0.5
            
        else:  # rossler or aizawa
            a = 0.2 + param_a * 0.1
            b = 0.2 + param_b * 0.1
            c = 5.7
            
            x, y, z = self.state
            dx = -y - z
            dy = x + a * y
            dz = b + z * (x - c)
            
            dt = 0.05 * speed
            self.state += np.array([dx, dy, dz]) * dt
            
            proj_x = (x / 15.0 + 1.0) * 0.5 * self.size
            proj_y = (y / 15.0 + 1.0) * 0.5 * self.size
        
        self.trail_buffer *= 0.98
        
        # ULTRA SAFE drawing
        x_px = int(np.clip(proj_x, 0, self.size - 1))
        y_px = int(np.clip(proj_y, 0, self.size - 1))
        
        if 0 <= x_px < self.size and 0 <= y_px < self.size:
            self.trail_buffer[y_px, x_px] += 1.0
        
        self.history.append(np.copy(self.state))
        if len(self.history) > 100:
            self.history.pop(0)
        
        if len(self.history) > 10:
            recent = np.array(self.history[-10:])
            variance = np.var(recent, axis=0)
            self.chaos_measure = np.mean(variance) / 100.0
        else:
            self.chaos_measure = 0.0
            
    def get_output(self, port_name):
        if port_name == 'image':
            normalized = np.clip(self.trail_buffer / (np.max(self.trail_buffer) + 1e-9), 0, 1)
            colored = cv2.applyColorMap((normalized * 255).astype(np.uint8), cv2.COLORMAP_VIRIDIS)
            return colored.astype(np.float32) / 255.0
        elif port_name == 'chaos':
            return self.chaos_measure
        return None


class ReactionDiffusionNode(BaseNode):
    """Reaction-diffusion - ULTRA SAFE VERSION"""
    NODE_CATEGORY = "Generator"
    NODE_COLOR = QtGui.QColor(150, 200, 100)
    
    def __init__(self, size=128, pattern='spots'):
        super().__init__()
        self.node_title = "Reaction-Diffusion"
        
        self.inputs = {
            'feed_rate': 'signal',
            'kill_rate': 'signal',
            'seed': 'signal'
        }
        self.outputs = {
            'image': 'image',
            'pattern_energy': 'signal'
        }
        
        self.size = int(size)
        self.pattern = pattern
        
        self.A = np.ones((self.size, self.size), dtype=np.float32)
        self.B = np.zeros((self.size, self.size), dtype=np.float32)
        
        center = self.size // 2
        radius = self.size // 10
        y, x = np.ogrid[-center:self.size-center, -center:self.size-center]
        mask = x*x + y*y <= radius*radius
        self.B[mask] = 1.0
        
        self.Da = 1.0
        self.Db = 0.5
        self.last_seed = 0.0
        self.pattern_energy = 0.0
        
    def step(self):
        feed_rate = self.get_blended_input('feed_rate', 'sum')
        kill_rate = self.get_blended_input('kill_rate', 'sum')
        seed = self.get_blended_input('seed', 'sum') or 0.0
        
        feed = 0.055 if feed_rate is None else 0.01 + (feed_rate + 1.0) * 0.05
        kill = 0.062 if kill_rate is None else 0.03 + (kill_rate + 1.0) * 0.04
        
        if seed > 0.5 and self.last_seed <= 0.5:
            x = self.size // 2
            y = self.size // 2
            radius = max(2, self.size // 20)
            
            for i in range(-radius, radius + 1):
                for j in range(-radius, radius + 1):
                    if i*i + j*j <= radius*radius:
                        xi = (x + i) % self.size
                        yi = (y + j) % self.size
                        if 0 <= xi < self.size and 0 <= yi < self.size:
                            self.B[yi, xi] = 1.0
        self.last_seed = seed
        
        kernel = np.array([[0.05, 0.2, 0.05],
                          [0.2, -1.0, 0.2],
                          [0.05, 0.2, 0.05]])
        
        laplaceA = cv2.filter2D(self.A, -1, kernel, borderType=cv2.BORDER_WRAP)
        laplaceB = cv2.filter2D(self.B, -1, kernel, borderType=cv2.BORDER_WRAP)
        
        reaction = self.A * self.B * self.B
        
        dA = self.Da * laplaceA - reaction + feed * (1.0 - self.A)
        dB = self.Db * laplaceB + reaction - (kill + feed) * self.B
        
        dt = 1.0
        self.A += dA * dt
        self.B += dB * dt
        
        self.A = np.clip(self.A, 0, 1)
        self.B = np.clip(self.B, 0, 1)
        
        self.pattern_energy = float(np.var(self.B))
        
    def get_output(self, port_name):
        if port_name == 'image':
            colored = cv2.applyColorMap((self.B * 255).astype(np.uint8), cv2.COLORMAP_MAGMA)
            return colored.astype(np.float32) / 255.0
        elif port_name == 'pattern_energy':
            return self.pattern_energy
        return None

=== FILE: dendriticattentionnode.py ===

"""
Dendritic Attention Node - Adaptive attention system using dendritic growth principles
Place this file in the 'nodes' folder
Requires: pip install scipy
"""

import numpy as np
from PyQt6 import QtGui
import cv2
import time

import __main__
BaseNode = __main__.BaseNode
PA_INSTANCE = getattr(__main__, "PA_INSTANCE", None)

try:
    from scipy import stats
    SCIPY_AVAILABLE = True
except ImportError:
    SCIPY_AVAILABLE = False
    print("Warning: DendriticAttentionNode requires 'scipy'.")


def box_count(data, box_size):
    """Count boxes containing any part of the pattern."""
    S = np.add.reduceat(
        np.add.reduceat(data, np.arange(0, data.shape[0], box_size), axis=0),
        np.arange(0, data.shape[1], box_size), axis=1)
    return np.sum(S > 0)


def fractal_dimension(Z, min_box=2, max_box=None, step=2):
    """Compute fractal dimension using box-counting method."""
    Z = Z > Z.mean()
    
    if max_box is None:
        max_box = min(Z.shape) // 4
    
    max_box = min(max_box, min(Z.shape) // 2)
    min_box = max(2, min_box)
    
    if max_box <= min_box:
        return 1.0
        
    sizes = np.arange(min_box, max_box, step)
    if len(sizes) < 2:
        sizes = np.array([min_box, max_box-1])
        
    counts = []
    for size in sizes:
        count = box_count(Z, size)
        counts.append(max(1, count))

    try:
        log_sizes = np.log(sizes)
        log_counts = np.log(counts)
        slope, _, _, _, _ = stats.linregress(log_sizes, log_counts)
        return -slope
    except:
        return 1.0


class DendriticAttentionNode(BaseNode):
    NODE_CATEGORY = "Transform"
    NODE_COLOR = QtGui.QColor(180, 100, 200)  # Purple for neural
    
    def __init__(self, n_dendrites=1000, learning_rate=0.05):
        super().__init__()
        self.node_title = "Dendritic Attention"
        
        self.inputs = {
            'image_in': 'image',
            'reset': 'signal'
        }
        
        self.outputs = {
            'attention_field': 'image',
            'visualization': 'image',
            'match_score': 'signal',
            'stability': 'signal',
            'attention_width': 'signal',
            'exploration': 'signal',
            'fractal_dim': 'signal',
            'adj_0': 'signal',  # Frequency adjustments for external control
            'adj_1': 'signal',
            'adj_2': 'signal',
            'adj_3': 'signal',
            'adj_4': 'signal'
        }
        
        if not SCIPY_AVAILABLE:
            self.node_title = "Dendritic (No SciPy!)"
            return
        
        # Parameters
        self.input_size = (64, 64)
        self.n_dendrites = int(n_dendrites)
        self.learning_rate = float(learning_rate)
        
        # Initialize dendrites
        self.positions = np.random.rand(self.n_dendrites, 2) * np.array(self.input_size)
        self.directions = self._normalize(np.random.randn(self.n_dendrites, 2))
        self.strengths = np.ones(self.n_dendrites) * 0.5
        
        # Attention state
        self.attention_field = np.ones(self.input_size)
        self.expected_pattern = None
        self.memory_strength = 0.0
        
        # Metrics
        self.attention_width = 0.5
        self.stability_measure = 0.5
        self.exploration_rate = 0.5
        self.fractal_dim_value = 1.5
        
        # History
        self.activity_history = []
        self.match_history = []
        self.reset_time = time.time()
        
        # Response vectors for frequency adjustments
        self.response_vectors = np.random.randn(4, 5) * 0.1
        self.activity_vector = np.zeros(4)
        
        # Output buffers
        self.vis_output = np.zeros((*self.input_size, 3), dtype=np.uint8)
        
    def _normalize(self, vectors):
        """Normalize vectors to unit length."""
        norms = np.linalg.norm(vectors, axis=1, keepdims=True)
        return vectors / (norms + 1e-8)
    
    def _resize_input(self, input_data):
        """Resize input to internal resolution."""
        if input_data.shape != self.input_size:
            return cv2.resize(input_data, (self.input_size[1], self.input_size[0]), 
                            interpolation=cv2.INTER_AREA)
        return input_data
    
    def _compute_match(self, input_data, expected):
        """Calculate pattern match score."""
        if input_data.shape != expected.shape:
            return 0.0
        
        input_flat = input_data.flatten()
        expected_flat = expected.flatten()
        
        input_centered = input_flat - np.mean(input_flat)
        expected_centered = expected_flat - np.mean(expected_flat)
        
        numerator = np.dot(input_centered, expected_centered)
        denominator = np.sqrt(np.sum(input_centered**2) * np.sum(expected_centered**2))
        
        if denominator < 1e-8:
            return 0.0
            
        correlation = numerator / denominator
        return max(0, (correlation + 1) / 2)
    
    def _dilate_attention(self):
        """Update attention field (iris effect)."""
        x, y = np.meshgrid(
            np.linspace(-1, 1, self.input_size[1]),
            np.linspace(-1, 1, self.input_size[0])
        )
        
        distance = np.sqrt(x**2 + y**2)
        sigma = 0.2 + self.attention_width * 1.0
        self.attention_field = np.exp(-(distance**2 / (2.0 * sigma**2)))
    
    def _grow_dendrites(self, input_data):
        """Grow dendrites toward areas of high activity."""
        for i in range(self.n_dendrites):
            x, y = self.positions[i].astype(int) % self.input_size
            x = min(x, self.input_size[0] - 1)
            y = min(y, self.input_size[1] - 1)
            
            activity = input_data[x, y]
            
            # Update strength
            self.strengths[i] = 0.95 * self.strengths[i] + 0.05 * activity
            
            # Grow strong dendrites
            if self.strengths[i] > 0.3:
                # Calculate gradient
                grad_x, grad_y = 0, 0
                if x > 0 and x < self.input_size[0] - 1:
                    grad_x = input_data[x+1, y] - input_data[x-1, y]
                if y > 0 and y < self.input_size[1] - 1:
                    grad_y = input_data[x, y+1] - input_data[x, y-1]
                
                # Update direction
                if abs(grad_x) > 0.01 or abs(grad_y) > 0.01:
                    gradient = np.array([grad_x, grad_y])
                    gradient_norm = np.linalg.norm(gradient)
                    if gradient_norm > 0:
                        gradient = gradient / gradient_norm
                        self.directions[i] = 0.8 * self.directions[i] + 0.2 * gradient
                        self.directions[i] = self.directions[i] / (np.linalg.norm(self.directions[i]) + 1e-8)
                
                # Move dendrite
                growth_rate = self.strengths[i] * 0.1
                self.positions[i] += self.directions[i] * growth_rate
                self.positions[i] = self.positions[i] % np.array(self.input_size)
    
    def _extract_features(self, input_data):
        """Extract features for response calculation."""
        total_activity = np.mean(input_data * self.attention_field)
        
        h, w = self.input_size
        top_left = np.mean(input_data[:h//2, :w//2])
        top_right = np.mean(input_data[:h//2, w//2:])
        bottom_left = np.mean(input_data[h//2:, :w//2])
        bottom_right = np.mean(input_data[h//2:, w//2:])
        
        self.activity_vector = np.array([
            total_activity,
            top_left - bottom_right,
            top_right - bottom_left,
            self.stability_measure
        ])
        
        self.activity_history.append(total_activity)
        if len(self.activity_history) > 100:
            self.activity_history.pop(0)
    
    def _get_frequency_adjustments(self):
        """Calculate adjustments for external control."""
        raw_adjustments = np.dot(self.activity_vector, self.response_vectors)
        scaled = raw_adjustments * (0.5 + self.exploration_rate)
        
        # Add exploration oscillation
        time_factor = np.sin(time.time() * np.pi * 0.1)
        exploration_wave = np.sin(np.linspace(0, 2*np.pi, 5) + time_factor)
        scaled += exploration_wave * self.exploration_rate * 0.2
        
        # Add instability noise
        if self.stability_measure < 0.5:
            scaled += np.random.randn(5) * (0.5 - self.stability_measure) * 0.3
            
        return scaled
    
    def _generate_visualization(self):
        """Create RGB visualization."""
        vis_img = np.zeros((*self.input_size, 3), dtype=np.float32)
        
        # Blue: attention field
        vis_img[:, :, 2] = self.attention_field
        
        # Green: active dendrites
        for i in range(self.n_dendrites):
            if self.strengths[i] > 0.2:
                x, y = self.positions[i].astype(int) % self.input_size
                try:
                    vis_img[x, y, 1] = min(1.0, vis_img[x, y, 1] + self.strengths[i])
                except IndexError:
                    pass
        
        # Red: expected pattern
        if self.expected_pattern is not None:
            vis_img[:, :, 0] = self.expected_pattern * 0.7
        
        return (vis_img * 255).astype(np.uint8)
    
    def step(self):
        if not SCIPY_AVAILABLE:
            return
        
        # Check for reset
        reset_sig = self.get_blended_input('reset', 'sum')
        if reset_sig is not None and reset_sig > 0.5:
            self._reset()
            return
        
        # Get input
        input_img = self.get_blended_input('image_in', 'mean')
        if input_img is None:
            return
        
        # Resize to internal resolution
        input_data = self._resize_input(input_img)
        
        # Compute match with expected pattern
        if self.expected_pattern is not None:
            match_score = self._compute_match(input_data, self.expected_pattern)
            self.match_history.append(match_score)
            if len(self.match_history) > 50:
                self.match_history.pop(0)
        else:
            self.expected_pattern = input_data.copy()
            self.memory_strength = 0.1
            match_score = 1.0
            self.match_history = [1.0]
        
        # Update stability
        if len(self.match_history) > 5:
            match_variance = np.var(self.match_history[-5:])
            self.stability_measure = 1.0 - min(1.0, match_variance * 10)
        
        # Update attention width (iris effect)
        target_width = 0.3 if match_score > 0.7 else 0.8
        self.attention_width = 0.95 * self.attention_width + 0.05 * target_width
        
        # Update attention field
        self._dilate_attention()
        
        # Grow dendrites
        self._grow_dendrites(input_data)
        
        # Extract features
        self._extract_features(input_data)
        
        # Update expected pattern
        if self.expected_pattern is not None:
            self.expected_pattern = (0.9 * self.expected_pattern + 
                                   0.1 * input_data * self.attention_field)
        
        # Calculate fractal dimension
        vis_img = self._generate_visualization()
        red_channel = vis_img[:, :, 0]
        self.fractal_dim_value = fractal_dimension(red_channel)
        
        # Update exploration rate
        runtime = time.time() - self.reset_time
        base_exploration = max(0.1, 1.0 - min(1.0, runtime / 60.0))
        stability_factor = 1.0 - self.stability_measure
        self.exploration_rate = 0.7 * self.exploration_rate + 0.3 * (base_exploration + 0.5 * stability_factor)
        
        # Store visualization
        self.vis_output = vis_img
    
    def _reset(self):
        """Reset the attention system."""
        self.expected_pattern = None
        self.memory_strength = 0.0
        self.attention_width = 0.5
        self.stability_measure = 0.5
        self.activity_history = []
        self.match_history = []
        self.reset_time = time.time()
        self.strengths = np.ones(self.n_dendrites) * 0.5
        self.directions = self._normalize(np.random.randn(self.n_dendrites, 2))
        self.exploration_rate = 0.5
    
    def get_output(self, port_name):
        if port_name == 'attention_field':
            return self.attention_field
        elif port_name == 'visualization':
            return self.vis_output.astype(np.float32) / 255.0
        elif port_name == 'match_score':
            return np.mean(self.match_history[-5:]) if len(self.match_history) >= 5 else 0.5
        elif port_name == 'stability':
            return self.stability_measure
        elif port_name == 'attention_width':
            return self.attention_width
        elif port_name == 'exploration':
            return self.exploration_rate
        elif port_name == 'fractal_dim':
            return self.fractal_dim_value
        elif port_name.startswith('adj_'):
            idx = int(port_name.split('_')[1])
            adjustments = self._get_frequency_adjustments()
            return adjustments[idx] if idx < len(adjustments) else 0.0
        return None
    
    def get_display_image(self):
        # Show the visualization
        img_resized = cv2.resize(self.vis_output, (96, 96), interpolation=cv2.INTER_LINEAR)
        img_resized = np.ascontiguousarray(img_resized)
        
        h, w = img_resized.shape[:2]
        return QtGui.QImage(img_resized.data, w, h, 3*w, QtGui.QImage.Format.Format_RGB888)
    
    def get_config_options(self):
        return [
            ("Num Dendrites", "n_dendrites", self.n_dendrites, None),
            ("Learning Rate", "learning_rate", self.learning_rate, None),
        ]

=== FILE: depthfrommathematicsnode.py ===

"""
DepthFromMathematicsNode

Extracts 3D depth information from 2D mathematical properties:
- Distance transform (topology â height)
- Fractal dimension (complexity â relief)
- Gradients (orientation â surface normals)

Creates emergent 3D from pure mathematics.
"""

import numpy as np
import cv2

import __main__
BaseNode = __main__.BaseNode
QtGui = __main__.QtGui

class DepthFromMathematicsNode(BaseNode):
    """
    Converts 2D mathematical structure into 3D depth map.
    Pure emergence - no 3D modeling required.
    """
    NODE_CATEGORY = "Transform"
    NODE_COLOR = QtGui.QColor(100, 200, 250)  # Sky blue
    
    def __init__(self, size=256):
        super().__init__()
        self.node_title = "Depth from Math"
        
        self.inputs = {
            'image_in': 'image',           # Binary or grayscale structure
            'fractal_dim': 'signal',       # Fractal dimension (complexity)
            'complexity': 'signal',        # Additional complexity measure
            'depth_scale': 'signal',       # Depth exaggeration (0-1)
            'relief_strength': 'signal'    # How much fractal affects depth
        }
        
        self.outputs = {
            'heightmap': 'image',          # Grayscale depth map
            'shaded': 'image',             # 3D-shaded version (RGB)
            'normals': 'image',            # Surface normals visualization
            'max_depth': 'signal',         # Maximum depth value
            'depth_variance': 'signal'     # Std dev of depth
        }
        
        self.size = int(size)
        self.heightmap = np.zeros((self.size, self.size), dtype=np.float32)
        self.shaded_img = np.zeros((self.size, self.size, 3), dtype=np.float32)
        self.normal_map_vis = np.zeros((self.size, self.size, 3), dtype=np.float32)

    def step(self):
        image = self.get_blended_input('image_in', 'first')
        if image is None:
            self.heightmap = np.zeros((self.size, self.size), dtype=np.float32)
            self.shaded_img = np.zeros((self.size, self.size, 3), dtype=np.float32)
            self.normal_map_vis = np.zeros((self.size, self.size, 3), dtype=np.float32)
            return

        # --- START FIX for CV_64F Error ---
        # 1. Convert to float32 if it isn't already
        if image.dtype != np.float32:
            # This will catch float64 (the error) and uint8 (common)
            image = image.astype(np.float32)

        # 2. Normalize to 0-1 if it's in 0-255 range
        if image.max() > 1.0:
            image = image / 255.0
            
        image = np.clip(image, 0, 1) # Ensure range
        # --- END FIX ---

        # Resize (This is now safe)
        image = cv2.resize(image, (self.size, self.size), interpolation=cv2.INTER_LINEAR)

        # --- 7. Convert to Grayscale ---
        if image.ndim == 3:
            # This line (76) is now safe
            image = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
        
        # Binarize
        binary_img = (image > 0.5).astype(np.uint8) * 255
        
        # --- 1. Topology â Height (Distance Transform) ---
        dist_transform = cv2.distanceTransform(binary_img, cv2.DIST_L2, 3)
        
        # Normalize
        if dist_transform.max() > 0:
            dist_norm = dist_transform / dist_transform.max()
        else:
            dist_norm = dist_transform
        
        # --- 2. Complexity â Relief (Fractal Dimension) ---
        fdim = self.get_blended_input('fractal_dim', 'sum') or 1.5
        complexity = self.get_blended_input('complexity', 'sum') or 0.5
        depth_scale = self.get_blended_input('depth_scale', 'sum') or 0.5
        relief_strength = self.get_blended_input('relief_strength', 'sum') or 0.5
        
        # Combine complexity measures
        # fdim 1.0 (line) -> low complexity
        # fdim 2.0 (plane) -> high complexity
        fdim_norm = (fdim - 1.0)
        complexity_mod = (fdim_norm + complexity) * relief_strength
        
        # Apply relief: more complex = "hillier" distance field
        heightmap = np.power(dist_norm, 1.0 + complexity_mod)
        
        # Apply depth scale
        self.heightmap = heightmap * (depth_scale + 0.5) # Scale 0.5 to 1.5
        self.heightmap = np.clip(self.heightmap, 0, 1)

        # --- 3. Orientation â Normals (Gradients) ---
        sobel_x = cv2.Sobel(self.heightmap, cv2.CV_32F, 1, 0, ksize=5)
        sobel_y = cv2.Sobel(self.heightmap, cv2.CV_32F, 0, 1, ksize=5)
        
        # Create normal vectors [Nx, Ny, Nz]
        # Nz is "up", set to 1.0 for a gentle slope
        normal_map = np.dstack((-sobel_x, -sobel_y, np.full(self.heightmap.shape, 1.0)))
        
        # Normalize vectors to length 1
        norms = np.linalg.norm(normal_map, axis=2, keepdims=True)
        norms[norms == 0] = 1.0 # Avoid divide-by-zero
        normal_map /= norms
        
        # --- 4. Create Shaded Image (Phong-like) ---
        light_dir = np.array([0.5, 0.5, 1.0]) # Light from top-right
        light_dir /= np.linalg.norm(light_dir)
        
        # Calculate diffuse light (dot product of normal and light dir)
        diffuse = np.dot(normal_map, light_dir)
        diffuse = np.clip(diffuse, 0, 1) # Light can't be negative
        
        # Add ambient light
        ambient = 0.2
        lighting = ambient + (diffuse * (1.0 - ambient))
        
        # Apply lighting to original structure
        color_img = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)
        self.shaded_img = color_img * lighting[..., np.newaxis]
        self.shaded_img = np.clip(self.shaded_img, 0, 1)
        
        # --- 5. Create Normal Map Visualization ---
        # Map normals [-1, 1] to color [0, 1]
        self.normal_map_vis = (normal_map * 0.5 + 0.5)
        
    def get_output(self, port_name):
        if port_name == 'heightmap':
            return self.heightmap
        elif port_name == 'shaded':
            return self.shaded_img
        elif port_name == 'normals':
            return self.normal_map_vis
        elif port_name == 'max_depth':
            return np.max(self.heightmap)
        elif port_name == 'depth_variance':
            return np.var(self.heightmap)
        return None

# --- Minimalist Contour Node for Pipeline 2 ---
# (Included here so file is self-contained with examples)

class ContourMomentsMini(BaseNode):
    NODE_CATEGORY = "Analyzer"
    NODE_COLOR = QtGui.QColor(220, 200, 100)

    def __init__(self, size=128):
        super().__init__()
        self.node_title = "Contour Moments (Mini)"
        self.inputs = {'image_in': 'image'}
        self.outputs = {
            'center_x': 'signal', 'center_y': 'signal',
            'area': 'signal', 'orientation': 'signal',
            'eccentricity': 'signal', 'circularity': 'signal',
            'vis': 'image'
        }
        self.size = int(size)
        self.center_x, self.center_y, self.area, self.orientation, self.eccentricity, self.circularity = 0, 0, 0, 0, 0, 0
        self.vis = np.zeros((size, size, 3), dtype=np.float32)

    def step(self):
        img = self.get_blended_input('image_in', 'first')
        if img is None: return

        if img.dtype != np.float32: img = img.astype(np.float32)
        if img.max() > 1.0: img /= 255.0
        
        img = cv2.resize(img, (self.size, self.size))
        if img.ndim == 3: img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        
        _, binary = cv2.threshold((img * 255).astype(np.uint8), 127, 255, cv2.THRESH_BINARY)
        
        self.vis = cv2.cvtColor(binary, cv2.COLOR_GRAY2RGB).astype(np.float32) / 255.0
        
        moments = cv2.moments(binary)
        m00 = moments['m00']
        
        if m00 > 0:
            self.area = m00 / (self.size * self.size)
            cx = moments['m10'] / m00
            cy = moments['m01'] / m00
            self.center_x = (cx / self.size) * 2.0 - 1.0
            self.center_y = (cy / self.size) * 2.0 - 1.0

            mu20, mu02, mu11 = moments['mu20'], moments['mu02'], moments['mu11']
            term = np.sqrt((mu20 - mu02)**2 + 4 * mu11**2)
            lambda1 = 0.5 * (mu20 + mu02 + term)
            lambda2 = 0.5 * (mu20 + mu02 - term)
            
            self.orientation = 0.5 * np.arctan2(2 * mu11, mu20 - mu02) / (np.pi / 2.0)
            if lambda1 > 0: self.eccentricity = np.sqrt(1.0 - (lambda2 / lambda1))
            
            contours, _ = cv2.findContours(binary, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)
            if contours:
                cnt = max(contours, key=cv2.contourArea)
                perimeter = cv2.arcLength(cnt, True)
                if perimeter > 0:
                    self.circularity = 4 * np.pi * (m00 / (perimeter**2))
            
            cv2.circle(self.vis, (int(cx), int(cy)), 3, (0, 1, 0), -1)
        else:
            self.area, self.center_x, self.center_y, self.orientation, self.eccentricity, self.circularity = 0, 0, 0, 0, 0, 0

    def get_output(self, port_name):
        if port_name == 'center_x':
            return self.center_x
        elif port_name == 'center_y':
            return self.center_y
        elif port_name == 'area':
            return self.area
        elif port_name == 'orientation':
            return self.orientation
        elif port_name == 'eccentricity':
            return self.eccentricity
        elif port_name == 'circularity':
            return self.circularity
        elif port_name == 'vis':
            return self.vis
        return None


"""
USAGE:

Pipeline 1: Pure Depth Extraction
  Webcam â Moire â Filament Boxcounter â DepthFromMath â HeightmapFlyer
  
  The fractal structure becomes 3D terrain automatically.

Pipeline 2: Geometry-Driven Control
  Filament â ContourMoments â Various outputs
  
  center_x/y â ParticleAttractor (structure attracts particles)
  orientation â Julia c_real (structure controls fractal)
  eccentricity â Audio amplitude
  area â Visual brightness

Pipeline 3: Full 3D Emergence
  Webcam â Moire â Filament â ContourMoments
                              â DepthFromMath (with fractal_dim)
                              â HeightmapFlyer
  
  Contour geometry feeds depth generation,
  creating fully emergent 3D from pure mathematics.

WHY IT WORKS:

The 3D is NOT programmed. It EMERGES from:

1. Distance transform: Topology encodes natural height
2. Fractal dimension: Complexity modulates relief
3. Gradients: Orientation becomes surface normals
4. Phong shading: Normals create lighting cues

Your brain receives:
- Shading cues (Phong lighting)
- Perspective cues (HeightmapFlyer)
- Motion cues (if animated)
- Texture cues (original structure)

All from pure 2D mathematics. No 3D modeling.
The depth was ALWAYS THERE in the topology.
We just made it VISIBLE.
"""

=== FILE: displacementwarpnode.py ===

"""
DisplacementWarpNode

Uses a heightmap to "pop out" or distort a texture,
creating a powerful, liquid-like 3D effect.
"""

import numpy as np
import cv2

# --- Magic import block ---
import __main__
BaseNode = __main__.BaseNode
QtGui = __main__.QtGui
# --------------------------

class DisplacementWarpNode(BaseNode):
    """
    Distorts an image based on a heightmap.
    """
    NODE_CATEGORY = "Filter"
    NODE_COLOR = QtGui.QColor(150, 100, 220) # Purple

    def __init__(self, size=256):
        super().__init__()
        self.node_title = "Displacement Warp"
        
        self.inputs = {
            'image_in': 'image',      # The texture (e.g., checkerboard)
            'heightmap_in': 'image',  # The displacement map (e.g., your pyramid)
            'strength': 'signal'      # 0-1, how much to distort
        }
        self.outputs = {'image': 'image'}
        
        self.size = int(size)
        
        # Pre-calculate grids
        y, x = np.mgrid[0:self.size, 0:self.size]
        self.grid_x = x.astype(np.float32)
        self.grid_y = y.astype(np.float32)
        
        # --- START FIX ---
        # Initialize the output variable so it exists before step() runs
        self.display_image = np.zeros((self.size, self.size, 3), dtype=np.float32)
        # --- END FIX ---

    def _prepare_image(self, img):
        """Helper to resize and format an input image."""
        if img is None:
            return None
        
        # Ensure float32 in 0-1 range
        if img.dtype != np.float32:
            img = img.astype(np.float32)
        if img.max() > 1.0:
            img = img / 255.0
            
        img_resized = cv2.resize(img, (self.size, self.size), 
                                 interpolation=cv2.INTER_LINEAR)
        return np.clip(img_resized, 0, 1)

    def step(self):
        # --- 1. Get Images and Controls ---
        img_texture = self._prepare_image(self.get_blended_input('image_in', 'first'))
        img_heightmap = self._prepare_image(self.get_blended_input('heightmap_in', 'first'))
        
        strength = (self.get_blended_input('strength', 'sum') or 0.2) * 100.0 # Scale to pixels
        
        # --- 2. Handle Missing Inputs ---
        if img_texture is None:
            # If no texture, just show the heightmap
            self.display_image = img_heightmap if img_heightmap is not None else \
                                 np.zeros((self.size, self.size, 3), dtype=np.float32)
            return
            
        if img_heightmap is None:
            # If no heightmap, just pass the texture through
            self.display_image = img_texture
            return
            
        # Ensure heightmap is grayscale
        if img_heightmap.ndim == 3:
            img_heightmap_gray = cv2.cvtColor(img_heightmap, cv2.COLOR_RGB2GRAY)
        else:
            img_heightmap_gray = img_heightmap
            
        # --- 3. Apply Displacement ---
        # Where heightmap is "high" (1.0), this will be a large offset
        # Where it's "low" (0.0), this will be 0 offset
        displacement = img_heightmap_gray * strength
        
        # Create the remap "flow"
        # We "push" pixels outwards from the center of the height
        map_x = (self.grid_x + displacement).astype(np.float32)
        map_y = (self.grid_y + displacement).astype(np.float32)
        
        # --- 4. Apply Warp ---
        self.display_image = cv2.remap(
            img_texture, map_x, map_y, 
            cv2.INTER_LINEAR, 
            borderMode=cv2.BORDER_REFLECT_101 # Reflects for cool psychedelic tiling
        )

    def get_output(self, port_name):
        if port_name == 'image':
            return self.display_image
        return None

=== FILE: display_nodes.py ===

"""
Display Nodes - Image viewer and signal plotter
Place this file in the 'nodes' folder
"""

import numpy as np
from PyQt6 import QtGui
import cv2
from collections import deque

import sys
import os
# --- This is the new, correct block ---
import __main__
BaseNode = __main__.BaseNode
PA_INSTANCE = getattr(__main__, "PA_INSTANCE", None)
# ------------------------------------

class ImageDisplayNode(BaseNode):
    NODE_CATEGORY = "Output"
    NODE_COLOR = QtGui.QColor(120, 40, 120) # Output Purple
    
    def __init__(self, width=160, height=120):
        super().__init__()
        self.node_title = "Image Display"
        self.inputs = {'image': 'image'}
        self.w, self.h = width, height
        self.img = np.zeros((self.h, self.w), dtype=np.float32)
        
    def step(self):
        img = self.get_blended_input('image', 'first')
        if img is not None:
            if img.shape != (self.h, self.w):
                # Use cv2.resize for robustness
                img = cv2.resize(img, (self.w, self.h), interpolation=cv2.INTER_NEAREST)
            self.img = img
        else:
            self.img *= 0.95 # Fade to black
            
    def get_display_image(self):
        img_u8 = (np.clip(self.img, 0, 1) * 255).astype(np.uint8)
        img_u8 = np.ascontiguousarray(img_u8)
        return QtGui.QImage(img_u8.data, self.w, self.h, self.w, QtGui.QImage.Format.Format_Grayscale8)

class SignalMonitorNode(BaseNode):
    NODE_CATEGORY = "Output"
    NODE_COLOR = QtGui.QColor(120, 40, 120) # Output Purple
    
    def __init__(self, history_len=500):
        super().__init__()
        self.node_title = "Signal Monitor"
        self.inputs = {'signal': 'signal'}
        self.history = deque(maxlen=history_len)
        self.history_len = history_len
        
    def step(self):
        val = self.get_blended_input('signal', 'sum') or 0.0
        
        # Handle potential arrays from mean blending
        if isinstance(val, np.ndarray):
            val = val.mean()
            
        self.history.append(float(val))
            
    def get_display_image(self):
        w, h = 64, 32 # Small preview
        img = np.zeros((h, w), dtype=np.uint8)
        if len(self.history) > 1:
            # Use last w samples
            history_array = np.array(list(self.history))
            if len(history_array) > w:
                history_array = history_array[-w:]
            
            min_val, max_val = np.min(history_array), np.max(history_array)
            range_val = max_val - min_val
            
            if range_val > 1e-6:
                vis_history = (history_array - min_val) / range_val
            else:
                vis_history = np.full_like(history_array, 0.5) 
            
            for i in range(len(vis_history) - 1):
                val1 = vis_history[i]
                y1 = int((1 - val1) * (h-1)) 
                x1 = int(i * (w / len(vis_history)))
                y1 = np.clip(y1, 0, h-1)
                img[y1, x1] = 255

        img = np.ascontiguousarray(img)
        return QtGui.QImage(img.data, w, h, w, QtGui.QImage.Format.Format_Grayscale8)

=== FILE: distancefieldnode.py ===

"""
DistanceFieldNode

Calculates the Euclidean distance from every pixel to the
nearest "on" pixel (filament) in a binary image.
"""

import numpy as np
import cv2

# --- Magic import block ---
import __main__
BaseNode = __main__.BaseNode
QtGui = __main__.QtGui
# --------------------------

class DistanceFieldNode(BaseNode):
    """
    Generates a distance transform (field) from an image's filaments.
    """
    NODE_CATEGORY = "Filter"
    NODE_COLOR = QtGui.QColor(150, 200, 100) # Olive

    def __init__(self, size=256):
        super().__init__()
        self.node_title = "Distance Field"
        
        self.inputs = {
            'image_in': 'image',
            'threshold': 'signal', # 0-1, to find the "filaments"
            'invert': 'signal'     # 0 = distance from filaments, 1 = distance from empty
        }
        self.outputs = {'image': 'image'}
        
        self.size = int(size)
        self.display_image = np.zeros((self.size, self.size, 3), dtype=np.float32)

    def step(self):
        # --- 1. Get and Prepare Image ---
        img = self.get_blended_input('image_in', 'first')
        if img is None:
            return # Do nothing if no image

        # Resize for consistency
        img_resized = cv2.resize(img, (self.size, self.size), 
                                 interpolation=cv2.INTER_LINEAR)
        
        # Convert to grayscale
        if img_resized.ndim == 3:
            img_gray = cv2.cvtColor(img_resized, cv2.COLOR_RGB2GRAY)
        else:
            img_gray = img_resized
            
        # Ensure 0-1 float
        if img_gray.max() > 1.0:
            img_gray = img_gray.astype(np.float32) / 255.0
        
        # --- 2. Get Binary Image ---
        threshold = self.get_blended_input('threshold', 'sum') or 0.5
        invert = self.get_blended_input('invert', 'sum') or 0.0
        
        _ , binary_img = cv2.threshold(
            (img_gray * 255).astype(np.uint8), 
            int(threshold * 255), 
            255, 
            cv2.THRESH_BINARY
        )
        
        if invert > 0.5:
            binary_img = cv2.bitwise_not(binary_img)
        
        # --- 3. Calculate Distance Transform ---
        # This is the core of the node.
        # It calculates the distance for each pixel to the nearest 0-pixel.
        # We want the distance to the nearest NON-ZERO pixel, so we invert
        # the binary image first.
        dist_transform = cv2.distanceTransform(cv2.bitwise_not(binary_img), 
                                               cv2.DIST_L2, # Euclidean
                                               3) # 3x3 mask
        
        # --- 4. Normalize and Display ---
        # Normalize the distance field to 0-1 range to be a viewable image
        if dist_transform.max() > 0:
            dist_norm = dist_transform / dist_transform.max()
        else:
            dist_norm = dist_transform
        
        # Use a colormap to make it look cool
        colored = cv2.applyColorMap((dist_norm * 255).astype(np.uint8), 
                                    cv2.COLORMAP_MAGMA)
        
        self.display_image = colored.astype(np.float32) / 255.0

    def get_output(self, port_name):
        if port_name == 'image':
            return self.display_image
        return None

=== FILE: documentationnode.py ===

"""
Documentation Node - Displays user-defined text for documenting a graph.
The text is saved with the graph file.
"""
import cv2
import numpy as np
from PyQt6 import QtGui
from PIL import Image, ImageDraw, ImageFont
import sys
import os

import __main__
BaseNode = __main__.BaseNode
PA_INSTANCE = getattr(__main__, "PA_INSTANCE", None)
QtGui = __main__.QtGui

class DocumentationNode(BaseNode):
    NODE_CATEGORY = "Utility"
    NODE_COLOR = QtGui.QColor(50, 50, 50) # Dark Gray for background utility
    
    def __init__(self, doc_text="[Graph Documentation]", width=200, height=100):
        super().__init__()
        self.node_title = "Documentation"
        
        # --- FIX: Use a simple output to force redraw ---
        self.outputs = {'refresh_flag': 'signal'}
        self.initial_refresh_counter = 5 # Pulse high for the first 5 frames
        # --- END FIX ---
        
        self.doc_text = str(doc_text)
        self.w, self.h = int(width), int(height)
        
        try:
            self.font = ImageFont.load_default()
        except IOError:
            self.font = None 

    def step(self):
        # Consume the initial refresh counter to force an update
        if self.initial_refresh_counter > 0:
            self.initial_refresh_counter -= 1
        pass

    def get_output(self, port_name):
        if port_name == 'refresh_flag':
            # Signal high for a few frames when first loading/running
            return 1.0 if self.initial_refresh_counter > 0 else 0.0
        return None
        
    def get_display_image(self):
        # Create a blank image
        img = np.zeros((self.h, self.w), dtype=np.uint8)
        img_pil = Image.fromarray(img)
        draw = ImageDraw.Draw(img_pil)
        
        text_lines = self.doc_text.split('\n')
        y_pos = 5
        
        font_to_use = self.font if self.font else ImageFont.load_default()

        try:
            for line in text_lines:
                draw.text((5, y_pos), line, fill=255, font=font_to_use)
                y_pos += 15
        except Exception:
            draw.text((5, 5), self.doc_text, fill=255, font=font_to_use)

        img = np.array(img_pil)
        img = np.ascontiguousarray(img)
        
        # Add border to distinguish it from the background
        cv2.rectangle(img, (0, 0), (self.w - 1, self.h - 1), 100, 1)
        
        return QtGui.QImage(img.data, self.w, self.h, self.w, QtGui.QImage.Format.Format_Grayscale8)

    def get_config_options(self):
        return [
            ("Documentation Text", "doc_text", self.doc_text, None),
            ("Width", "w", self.w, None),
            ("Height", "h", self.h, None),
        ]

=== FILE: dontyoucomearoundherenomore.py ===

"""
PsychedelicWarpNode

Applies a "liquid" sinusoidal warp, color-cycling,
and video feedback to an image. Perfect for that
'melting checkerboard' effect.
"""

import numpy as np
import cv2

# --- Magic import block ---
import __main__
BaseNode = __main__.BaseNode
QtGui = __main__.QtGui
# --------------------------

class PsychedelicWarpNode(BaseNode):
    """
    Applies a "liquid" psychedelic distortion filter.
    """
    NODE_CATEGORY = "Filter"
    NODE_COLOR = QtGui.QColor(220, 100, 220) # Psychedelic Magenta

    def __init__(self, size=256):
        super().__init__()
        self.node_title = "Psychedelic Warp"
        
        self.inputs = {
            'image_in': 'image',
            'warp_speed': 'signal',   # How fast the "liquid" moves
            'warp_strength': 'signal',# How much the image distorts
            'feedback': 'signal',     # 0 (no trails) to 1 (infinite trails)
            'hue_shift': 'signal'     # -1 to 1, speed of color cycling
        }
        self.outputs = {'image': 'image'}
        
        self.size = int(size)
        
        # Internal buffer for feedback
        self.buffer = np.zeros((self.size, self.size, 3), dtype=np.float32)
        
        # Internal "time" for warp animation
        self.t = 0.0
        
        # Pre-calculate grids
        y, x = np.mgrid[0:self.size, 0:self.size]
        self.grid_x = x.astype(np.float32)
        self.grid_y = y.astype(np.float32)

    def step(self):
        # --- 1. Get Control Signals ---
        warp_speed = self.get_blended_input('warp_speed', 'sum') or 0.2
        warp_strength = (self.get_blended_input('warp_strength', 'sum') or 0.3) * 50.0
        feedback = self.get_blended_input('feedback', 'sum') or 0.9
        hue_shift = (self.get_blended_input('hue_shift', 'sum') or 0.05) * 10.0
        
        # Clamp feedback to prevent 1.0 (which would block new images)
        feedback_amount = np.clip(feedback, 0.0, 0.98)

        # --- 2. Get and Prepare Input Image ---
        img = self.get_blended_input('image_in', 'first')
        if img is None:
            # If no input, just fade the buffer
            self.buffer *= feedback_amount
            return

        # Resize and format
        img_resized = cv2.resize(img, (self.size, self.size), 
                                 interpolation=cv2.INTER_LINEAR)
        if img_resized.ndim == 2:
            img_resized = cv2.cvtColor(img_resized, cv2.COLOR_GRAY2RGB)
        
        if img_resized.dtype != np.float32:
            img_resized = img_resized.astype(np.float32)
        if img_resized.max() > 1.0:
            img_resized /= 255.0
            
        img_resized = np.clip(img_resized, 0, 1)
        
        # --- 3. Apply Psychedelic Color Shift ---
        # Convert to HSV, shift Hue, convert back
        img_hsv = cv2.cvtColor(img_resized, cv2.COLOR_RGB2HSV)
        
        # Add hue shift (and wrap around 0-180)
        img_hsv[:, :, 0] = (img_hsv[:, :, 0] + hue_shift) % 180.0
        
        processed_input = cv2.cvtColor(img_hsv, cv2.COLOR_HSV2RGB)

        # --- 4. Create Liquid Warp ---
        self.t += warp_speed * 0.1
        
        # Create a moving, sinusoidal displacement map
        dx = np.sin((self.grid_y / 20.0) + self.t) * warp_strength
        dy = np.cos((self.grid_x / 20.0) + self.t) * warp_strength
        
        map_x = (self.grid_x + dx).astype(np.float32)
        map_y = (self.grid_y + dy).astype(np.float32)
        
        # --- 5. Apply Warp and Feedback ---
        # Warp the *last* frame (the buffer)
        warped_buffer = cv2.remap(
            self.buffer, map_x, map_y, 
            cv2.INTER_LINEAR, 
            borderMode=cv2.BORDER_REFLECT_101
        )
        
        # --- 6. Blend ---
        # Blend the warped old frame with the new color-shifted frame
        self.buffer = (warped_buffer * feedback_amount) + \
                     (processed_input * (1.0 - feedback_amount))
        
        self.buffer = np.clip(self.buffer, 0, 1)

    def get_output(self, port_name):
        if port_name == 'image':
            return self.buffer
        return None

=== FILE: emergent_gravity.py ===

"""
Emergent Gravity Node - Simulates a 2D potential field from constraint density
Implements the $\rho_C$ -> $T_{\mu\nu}^{(C)}$ -> $G_{\mu\nu}$ link from the IHT-AI paper
in a simplified, real-time 2D model.

Place this file in the 'nodes' folder
"""

import numpy as np
from PyQt6 import QtGui
import cv2

import __main__
BaseNode = __main__.BaseNode
PA_INSTANCE = getattr(__main__, "PA_INSTANCE", None)

class EmergentGravityNode(BaseNode):
    NODE_CATEGORY = "Transform"
    NODE_COLOR = QtGui.QColor(60, 60, 100)  # Dark, "heavy" blue
    
    def __init__(self, g_coupling=1.0, blur_strength=21):
        super().__init__()
        self.node_title = "Emergent Gravity"
        
        self.inputs = {
            'constraint_density': 'image', # $\rho_C$ from IHTPhaseFieldNode
            'g_coupling': 'signal'         # Gravitational constant G
        }
        
        self.outputs = {
            'gravity_potential': 'image',   # The $\Phi$ field (potential well)
            'curvature_field': 'image',   # Approx. $\nabla^2\Phi$ (spacetime bending)
            'total_mass': 'signal'        # Total integrated constraint $\int \rho_C$
        }
        
        self.g_coupling = float(g_coupling)
        self.blur_strength = int(blur_strength)
        
        # Internal state
        self.potential_field = None
        self.curvature_field = None
        self.total_mass = 0.0

    def _normalize_for_vis(self, field):
        """Safely normalize a 2D field to [0, 1] for image output."""
        if field is None:
            return None # Return None, not a default array
        
        min_v, max_v = field.min(), field.max()
        range_v = max_v - min_v
        
        if range_v < 1e-9:
            return np.zeros_like(field, dtype=np.float32)
            
        return (field - min_v) / range_v
        
    def step(self):
        # Update parameters from inputs
        g_signal = self.get_blended_input('g_coupling', 'sum')
        if g_signal is not None:
            # Map signal [-1, 1] to a positive range [0, 2]
            self.g_coupling = (g_signal + 1.0)
            
        rho_c = self.get_blended_input('constraint_density', 'mean')
        
        if rho_c is None:
            if self.potential_field is not None:
                self.potential_field *= 0.95
            if self.curvature_field is not None: # Check before multiplying
                self.curvature_field *= 0.95
            self.total_mass *= 0.95
            return
            
        # Ensure blur strength is odd
        if self.blur_strength % 2 == 0:
            self.blur_strength += 1
            
        # 1. Calculate Total "Mass" (Total Constraint)
        self.total_mass = np.sum(rho_c)
        
        # 2. Calculate Gravitational Potential $\Phi$
        # A Gaussian blur is a fast, real-time approximation of the
        # gravitational potential well created by the mass density $\rho_C$.
        self.potential_field = cv2.GaussianBlur(
            rho_c, 
            (self.blur_strength, self.blur_strength), 
            0
        )
        
        # 3. Calculate Curvature (Approx. $\nabla^2\Phi$)
        # The Laplacian of the potential field shows where the potential
        # is "bending" the most, i.e., the curvature.
        
        # --- FIX ---
        # Destination depth (cv2.CV_64F) must match the source depth (np.float64)
        self.curvature_field = cv2.Laplacian(self.potential_field, cv2.CV_64F, ksize=3)
        # --- END FIX ---

        # Apply coupling constant
        self.potential_field *= self.g_coupling
        self.curvature_field *= self.g_coupling
        
    def get_output(self, port_name):
        if port_name == 'gravity_potential':
            # Normalize to float32 for other nodes
            norm_field = self._normalize_for_vis(self.potential_field)
            return norm_field.astype(np.float32) if norm_field is not None else None
            
        elif port_name == 'curvature_field':
            # Curvature can be positive or negative, so we take abs()
            # Check for None before np.abs()
            if self.curvature_field is None:
                return None
            norm_field = self._normalize_for_vis(np.abs(self.curvature_field))
            # Normalize to float32 for other nodes
            return norm_field.astype(np.float32) if norm_field is not None else None
            
        elif port_name == 'total_mass':
            return self.total_mass
            
        return None
        
    def get_display_image(self):
        # We visualize the curvature field, as it's more dynamic
        
        # Check if self.curvature_field is None before calling np.abs
        if self.curvature_field is None:
            vis_field = None
        else:
            vis_field = np.abs(self.curvature_field)
            
        vis_field_normalized = self._normalize_for_vis(vis_field)
        
        if vis_field_normalized is None:
             vis_field_normalized = np.zeros((64, 64), dtype=np.float32)

        img_u8 = (vis_field_normalized * 255).astype(np.uint8)
        
        # Apply a colormap to make it look "gravitational"
        img_color = cv2.applyColorMap(img_u8, cv2.COLORMAP_BONE)
        
        img_color = np.ascontiguousarray(img_color)
        h, w = img_color.shape[:2]
        return QtGui.QImage(img_color.data, w, h, 3*w, QtGui.QImage.Format.Format_BGR888)

    def get_config_options(self):
        return [
            ("G Coupling (Strength)", "g_coupling", self.g_coupling, None),
            ("Blur (Range)", "blur_strength", self.blur_strength, None),
        ]


=== FILE: emergentrealitynode.py ===

"""
Emergent Reality Node - Simulates "Reality as a Living Computation"
Ported from live.py. Models emergent physics (mass, energy, spacetime speed)
from iterative non-linear wave computations.

Outputs key fields (Intensity, Processing Speed) as images and global
metrics (Energy, Curvature) as signals.
Requires: pip install numpy scipy
"""

import numpy as np
from PyQt6 import QtGui
import cv2
import sys
import os
import random
from scipy.fft import fft2, ifft2, fftfreq
from scipy.ndimage import gaussian_filter

import __main__
BaseNode = __main__.BaseNode
PA_INSTANCE = getattr(__main__, "PA_INSTANCE", None)
QtGui = __main__.QtGui

try:
    from scipy.fft import fft2, ifft2, fftfreq
    from scipy.ndimage import gaussian_filter
    SCIPY_AVAILABLE = True
except ImportError:
    SCIPY_AVAILABLE = False
    print("Warning: EmergentRealityNode requires 'scipy'.")


# --- Core Simulation Classes (from live.py) ---

class RealitySimulator:
    def __init__(self, size=64, dt=0.005, c0=1.0, domain_size=10.0):
        self.size = size
        self.dt = dt
        self.c0 = c0  # Base processing speed
        self.domain_size = domain_size
        
        self.x = np.linspace(-domain_size, domain_size, size)
        self.y = np.linspace(-domain_size, domain_size, size)
        self.X, self.Y = np.meshgrid(self.x, self.y)
        
        kx = fftfreq(size, d=(self.x[1] - self.x[0])) * 2 * np.pi
        ky = fftfreq(size, d=(self.y[1] - self.y[0])) * 2 * np.pi
        self.KX, self.KY = np.meshgrid(kx, ky)
        self.K_squared = self.KX**2 + self.KY**2
        
        self.phi = np.zeros((size, size), dtype=complex)
        self.phi_prev = self.phi.copy() # For better stability
        
        # Physics parameters (simplified from live.py)
        self.alpha_quantum = 0.01
        self.alpha_gravity = 2.0
        self.current_alpha = self.alpha_gravity # Start in a stable regime
        
        self.a = 0.8   # Linear coefficient
        self.b = 0.05  # Nonlinear coefficient
        self.damping = 0.001
        
        self.time = 0
        self.step_count = 0
        
        # Initial seeding
        self.create_initial_state()
        
    def create_initial_state(self):
        """Seed the field with a couple of stable structures"""
        self.phi.fill(0)
        self.create_particle_cluster(center_x=-2, center_y=0, num_particles=3)
        self.create_massive_object(x_pos=2, y_pos=0, mass=5.0)
        self.add_quantum_foam(strength=0.1)

    def effective_speed_squared(self):
        """cÂ²_eff = câÂ² / (1 + Î±|Î¦|Â²). Emergent spacetime metric."""
        phi_intensity = np.abs(self.phi)**2
        return self.c0**2 / (1 + self.current_alpha * phi_intensity)
    
    def create_particle_cluster(self, center_x=0, center_y=0, num_particles=3, spread=1.0, amplitude=1.5):
        """Create particle-like solitons (simplified)"""
        for i in range(num_particles):
            angle = 2 * np.pi * i / num_particles + random.random() * 0.5
            r = spread * random.random()
            x_pos = center_x + r * np.cos(angle)
            y_pos = center_y + r * np.sin(angle)
            
            r_from_center = np.sqrt((self.X - x_pos)**2 + (self.Y - y_pos)**2)
            envelope = amplitude * np.exp(-r_from_center**2 / 1.0)
            
            particle = envelope * np.exp(1j * 0.5 * (self.X - x_pos))
            self.phi += particle
            
    def create_massive_object(self, x_pos=0, y_pos=0, mass=5.0, width=3.0):
        """Create a massive object that warps spacetime significantly"""
        r_from_center = np.sqrt((self.X - x_pos)**2 + (self.Y - y_pos)**2)
        envelope = mass * np.exp(-r_from_center**2 / (2 * width**2))
        
        theta = np.arctan2(self.Y - y_pos, self.X - x_pos)
        spiral_phase = 0.2 * theta
        
        massive_object = envelope * np.exp(1j * spiral_phase)
        self.phi += massive_object

    def add_quantum_foam(self, strength=0.05):
        """Add continuous random fluctuations (simplified noise)"""
        if strength > 0.0:
            noise_real = np.random.randn(self.size, self.size) * strength
            self.phi += noise_real
    
    def wave_equation_step(self):
        """The core processing step (modified Klein-Gordon/Non-linear SchrÃ¶dinger)"""
        
        # 1. Compute Derivatives
        phi_fft = fft2(self.phi)
        laplacian_fft = -self.K_squared * phi_fft
        laplacian = ifft2(laplacian_fft)
        
        # 2. Get Effective Speed and Nonlinear Terms
        c_eff_squared = self.effective_speed_squared()
        nonlinear_term = self.a * self.phi - self.b * np.abs(self.phi)**2 * self.phi
        damping_term = -self.damping * self.phi
        
        # 3. Time Evolution (Implicit in the formula, based on live.py)
        phi_new = (self.phi + 
                  self.dt * c_eff_squared * laplacian + 
                  self.dt * nonlinear_term +
                  self.dt * damping_term)
        
        # Update field and step count
        self.phi_prev = self.phi.copy()
        self.phi = phi_new
        self.time += self.dt
        self.step_count += 1
        
        # Simple re-normalization to prevent full collapse/blow-up
        self.phi *= 0.999 # Slight decay helps stability

    def measure_energy(self):
        """Measure total field energy (Approximation)"""
        return np.sum(np.abs(self.phi)**2)
    
    def measure_spacetime_curvature(self):
        """Measure the variation in processing speed (Spacetime Curvature)"""
        c_eff = np.sqrt(self.effective_speed_squared())
        mean_c = np.mean(c_eff)
        if mean_c < 1e-9: return 0.0
        return np.std(c_eff) / mean_c # Curvature is fractional change


class EmergentRealityNode(BaseNode):
    NODE_CATEGORY = "Source"
    NODE_COLOR = QtGui.QColor(255, 150, 50) # Orange for Emergent Physics
    
    def __init__(self, resolution=64, alpha_resistence=2.0, steps_per_frame=5):
        super().__init__()
        self.node_title = "Emergent Reality"
        
        self.inputs = {
            'alpha_control': 'signal', # Controls the key Alpha parameter
            'reset': 'signal'
        }
        self.outputs = {
            'intensity': 'image',        # Matter/Energy Density |Î¦|Â²
            'speed_of_light': 'image',   # Processing Speed c_eff
            'total_energy': 'signal',    # Global Energy Metric
            'curvature': 'signal',       # Global Curvature Metric
        }
        
        if not SCIPY_AVAILABLE:
            self.node_title = "Reality (No SciPy!)"
            return
            
        self.resolution = int(resolution)
        self.current_alpha = float(alpha_resistence)
        self.steps_per_frame = int(steps_per_frame)
        
        # Initialize simulation
        self.sim = RealitySimulator(size=self.resolution, dt=0.005, c0=1.0)
        self.sim.current_alpha = self.current_alpha
        
        # Outputs
        self.intensity_data = np.zeros((self.resolution, self.resolution), dtype=np.float32)
        self.speed_data = self.intensity_data.copy()
        self.energy_value = 0.0
        self.curvature_value = 0.0

    def randomize(self):
        """Called by 'R' button - reset/reseed the universe"""
        if SCIPY_AVAILABLE:
            self.sim.create_initial_state()

    def step(self):
        if not SCIPY_AVAILABLE:
            return
            
        # 1. Update control parameter
        alpha_in = self.get_blended_input('alpha_control', 'sum')
        if alpha_in is not None:
            # Map signal [-1, 1] to alpha resistance [0.01, 5.0]
            self.current_alpha = np.clip((alpha_in + 1.0) / 2.0 * 5.0, 0.01, 5.0)
            self.sim.current_alpha = self.current_alpha
            
        # 2. Check for reset
        reset_sig = self.get_blended_input('reset', 'sum')
        if reset_sig is not None and reset_sig > 0.5:
            self.randomize()

        # 3. Run simulation steps
        for _ in range(self.steps_per_frame):
            self.sim.wave_equation_step()
            
        # 4. Generate outputs
        self.energy_value = self.sim.measure_energy()
        self.curvature_value = self.sim.measure_spacetime_curvature()
        
        intensity_raw = np.abs(self.sim.phi)**2
        speed_raw = np.sqrt(self.sim.effective_speed_squared())
        
        # Normalize intensity for image output [0, 1]
        max_i = np.max(intensity_raw)
        self.intensity_data = intensity_raw / (max_i + 1e-9)
        
        # Normalize speed (c_eff) for image output [0, 1]
        min_c, max_c = np.min(speed_raw), np.max(speed_raw)
        range_c = max_c - min_c
        self.speed_data = (speed_raw - min_c) / (range_c + 1e-9)
        

    def get_output(self, port_name):
        if port_name == 'intensity':
            return self.intensity_data
        elif port_name == 'speed_of_light':
            return self.speed_data
        elif port_name == 'total_energy':
            # Scale energy to a manageable signal range (e.g., 0-10)
            return np.clip(self.energy_value / 5000.0, 0.0, 10.0) 
        elif port_name == 'curvature':
            # Curvature is already fractional (0-1)
            return np.clip(self.curvature_value * 10.0, 0.0, 1.0) # Scale up to 0-1
        return None
        
    def get_display_image(self):
        # Visualize Intensity data (Matter Density)
        img_u8 = (np.clip(self.intensity_data, 0, 1) * 255).astype(np.uint8)
        
        # Apply colormap (Hot for intensity)
        img_color = cv2.applyColorMap(img_u8, cv2.COLORMAP_HOT)
        
        # Add Curvature bar at bottom
        bar_h = 5
        curvature_color = int(np.clip(self.curvature_value * 255 * 10, 0, 255))
        img_color[-bar_h:, :] = [curvature_color, curvature_color, 0] # Yellowish bar
        
        # Resize to thumbnail size
        img_resized = cv2.resize(img_color, (96, 96), interpolation=cv2.INTER_LINEAR)
        img_resized = np.ascontiguousarray(img_resized)
        
        h, w = img_resized.shape[:2]
        return QtGui.QImage(img_resized.data, w, h, 3*w, QtGui.QImage.Format.Format_BGR888)

    def get_config_options(self):
        return [
            ("Resolution (NxN)", "resolution", self.resolution, None),
            ("Initial Alpha (Î±)", "alpha_resistence", self.current_alpha, None),
            ("Steps per Frame", "steps_per_frame", self.steps_per_frame, None),
        ]

=== FILE: fft_cochlea.py ===

"""
FFT Cochlea Node - Performs frequency analysis on signals and images
Place this file in the 'nodes' folder
"""

import numpy as np
import math
import cv2
from scipy.fft import rfft
from PyQt6 import QtGui

import sys
import os
# --- This is the new, correct block ---
import __main__
BaseNode = __main__.BaseNode
PA_INSTANCE = getattr(__main__, "PA_INSTANCE", None)
# ------------------------------------

class FFTCochleaNode(BaseNode):
    NODE_CATEGORY = "Transform"
    NODE_COLOR = QtGui.QColor(220, 120, 40)
    
    def __init__(self, freq_bins=64):
        super().__init__()
        self.node_title = "FFT Cochlea"
        self.inputs = {'image': 'image', 'signal': 'signal'}
        self.outputs = {
            'spectrum': 'spectrum', 
            'signal': 'signal', 
            'image': 'image', 
            'complex_spectrum': 'complex_spectrum'
        }
        
        self.freq_bins = freq_bins
        self.buffer = np.zeros(128, dtype=np.float32)
        self.x = 0.0
        self.internal_freq = np.random.uniform(2.0, 15.0)
        self.cochlea_img = np.zeros((64, 64), dtype=np.uint8) 
        self.spectrum_data = None
        self.complex_spectrum_data = None
        
    def step(self):
        u = self.get_blended_input('signal', 'sum') or 0.0
        
        alpha = 0.45
        decay = 0.92
        gain = 0.9
        
        newx = decay * self.x + gain * math.tanh(u + alpha * self.x)
        self.x = newx
        
        self.buffer *= 0.998
        if abs(self.x) > 0.09:
            amp = np.tanh(self.x) * 0.25
            t = np.linspace(0, 1, 10)
            sig = amp * np.sin(2*np.pi*(self.internal_freq + amp*10) * t)
            self.buffer[:-len(sig)] = self.buffer[len(sig):]
            self.buffer[-len(sig):] = sig
            
        img = self.get_blended_input('image', 'mean')
        if img is not None:
            self.compute_image_spectrum(img)
        else:
            self.compute_buffer_spectrum()
            
    def compute_buffer_spectrum(self):
        f = np.fft.fft(self.buffer)
        fsh = np.fft.fftshift(f)
        mag = np.abs(fsh)
        center = len(mag)//2
        half = min(self.freq_bins//2, center-1)
        spec = mag[center-half:center+half]
        self.spectrum_data = spec
        self.complex_spectrum_data = None
        self.update_display_from_spectrum(spec)
        
    def compute_image_spectrum(self, img):
        if img.ndim != 2:
            return
        
        spec = rfft(img.astype(np.float64), axis=1)
        self.complex_spectrum_data = spec.copy()
        mag = np.abs(spec)
        
        if mag.shape[1] > self.freq_bins:
            indices = np.linspace(0, mag.shape[1]-1, self.freq_bins).astype(int)
            mag = mag[:, indices]
        
        self.spectrum_data = np.mean(mag, axis=0)
        
        display = np.log1p(mag)
        display = (display - display.min()) / (display.max() - display.min() + 1e-9)
        
        h_target, w_target = self.cochlea_img.shape
        display_u8 = (display * 255).astype(np.uint8)
        self.cochlea_img = cv2.resize(display_u8, (w_target, h_target), interpolation=cv2.INTER_LINEAR)
        
    def update_display_from_spectrum(self, spec):
        arr = np.log1p(spec)
        arr = (arr - arr.min()) / (arr.max() - arr.min() + 1e-9)
        
        w, h = self.cochlea_img.shape
        self.cochlea_img = np.zeros((h, w), dtype=np.uint8)
        
        for i in range(min(len(arr), w)):
            v = int(255 * arr[i])
            self.cochlea_img[h - v:, i] = 255
        self.cochlea_img = np.flipud(self.cochlea_img)
        
    def get_output(self, port_name):
        if port_name == 'spectrum':
            return self.spectrum_data
        elif port_name == 'signal':
            return self.x
        elif port_name == 'image':
            return self.cochlea_img.astype(np.float32) / 255.0
        elif port_name == 'complex_spectrum':
            return self.complex_spectrum_data
        return None
        
    def get_display_image(self):
        img = np.ascontiguousarray(self.cochlea_img)
        h, w = img.shape
        return QtGui.QImage(img.data, w, h, w, QtGui.QImage.Format.Format_Grayscale8)
        
    def randomize(self):
        self.internal_freq = np.random.uniform(2.0, 15.0)
        self.x = np.random.uniform(-0.5, 0.5)

=== FILE: field_generator.py ===

"""
Neural Field Node - Generates a 2D field from frequency-band signals
Place this file in the 'nodes' folder
"""

import numpy as np
from PyQt6 import QtGui
import cv2

import sys
import os
# --- This is the new, correct block ---
import __main__
BaseNode = __main__.BaseNode
PA_INSTANCE = getattr(__main__, "PA_INSTANCE", None)
# ------------------------------------

class NeuralFieldNode(BaseNode):
    NODE_CATEGORY = "Transform"
    NODE_COLOR = QtGui.QColor(200, 80, 200) # A generative purple
    
    def __init__(self, width=128, height=96):
        super().__init__()
        self.node_title = "Neural Field"
        self.inputs = {
            'theta': 'signal',  # 4-8 Hz (coarse structure)
            'alpha': 'signal',  # 8-13 Hz (intermediate)
            'beta': 'signal',   # 13-30 Hz (fine structure)
            'gamma': 'signal'   # 30-100 Hz (finest details)
        }
        self.outputs = {'image': 'image', 'signal': 'signal'}
        
        self.w, self.h = width, height
        
        # Pre-generate noise patterns at different scales
        self.noise_layers = [
            (cv2.resize(np.random.rand(self.h // 8, self.w // 8).astype(np.float32), (self.w, self.h), interpolation=cv2.INTER_CUBIC)),
            (cv2.resize(np.random.rand(self.h // 4, self.w // 4).astype(np.float32), (self.w, self.h), interpolation=cv2.INTER_CUBIC)),
            (cv2.resize(np.random.rand(self.h // 2, self.w // 2).astype(np.float32), (self.w, self.h), interpolation=cv2.INTER_LINEAR)),
            (np.random.rand(self.h, self.w).astype(np.float32))
        ]
        
        # Normalize noise layers
        self.noise_layers = [(layer - layer.min()) / (layer.max() - layer.min() + 1e-9) for layer in self.noise_layers]
        
        self.field = np.zeros((self.h, self.w), dtype=np.float32)

    def step(self):
        # Get blended power from each band (normalize from [-1, 1] to [0, 1])
        theta_power = (self.get_blended_input('theta', 'sum') or 0.0 + 1.0) / 2.0
        alpha_power = (self.get_blended_input('alpha', 'sum') or 0.0 + 1.0) / 2.0
        beta_power  = (self.get_blended_input('beta', 'sum') or 0.0 + 1.0) / 2.0
        gamma_power = (self.get_blended_input('gamma', 'sum') or 0.0 + 1.0) / 2.0
        
        powers = [theta_power, alpha_power, beta_power, gamma_power]
        total_power = sum(powers) + 1e-9
        
        # Combine noise layers based on weighted average of powers
        self.field.fill(0.0)
        for i, layer in enumerate(self.noise_layers):
            self.field += layer * (powers[i] / total_power)
            
        # Add a slow "scrolling" effect to the noise
        self.noise_layers = [np.roll(layer, (1, 1), axis=(0, 1)) for layer in self.noise_layers]
        
        # Final normalization
        self.field = (self.field - self.field.min()) / (self.field.max() - self.field.min() + 1e-9)
        
    def get_output(self, port_name):
        if port_name == 'image':
            return self.field
        elif port_name == 'signal':
            return np.mean(self.field) * 2.0 - 1.0 # Remap to [-1, 1]
        return None
        
    def get_display_image(self):
        img_u8 = (np.clip(self.field, 0, 1) * 255).astype(np.uint8)
        img_u8 = np.ascontiguousarray(img_u8)
        return QtGui.QImage(img_u8.data, self.w, self.h, self.w, QtGui.QImage.Format.Format_Grayscale8)

    def get_config_options(self):
        return [
            ("Width", "w", self.w, None),
            ("Height", "h", self.h, None),
        ]

=== FILE: fractal_explorer.py ===

"""
Fractal Explorer Nodes - Real-time Mandelbrot and Julia set generators
Requires: pip install numba
Place this file in the 'nodes' folder
"""

import numpy as np
from PyQt6 import QtGui
import cv2

import sys
import os
# --- This is the new, correct block ---
import __main__
BaseNode = __main__.BaseNode
PA_INSTANCE = getattr(__main__, "PA_INSTANCE", None)
# ------------------------------------

try:
    from numba import jit
    NUMBA_AVAILABLE = True
except ImportError:
    NUMBA_AVAILABLE = False
    print("Warning: FractalExplorer nodes require 'numba'.")
    print("Please run: pip install numba")

# ======================================================================
# HIGH-SPEED JIT-COMPILED FRACTAL FUNCTIONS
# ======================================================================

@jit(nopython=True, fastmath=True)
def compute_mandelbrot(width, height, center_x, center_y, zoom, max_iter):
    """
    Fast Numba-compiled Mandelbrot set calculator.
    """
    result = np.zeros((height, width), dtype=np.int32)
    
    # Calculate scale
    scale = 2.0 / (width * zoom)
    
    for y in range(height):
        for x in range(width):
            # Map pixel to complex plane
            c_real = center_x + (x - width / 2) * scale
            c_imag = center_y + (y - height / 2) * scale
            
            z_real = 0.0
            z_imag = 0.0
            
            n = 0
            while n < max_iter:
                if z_real * z_real + z_imag * z_imag > 4.0:
                    break
                
                # z = z*z + c
                new_z_real = z_real * z_real - z_imag * z_imag + c_real
                z_imag = 2.0 * z_real * z_imag + c_imag
                z_real = new_z_real
                
                n += 1
                
            result[y, x] = n
            
    return result

@jit(nopython=True, fastmath=True)
def compute_julia(width, height, c_real, c_imag, max_iter):
    """
    Fast Numba-compiled Julia set calculator.
    """
    result = np.zeros((height, width), dtype=np.int32)
    
    for y in range(height):
        for x in range(width):
            # Map pixel to z in complex plane
            z_real = (x - width / 2) * 2.0 / width
            z_imag = (y - height / 2) * 2.0 / height
            
            n = 0
            while n < max_iter:
                if z_real * z_real + z_imag * z_imag > 4.0:
                    break
                
                # z = z*z + c
                new_z_real = z_real * z_real - z_imag * z_imag + c_real
                z_imag = 2.0 * z_real * z_imag + c_imag
                z_real = new_z_real
                
                n += 1
                
            result[y, x] = n
            
    return result

# ======================================================================
# MANDELBROT NODE
# ======================================================================

class MandelbrotNode(BaseNode):
    NODE_CATEGORY = "Source"
    NODE_COLOR = QtGui.QColor(50, 80, 160) # Deep blue
    
    def __init__(self, resolution=128, max_iterations=30):
        super().__init__()
        self.node_title = "Mandelbrot Explorer"
        
        self.inputs = {'zoom': 'signal', 'x_pos': 'signal', 'y_pos': 'signal'}
        self.outputs = {'image': 'image'}
        
        self.resolution = int(resolution)
        self.max_iterations = int(max_iterations)
        
        # Internal navigation state
        self.center_x = -0.7
        self.center_y = 0.0
        self.zoom = 0.5
        
        self.fractal_data = np.zeros((self.resolution, self.resolution), dtype=np.int32)
        
        if not NUMBA_AVAILABLE:
            self.node_title = "Mandelbrot (No Numba!)"

    def step(self):
        if not NUMBA_AVAILABLE:
            return
            
        # Get signals
        zoom_in = self.get_blended_input('zoom', 'sum') or 0.0
        move_x = self.get_blended_input('x_pos', 'sum') or 0.0
        move_y = self.get_blended_input('y_pos', 'sum') or 0.0
        
        # Update navigation state
        # A positive zoom signal (0 to 1) increases zoom
        self.zoom *= (1.0 + (zoom_in * 0.1))
        # Move signals ( -1 to 1) pan the view
        self.center_x += (move_x * 0.1) / self.zoom
        self.center_y += (move_y * 0.1) / self.zoom
        
        # Compute the fractal
        self.fractal_data = compute_mandelbrot(
            self.resolution, self.resolution,
            self.center_x, self.center_y,
            self.zoom, self.max_iterations
        )

    def get_output(self, port_name):
        if port_name == 'image':
            # Normalize iteration data to [0, 1]
            if self.max_iterations > 0:
                return self.fractal_data.astype(np.float32) / self.max_iterations
        return None
        
    def get_display_image(self):
        # Normalize and apply a color map
        img_norm = self.fractal_data.astype(np.float32) / self.max_iterations
        img_u8 = (img_norm * 255).astype(np.uint8)
        
        img_color = cv2.applyColorMap(img_u8, cv2.COLORMAP_JET)
        
        img_color = np.ascontiguousarray(img_color)
        h, w = img_color.shape[:2]
        return QtGui.QImage(img_color.data, w, h, 3*w, QtGui.QImage.Format.Format_BGR888)

    def get_config_options(self):
        return [
            ("Resolution", "resolution", self.resolution, None),
            ("Max Iterations", "max_iterations", self.max_iterations, None),
        ]

# ======================================================================
# JULIA NODE
# ======================================================================

class JuliaNode(BaseNode):
    NODE_CATEGORY = "Transform"
    NODE_COLOR = QtGui.QColor(180, 80, 180) # Generative Purple
    
    def __init__(self, resolution=128, max_iterations=40):
        super().__init__()
        self.node_title = "Julia Set Explorer"
        
        self.inputs = {'c_real': 'signal', 'c_imag': 'signal'}
        self.outputs = {'image': 'image'}
        
        self.resolution = int(resolution)
        self.max_iterations = int(max_iterations)
        
        # Internal state
        self.c_real = -0.7
        self.c_imag = 0.27015
        
        self.fractal_data = np.zeros((self.resolution, self.resolution), dtype=np.int32)
        
        if not NUMBA_AVAILABLE:
            self.node_title = "Julia (No Numba!)"

    def step(self):
        if not NUMBA_AVAILABLE:
            return
            
        # Get signals
        # Map input signals [-1, 1] to a good range for c, e.g., [-1, 1]
        self.c_real = self.get_blended_input('c_real', 'sum') or self.c_real
        self.c_imag = self.get_blended_input('c_imag', 'sum') or self.c_imag
        
        # Compute the fractal
        self.fractal_data = compute_julia(
            self.resolution, self.resolution,
            self.c_real, self.c_imag,
            self.max_iterations
        )

    def get_output(self, port_name):
        if port_name == 'image':
            # Normalize iteration data to [0, 1]
            if self.max_iterations > 0:
                return self.fractal_data.astype(np.float32) / self.max_iterations
        return None
        
    def get_display_image(self):
        # Normalize and apply a color map
        img_norm = self.fractal_data.astype(np.float32) / self.max_iterations
        img_u8 = (img_norm * 255).astype(np.uint8)
        
        img_color = cv2.applyColorMap(img_u8, cv2.COLORMAP_JET)
        
        img_color = np.ascontiguousarray(img_color)
        h, w = img_color.shape[:2]
        return QtGui.QImage(img_color.data, w, h, 3*w, QtGui.QImage.Format.Format_BGR888)

    def get_config_options(self):
        return [
            ("Resolution", "resolution", self.resolution, None),
            ("Max Iterations", "max_iterations", self.max_iterations, None),
        ]

=== FILE: fractal_surfer_node.py ===

"""
Fractal Surfer Node - Simulates a consciousness "surfer" on a quantum field.
Logic ported from the user-provided fractal_surfer.html file.
Place this file in the 'nodes' folder
"""

import numpy as np
from PyQt6 import QtGui
import cv2

import sys
import os
# --- This is the new, correct block ---
import __main__
BaseNode = __main__.BaseNode
PA_INSTANCE = getattr(__main__, "PA_INSTANCE", None)
# ------------------------------------

# --- Internal classes based on fractal_surfer.html ---

class QuantumField:
    """Numpy implementation of the QuantumField class."""
    def __init__(self, size):
        self.size = size
        self.mu = np.zeros((size, size), dtype=np.float32)
        self.sigma = np.zeros((size, size), dtype=np.float32)
        self.collapsed = np.zeros((size, size), dtype=np.float32)
        self.reset()

    def reset(self):
        self.mu = (np.random.rand(self.size, self.size) - 0.5) * 0.2
        self.sigma = 0.8 + np.random.rand(self.size, self.size) * 0.4
        self.collapsed.fill(0.0)

    def _laplacian(self, field):
        """Compute the laplacian using np.roll for periodic boundaries."""
        return (np.roll(field, 1, axis=0) + np.roll(field, -1, axis=0) +
                np.roll(field, 1, axis=1) + np.roll(field, -1, axis=1) - 4 * field)

    def evolve(self, rate):
        """Evolve the mu and sigma fields."""
        mu_lap = self._laplacian(self.mu)
        sigma_lap = self._laplacian(self.sigma)
        
        self.mu = self.mu + rate * mu_lap * 0.1
        self.mu *= 0.995 # Damping
        
        self.sigma = self.sigma + rate * sigma_lap * 0.02
        self.sigma *= 1.0002 # Entropy increase
        self.sigma = np.clip(self.sigma, 0.1, 2.0)
    
    def injectChaos(self):
        self.mu += (np.random.rand(self.size, self.size) - 0.5) * 0.5
        self.sigma += np.random.rand(self.size, self.size) * 0.3
        self.sigma = np.clip(self.sigma, 0.1, 2.0)

class FractalSurfer:
    """Numpy implementation of the FractalSurfer class."""
    def __init__(self, quantumField, search_radius):
        self.field = quantumField
        self.size = quantumField.size
        self.x = self.size / 2.0
        self.y = self.size / 2.0
        self.memory = 0.0
        self.sensation = 0.0
        self.collapseCount = 0
        self.search_radius = int(search_radius)

    def _gaussian_random(self, mu, sigma):
        """Box-Muller transform for Gaussian random numbers."""
        u, v = np.random.rand(2)
        z0 = np.sqrt(-2.0 * np.log(u)) * np.cos(2.0 * np.pi * v)
        return z0 * sigma + mu

    def update(self, exploration, plasticity, feedback):
        x, y = int(self.x), int(self.y)
        
        # 1. Wave function collapse
        local_mu = self.field.mu[y, x]
        local_sigma = self.field.sigma[y, x]
        self.sensation = self._gaussian_random(local_mu, local_sigma)
        
        self.field.collapsed[y, x] = self.sensation
        self.collapseCount += 1
        
        # 2. Learning from experience
        learning_signal = np.abs(self.sensation)
        if learning_signal > 0.3:
            self.memory = (1 - plasticity) * self.memory + plasticity * learning_signal
        self.memory *= 0.999 # Memory decay
        
        # 3. Consciousness feedback (reduce uncertainty)
        uncertainty_reduction = self.memory * feedback
        self.field.sigma[y, x] = np.maximum(0.1, self.field.sigma[y, x] - uncertainty_reduction)
        
        # 4. Navigate
        self.navigate(exploration)

    def navigate(self, exploration_bias):
        """Find the best nearby location and move towards it."""
        cx, cy = int(self.x), int(self.y)
        r = self.search_radius
        
        # Create coordinates for the search area
        x_coords = np.arange(cx - r, cx + r + 1) % self.size
        y_coords = np.arange(cy - r, cy + r + 1) % self.size
        xx, yy = np.meshgrid(x_coords, y_coords)
        
        # Get field values in the search area
        potential = self.field.mu[yy, xx]
        uncertainty = self.field.sigma[yy, xx]
        
        # Calculate distance penalty
        dx = (xx - cx + self.size/2) % self.size - self.size/2
        dy = (yy - cy + self.size/2) % self.size - self.size/2
        distance = np.sqrt(dx**2 + dy**2)
        
        # Score = weighted combo of potential, uncertainty, and distance
        score = ( (1 - exploration_bias) * potential + 
                  exploration_bias * uncertainty -
                  distance * 0.01 )
        
        # Find the best location
        best_idx = np.unravel_index(np.argmax(score), score.shape)
        bestX, bestY = x_coords[best_idx[1]], y_coords[best_idx[0]]
        
        # Move towards best location
        smoothing = 0.15
        self.x = (1 - smoothing) * self.x + smoothing * bestX
        self.y = (1 - smoothing) * self.y + smoothing * bestY
        
    def getCoherence(self):
        avg_uncertainty = np.mean(self.field.sigma)
        return np.maximum(0, 1 - avg_uncertainty / 2.0)

# --- The Node ---

class FractalSurferNode(BaseNode):
    NODE_CATEGORY = "Transform"
    NODE_COLOR = QtGui.QColor(60, 180, 160) # A generative teal
    
    def __init__(self, grid_size=64, search_radius=8):
        super().__init__()
        self.node_title = "Fractal Surfer"
        
        self.inputs = {
            'energy_in': 'signal',
            'exploration_in': 'signal',
            'plasticity_in': 'signal'
        }
        self.outputs = {
            'quantum_sea': 'image',
            'reality': 'image',
            'coherence': 'signal',
            'surfer_x': 'signal',
            'surfer_y': 'signal'
        }
        
        self.size = int(grid_size)
        self.search_radius = int(search_radius)
        
        # Initialize simulation state
        self.field = QuantumField(self.size)
        self.surfer = FractalSurfer(self.field, self.search_radius)
        
        self.feedback_strength = 0.1 # From original script
        
        self.display_img = np.zeros((self.size, self.size, 3), dtype=np.uint8)

    def step(self):
        # Get control signals
        evolution_rate = self.get_blended_input('energy_in', 'sum') or 0.0
        exploration = (self.get_blended_input('exploration_in', 'sum') or 0.0 + 1.0) / 2.0 # Map [-1,1] to [0,1]
        plasticity = (self.get_blended_input('plasticity_in', 'sum') or 0.0 + 1.0) / 2.0 # Map [-1,1] to [0,1]
        
        # Clamp plasticity to valid range
        plasticity = np.clip(plasticity * 0.1, 0.001, 0.1) 
        
        # Only evolve if energy is positive
        if evolution_rate > 0.0:
            self.field.evolve(evolution_rate)
        
        self.surfer.update(exploration, plasticity, self.feedback_strength)
        
        # Update the display image
        self._render_quantum_field()

    def _render_quantum_field(self):
        """Internal render function for quantum sea."""
        # Map mu (potential) to red
        potential = np.clip((self.field.mu + 1.0) / 2.0, 0, 1)
        # Map sigma (uncertainty) to green
        uncertainty = np.clip(self.field.sigma / 2.0, 0, 1)
        # Blue channel
        blue = np.clip((1 - uncertainty) * 0.5 + potential * 0.5, 0, 1)
        
        self.display_img[:,:,0] = (potential * 255).astype(np.uint8) # Red
        self.display_img[:,:,1] = (uncertainty * 255).astype(np.uint8) # Green
        self.display_img[:,:,2] = (blue * 255).astype(np.uint8) # Blue
        
        # Draw the surfer
        sx, sy = int(self.surfer.x), int(self.surfer.y)
        cv2.circle(self.display_img, (sx, sy), 2, (255, 255, 255), -1)

    def get_output(self, port_name):
        if port_name == 'quantum_sea':
            return self.display_img.astype(np.float32) / 255.0
        elif port_name == 'reality':
            return self.field.collapsed # Already [0,1]
        elif port_name == 'coherence':
            return self.surfer.getCoherence()
        elif port_name == 'surfer_x':
            return (self.surfer.x / self.size) * 2.0 - 1.0 # Map to [-1, 1]
        elif port_name == 'surfer_y':
            return (self.surfer.y / self.size) * 2.0 - 1.0 # Map to [-1, 1]
        return None
        
    def get_display_image(self):
        rgb = np.ascontiguousarray(self.display_img)
        h, w = rgb.shape[:2]
        return QtGui.QImage(rgb.data, w, h, 3*w, QtGui.QImage.Format.Format_BGR888)

    def randomize(self):
        """Called by 'R' button, injects chaos"""
        self.field.injectChaos()

    def get_config_options(self):
        return [
            ("Grid Size", "size", self.size, None),
            ("Search Radius", "search_radius", self.search_radius, None),
        ]

=== FILE: fractalanalyzernode.py ===

"""
Robust Fractal Analyzer Node - Measures scale-invariant structure
Computes fractal beta (power spectrum slope) with robust fallbacks.
Works with natural images, physics simulations, and extreme patterns.

Place this file in the 'nodes' folder as 'fractal_analyzer_robust.py'
"""

import numpy as np
from PyQt6 import QtGui
import cv2

import __main__
BaseNode = __main__.BaseNode
PA_INSTANCE = getattr(__main__, "PA_INSTANCE", None)
QtGui = __main__.QtGui

try:
    from scipy.fft import fft2, fftshift, ifft2, rfftfreq
    from scipy.stats import linregress
    import pywt
    LIBS_AVAILABLE = True
except ImportError:
    LIBS_AVAILABLE = False
    print("Warning: FractalAnalyzerNode requires 'scipy' and 'PyWavelets'.")
    print("Please run: pip install scipy pywavelets")


class FractalAnalyzerNode(BaseNode):
    NODE_CATEGORY = "Analysis"
    NODE_COLOR = QtGui.QColor(220, 180, 40)  # Golden Analysis Color
    
    def __init__(self, size=96, fit_range_min=5, levels=5):
        super().__init__()
        self.node_title = "Fractal Analyzer (Robust)"
        
        self.inputs = {'image_in': 'image'}
        self.outputs = {
            'fractal_beta': 'signal',       # Primary: Power spectrum slope
            'complexity': 'signal',          # Fallback: Wavelet-based complexity
            'spectral_energy': 'signal',     # Total high-frequency energy
            'spectrum_image': 'image',       # Visualization of power spectrum
            'fractal_twin': 'image'          # Synthesized random-phase version
        }
        
        self.size = int(size)
        self.fit_range_min = int(fit_range_min)
        self.levels = int(levels)
        
        # Internal state
        self.fractal_beta = 0.0
        self.complexity_value = 0.0
        self.spectral_energy = 0.0
        self.last_power_spectrum = None
        self.synthesized_img = np.zeros((self.size, self.size), dtype=np.float32)
        self.measurement_method = "none"  # Track which method succeeded
        
        if not LIBS_AVAILABLE:
            self.node_title = "Fractal (Libs Missing!)"

    def _compute_radial_profile(self, power_2d):
        """
        Compute radially averaged power spectrum.
        Returns: (frequencies, radial_power)
        """
        h, w = power_2d.shape
        center_y, center_x = h // 2, w // 2
        
        # Create radius map
        y, x = np.ogrid[:h, :w]
        r = np.sqrt((x - center_x)**2 + (y - center_y)**2).astype(int)
        
        # Radial binning
        r_max = min(center_x, center_y)
        radial_profile = np.zeros(r_max)
        radial_counts = np.zeros(r_max)
        
        for radius in range(r_max):
            mask = (r == radius)
            if np.any(mask):
                radial_profile[radius] = np.mean(power_2d[mask])
                radial_counts[radius] = np.sum(mask)
        
        # Only return frequencies with sufficient samples
        valid = radial_counts > 0
        frequencies = np.arange(r_max)[valid]
        radial_power = radial_profile[valid]
        
        return frequencies, radial_power

    def _robust_fractal_beta(self, gray_img):
        """
        Primary method: Compute fractal beta from power spectrum slope.
        Returns: (beta, success_flag, method_name)
        """
        try:
            # 1. Compute 2D FFT
            F = fft2(gray_img)
            power_2d = np.abs(fftshift(F))**2
            
            # 2. Add epsilon to prevent log(0)
            power_2d += 1e-10
            
            # 3. Store for visualization
            self.last_power_spectrum = power_2d
            
            # 4. Compute radial average
            freqs, radial_power = self._compute_radial_profile(power_2d)
            
            # 5. Skip DC component and ensure we have enough points
            if len(freqs) < self.fit_range_min:
                return 0.0, False, "too_few_points"
            
            freqs = freqs[1:]  # Skip r=0 (DC)
            radial_power = radial_power[1:]
            
            # 6. Fit only in valid frequency range
            fit_start = max(1, self.fit_range_min)
            fit_end = len(freqs)
            
            if fit_end - fit_start < 3:
                return 0.0, False, "insufficient_range"
            
            log_freqs = np.log(freqs[fit_start:fit_end])
            log_power = np.log(radial_power[fit_start:fit_end])
            
            # 7. Check for valid values (no NaN, no Inf)
            valid_mask = np.isfinite(log_freqs) & np.isfinite(log_power)
            if np.sum(valid_mask) < 3:
                return 0.0, False, "invalid_values"
            
            log_freqs = log_freqs[valid_mask]
            log_power = log_power[valid_mask]
            
            # 8. Perform linear regression
            slope, intercept, r_value, p_value, std_err = linregress(log_freqs, log_power)
            
            # 9. Sanity check: beta should be negative and reasonable
            if not np.isfinite(slope):
                return 0.0, False, "infinite_slope"
            
            if slope > 0:  # Physically impossible for power spectrum
                return 0.0, False, "positive_slope"
            
            if slope < -10:  # Probably numerical error
                return -10.0, True, "clamped_low"
            
            # 10. Success!
            return slope, True, "fractal_beta"
            
        except Exception as e:
            return 0.0, False, f"exception_{type(e).__name__}"

    def _wavelet_complexity(self, gray_img):
        """
        Fallback method 1: Wavelet-based complexity measure.
        Returns: (complexity, success_flag, method_name)
        """
        try:
            # Compute DWT
            coeffs = pywt.wavedec2(gray_img, wavelet='db4', level=self.levels)
            
            # Compute energy at each level
            energies = []
            
            # Approximation (low-frequency)
            cA = coeffs[0]
            low_freq_energy = np.sum(cA**2)
            energies.append(low_freq_energy)
            
            # Details (high-frequency)
            high_freq_energy = 0.0
            for detail in coeffs[1:]:
                cH, cV, cD = detail
                level_energy = np.sum(cH**2) + np.sum(cV**2) + np.sum(cD**2)
                energies.append(level_energy)
                high_freq_energy += level_energy
            
            # Complexity = ratio of high-freq to low-freq energy
            total_energy = np.sum(energies)
            if total_energy < 1e-10:
                return 0.0, False, "zero_energy"
            
            complexity = high_freq_energy / total_energy
            
            # Convert to pseudo-beta (map [0,1] to [-3, -1])
            pseudo_beta = -3.0 + complexity * 2.0
            
            return pseudo_beta, True, "wavelet_fallback"
            
        except Exception as e:
            return 0.0, False, f"wavelet_exception_{type(e).__name__}"

    def _std_complexity(self, gray_img):
        """
        Fallback method 2: Simple standard deviation.
        Returns: (complexity, success_flag, method_name)
        """
        try:
            std = np.std(gray_img)
            
            # Convert to pseudo-beta (map std [0, 0.5] to [-3, -1])
            pseudo_beta = -3.0 + np.clip(std * 4.0, 0, 1) * 2.0
            
            return pseudo_beta, True, "std_fallback"
            
        except:
            return -2.0, True, "default_fallback"

    def _synthesize_random_phase(self, gray_img):
        """
        Create a 'fractal twin' with same amplitude spectrum but random phase.
        """
        try:
            F_orig = fft2(gray_img)
            F_mag = np.abs(F_orig)
            
            # Deterministic random phase
            np.random.seed(42)
            random_phase = np.exp(1j * 2 * np.pi * np.random.rand(*F_orig.shape))
            
            F_synth = F_mag * random_phase
            img_synth = np.real(ifft2(F_synth))
            
            # Normalize to [0, 1]
            img_synth -= img_synth.min()
            img_synth /= (img_synth.max() + 1e-9)
            
            return img_synth.astype(np.float32)
            
        except:
            return np.zeros_like(gray_img, dtype=np.float32)

    def _generate_spectrum_visualization(self):
        """
        Create a visual representation of the power spectrum.
        """
        if self.last_power_spectrum is None:
            return np.zeros((64, 64), dtype=np.float32)
        
        # Log scale for better visualization
        log_power = np.log(self.last_power_spectrum + 1e-10)
        
        # Normalize
        log_power -= log_power.min()
        log_power /= (log_power.max() + 1e-9)
        
        # Resize for display
        vis = cv2.resize(log_power, (64, 64), interpolation=cv2.INTER_LINEAR)
        
        return vis.astype(np.float32)

    def step(self):
        if not LIBS_AVAILABLE:
            return
        
        # Get input image (use 'first' to avoid blending issues)
        img_in = self.get_blended_input('image_in', 'first')
        
        if img_in is None:
            # Decay outputs when no input
            self.fractal_beta *= 0.95
            self.complexity_value *= 0.95
            self.spectral_energy *= 0.95
            return
        
        # Ensure grayscale
        if img_in.ndim == 3:
            if img_in.shape[2] == 4:  # RGBA
                img_in = cv2.cvtColor(img_in.astype(np.float32), cv2.COLOR_RGBA2GRAY)
            else:  # RGB/BGR
                img_in = cv2.cvtColor(img_in.astype(np.float32), cv2.COLOR_BGR2GRAY)
        
        # Resize to working resolution
        gray_img = cv2.resize(img_in, (self.size, self.size), interpolation=cv2.INTER_AREA)
        
        # Normalize to [0, 1]
        if gray_img.max() > 1.0:
            gray_img = gray_img / 255.0
        
        # === Cascade of measurement methods ===
        
        # Method 1: Try fractal beta (primary)
        beta, success, method = self._robust_fractal_beta(gray_img)
        
        if success:
            self.fractal_beta = beta
            self.measurement_method = method
        else:
            # Method 2: Try wavelet complexity (fallback 1)
            beta, success, method = self._wavelet_complexity(gray_img)
            
            if success:
                self.fractal_beta = beta
                self.measurement_method = method
            else:
                # Method 3: Use std dev (fallback 2)
                beta, success, method = self._std_complexity(gray_img)
                self.fractal_beta = beta
                self.measurement_method = method
        
        # Compute spectral energy (total high-frequency content)
        if self.last_power_spectrum is not None:
            center = self.size // 2
            high_freq_mask = np.zeros_like(self.last_power_spectrum)
            y, x = np.ogrid[:self.size, :self.size]
            r = np.sqrt((x - center)**2 + (y - center)**2)
            high_freq_mask[r > center // 2] = 1.0
            self.spectral_energy = np.sum(self.last_power_spectrum * high_freq_mask)
            self.spectral_energy = np.log10(self.spectral_energy + 1.0) / 10.0  # Normalize
        
        # Compute wavelet-based complexity (always, for secondary output)
        _, wavelet_success, _ = self._wavelet_complexity(gray_img)
        if wavelet_success:
            # Store as 0-1 normalized complexity
            self.complexity_value = (self.fractal_beta + 3.0) / 2.0  # Map [-3,-1] to [0,1]
        
        # Synthesize fractal twin
        self.synthesized_img = self._synthesize_random_phase(gray_img)

    def get_output(self, port_name):
        if port_name == 'fractal_beta':
            return self.fractal_beta
        
        elif port_name == 'complexity':
            return self.complexity_value
        
        elif port_name == 'spectral_energy':
            return self.spectral_energy
        
        elif port_name == 'spectrum_image':
            return self._generate_spectrum_visualization()
        
        elif port_name == 'fractal_twin':
            return self.synthesized_img
        
        return None
    
    def get_display_image(self):
        if not LIBS_AVAILABLE:
            return None
        
        # Show the synthesized fractal twin
        img_u8 = (np.clip(self.synthesized_img, 0, 1) * 255).astype(np.uint8)
        
        # Overlay the fractal beta value and method
        h, w = img_u8.shape
        font = cv2.FONT_HERSHEY_SIMPLEX
        
        # Format beta with sign
        beta_text = f"Î²: {self.fractal_beta:.2f}"
        method_text = f"{self.measurement_method}"
        
        # Draw text with shadow for readability
        cv2.putText(img_u8, beta_text, (6, h - 16), font, 0.3, 0, 1, cv2.LINE_AA)
        cv2.putText(img_u8, beta_text, (5, h - 17), font, 0.3, 255, 1, cv2.LINE_AA)
        
        cv2.putText(img_u8, method_text, (6, h - 4), font, 0.25, 0, 1, cv2.LINE_AA)
        cv2.putText(img_u8, method_text, (5, h - 5), font, 0.25, 200, 1, cv2.LINE_AA)
        
        img_u8 = np.ascontiguousarray(img_u8)
        return QtGui.QImage(img_u8.data, w, h, w, QtGui.QImage.Format.Format_Grayscale8)

    def get_config_options(self):
        return [
            ("Resolution (NxN)", "size", self.size, None),
            ("Fit Range Min", "fit_range_min", self.fit_range_min, None),
            ("Wavelet Levels", "levels", self.levels, None),
        ]

=== FILE: fractalblend.py ===

"""
FractalBlendNode

Uses a Julia set calculation as a dynamic mask
to blend between two input images.
"""

import numpy as np
import cv2

# --- Magic import block ---
import __main__
BaseNode = __main__.BaseNode
QtGui = __main__.QtGui
# --------------------------

class FractalBlendNode(BaseNode):
    """
    Blends two images using a fractal (Julia set) mask.
    """
    NODE_CATEGORY = "Filter"
    NODE_COLOR = QtGui.QColor(100, 220, 180) # Teal

    def __init__(self, size=256):
        super().__init__()
        self.node_title = "Fractal Blender"
        
        self.inputs = {
            'image_in1': 'image', # Background image
            'image_in2': 'image', # Foreground image
            'c_real': 'signal',   # Julia set 'c' real part
            'c_imag': 'signal',   # Julia set 'c' imaginary part
            'max_iter': 'signal'  # Fractal detail (0-1)
        }
        self.outputs = {'image': 'image'}
        
        self.size = int(size)
        self.blended_image = np.zeros((self.size, self.size, 3), dtype=np.float32)
        
        # Pre-calculate the 'Z' grid
        y, x = np.mgrid[0:self.size, 0:self.size]
        self.z_real = (x / (self.size - 1) - 0.5) * 4.0
        self.z_imag = (y / (self.size - 1) - 0.5) * 4.0
        
    def _prepare_image(self, img):
        """Helper to resize and format an input image."""
        if img is None:
            return None
        
        img_resized = cv2.resize(img, (self.size, self.size), 
                                 interpolation=cv2.INTER_LINEAR)
        
        if img_resized.ndim == 2:
            img_resized = cv2.cvtColor(img_resized, cv2.COLOR_GRAY2RGB)
        elif img_resized.shape[2] == 4:
            img_resized = cv2.cvtColor(img_resized, cv2.COLOR_RGBA2RGB)
        
        if img_resized.max() > 1.0:
            img_resized = img_resized.astype(np.float32) / 255.0
            
        return np.clip(img_resized, 0, 1)

    def step(self):
        # --- 1. Get Control Signals ---
        c_real = self.get_blended_input('c_real', 'sum') or -0.7
        c_imag = self.get_blended_input('c_imag', 'sum') or 0.27
        
        # Max iterations: 10 to 80
        iter_in = self.get_blended_input('max_iter', 'sum') or 0.2
        max_iter = int(10 + iter_in * 70)
        
        # --- 2. Get and Prepare Input Images ---
        img1 = self._prepare_image(self.get_blended_input('image_in1', 'first'))
        img2 = self._prepare_image(self.get_blended_input('image_in2', 'first'))
        
        # Handle missing images
        if img1 is None and img2 is None:
            self.blended_image *= 0.9 # Fade to black
            return
        elif img1 is None:
            img1 = np.zeros((self.size, self.size, 3), dtype=np.float32)
        elif img2 is None:
            img2 = np.zeros((self.size, self.size, 3), dtype=np.float32)

        # --- 3. Perform Fractal Calculation (Julia Set) ---
        
        # Initialize Z and C grids
        Zr = self.z_real.copy()
        Zi = self.z_imag.copy()
        Cr = c_real
        Ci = c_imag
        
        # Output mask (stores escape time)
        fractal_mask = np.full(Zr.shape, max_iter, dtype=np.float32)
        
        # Create a boolean mask for pixels still iterating
        active = np.ones(Zr.shape, dtype=bool)

        for i in range(max_iter):
            if not active.any(): # Stop if all pixels escaped
                break
            
            # Check for escape
            mag_sq = Zr[active]**2 + Zi[active]**2
            escaped = mag_sq > 4.0
            
            # Store iteration count for newly escaped pixels
            fractal_mask[active][escaped] = i
            
            # Update active mask (remove escaped pixels)
            active[active] = ~escaped
            
            if not active.any():
                break

            # Z = Z^2 + C
            # Z.real = Z.real^2 - Z.imag^2 + C.real
            # Z.imag = 2 * Z.real * Z.imag + C.imag
            Zr_temp = Zr[active]**2 - Zi[active]**2 + Cr
            Zi[active] = 2 * Zr[active] * Zi[active] + Ci
            Zr[active] = Zr_temp

        # --- 4. Normalize mask and blend ---
        
        # Normalize the mask from 0 to 1
        mask_norm = (fractal_mask / (max_iter - 1.0))
        # Use sine for a smoother, pulsing blend
        mask_smooth = (np.sin(mask_norm * np.pi * 2.0 - np.pi/2.0) + 1.0) * 0.5
        
        # Expand mask to 3 channels (H, W, 1) for broadcasting
        mask_3d = mask_smooth[..., np.newaxis]
        
        # Blend: img1 is background, img2 is foreground
        self.blended_image = (img1 * (1.0 - mask_3d)) + (img2 * mask_3d)

    def get_output(self, port_name):
        if port_name == 'image':
            return self.blended_image
        return None

=== FILE: fractalquantumgatenode.py ===

"""
Fractal Quantum Gate Node - A SchrÃ¶dinger-like wave simulator with fractal potential
and animated quantum gate operations (Hadamard, NOT, Entanglement).
Ported from nphard2.py (SchrÃ¶dinger equation) and bmonsphere.py (Gates/Potential).
Requires: pip install numpy scipy
"""

import numpy as np
from PyQt6 import QtGui
import cv2
from scipy.ndimage import gaussian_filter
import sys
import os

import __main__
BaseNode = __main__.BaseNode
PA_INSTANCE = getattr(__main__, "PA_INSTANCE", None)
QtGui = __main__.QtGui

try:
    from scipy.ndimage import gaussian_filter
    SCIPY_AVAILABLE = True
except ImportError:
    SCIPY_AVAILABLE = False
    print("Warning: FractalQuantumGateNode requires 'scipy'.")


class FractalQuantumGateNode(BaseNode):
    NODE_CATEGORY = "Source"
    NODE_COLOR = QtGui.QColor(150, 100, 255)  # Purple/Violet for Quantum Gates
    
    def __init__(self, size=64, dt=0.05, potential_strength=1.5):
        super().__init__()
        self.node_title = "Fractal Quantum Gate"
        
        self.inputs = {
            'potential_strength': 'signal', # Control V_eff strength
            'damping': 'signal',          # Control wave decay
            'operation_trigger': 'signal' # Trigger a quantum operation
        }
        self.outputs = {
            'prob_density': 'image',      # |Ï|Â² (Probability)
            'phase_field': 'image',       # Phase (Angle)
            'current_operation': 'signal' # Shows if gate is active
        }
        
        if not SCIPY_AVAILABLE:
            self.node_title = "FOG (No SciPy!)"
            return
            
        self.size = int(size)
        self.dt = float(dt)
        self.time = 0
        
        # Physics parameters
        self.hbar_eff = 1.0
        self.mass_eff = 1.0
        self.potential_strength = float(potential_strength)
        self.damping = 0.005
        
        # State grids
        self.psi = np.zeros((self.size, self.size), dtype=np.complex64)
        self.potential = self._generate_fractal_potential()
        
        # Operation tracking
        self.operation = None # "hadamard", "x_gate", "entanglement"
        self.operation_step = 0
        self.total_steps = 30
        self.last_trigger_val = 0.0

        self._initialize_wave_packet()
    
    def _generate_fractal_potential(self):
        """Generate a static potential field (simplified version of source code)."""
        if not SCIPY_AVAILABLE:
            return np.zeros((self.size, self.size))

        potential = np.zeros((self.size, self.size))
        octaves = 4
        persistence = 0.5
        lacunarity = 2.0
        
        yy, xx = np.mgrid[:self.size, :self.size]
        
        for i in range(octaves):
            freq = lacunarity ** i
            amp = persistence ** i
            
            # Use simple sin/cos modulation on position for pseudo-fractal structure
            noise_x = np.sin(xx / self.size * freq * 2 * np.pi)
            noise_y = np.cos(yy / self.size * freq * 2 * np.pi)
            noise_val = noise_x * noise_y

            potential += amp * noise_val
        
        # Normalize and smooth
        potential = (potential - np.min(potential)) / (np.max(potential) - np.min(potential) + 1e-9)
        return gaussian_filter(potential, sigma=1.0)
    
    def _initialize_wave_packet(self):
        """Initialize a Gaussian wave packet."""
        center = (self.size // 4, self.size // 4)
        sigma = self.size * 0.06
        kx, ky = 1.5, 1.0 # Base momentum
        
        y0, x0 = center
        yy, xx = np.mgrid[:self.size, :self.size]
        
        envelope = np.exp(-((xx - x0)**2 + (yy - y0)**2) / (4 * sigma**2))
        phase = kx * (xx - x0) + ky * (yy - y0)
        self.psi = (envelope * np.exp(1j * phase)).astype(np.complex64)
        
        # Normalize
        norm = np.sqrt(np.sum(np.abs(self.psi)**2))
        if norm > 1e-9:
            self.psi /= norm

    def randomize(self):
        """Called by 'R' button - Re-initializes the wave packet and potential."""
        self.potential = self._generate_fractal_potential()
        self._initialize_wave_packet()
        self.operation = None
        self.operation_step = 0
        
    def _apply_gate(self, progress):
        """Simplified gate application (animation/interpolation)."""
        current_psi = self.psi.copy()
        
        if self.operation == "hadamard":
            # H: superposition, represented as splitting/reflection
            reflected_psi = np.roll(current_psi, self.size//2, axis=0) # Shift half way
            target_psi = (current_psi + reflected_psi)
            
        elif self.operation == "x_gate":
            # X: NOT gate, represented as vertical flip
            target_psi = np.flip(current_psi, axis=0)
            
        elif self.operation == "entanglement":
            # Entanglement: create correlation/diagonal structure
            correlated_psi = np.diag(np.ones(self.size)) + np.diag(np.ones(self.size-1), 1)
            correlated_psi = np.pad(correlated_psi, (0, self.size-correlated_psi.shape[0]), 'constant')[:self.size, :self.size] # Handle padding/truncation
            phase_pattern = np.exp(1j * np.pi * self.potential)
            target_psi = correlated_psi.astype(np.complex64) * phase_pattern
        else:
            return
            
        # Normalize target state
        norm_target = np.sqrt(np.sum(np.abs(target_psi)**2))
        if norm_target > 1e-9:
            target_psi /= norm_target
            
        # Interpolate
        self.psi = (1 - progress) * current_psi + progress * target_psi
        
        # Ensure final normalization
        norm = np.sqrt(np.sum(np.abs(self.psi)**2))
        if norm > 1e-9:
            self.psi /= norm
    
    def _update_dynamics(self):
        """Evolve the wave function using SchrÃ¶dinger-like dynamics."""
        # Calculate Laplacian (Periodic boundaries are implicit with roll)
        lap_psi = (np.roll(self.psi, 1, axis=0) + np.roll(self.psi, -1, axis=0) +
                   np.roll(self.psi, 1, axis=1) + np.roll(self.psi, -1, axis=1) - 4 * self.psi)
        
        # Potential term (only using the static fractal potential V)
        V_eff = self.potential_strength * self.potential
        
        # SchrÃ¶dinger-like evolution: i*dpsi/dt = H*psi -> dpsi = -i * H * dt
        H_psi = (-self.hbar_eff**2 / (2 * self.mass_eff) * lap_psi + V_eff * self.psi)
        
        # Euler update
        self.psi += (-1j / self.hbar_eff) * H_psi * self.dt
        
        # Apply damping
        self.psi *= (1 - self.damping * self.dt)
        
        # Re-normalize periodically
        norm = np.sqrt(np.sum(np.abs(self.psi)**2))
        if norm > 1e-9:
            self.psi /= norm

    def step(self):
        if not SCIPY_AVAILABLE:
            return
            
        # Get inputs
        pot_in = self.get_blended_input('potential_strength', 'sum')
        damp_in = self.get_blended_input('damping', 'sum')
        trigger_val = self.get_blended_input('operation_trigger', 'sum') or 0.0

        if pot_in is not None:
            self.potential_strength = np.clip(pot_in, 0.0, 5.0)
            
        if damp_in is not None:
            self.damping = np.clip(damp_in * 0.1, 0.001, 0.1) # Map to small range

        # --- Handle Gate Trigger ---
        if trigger_val > 0.5 and self.last_trigger_val <= 0.5:
            # Trigger detected (rising edge)
            if self.operation is None:
                # Cycle through gates
                gates = ["hadamard", "x_gate", "entanglement"]
                
                # Simple cycling logic based on current operation
                try:
                    current_idx = (gates.index(self.operation) + 1) if self.operation in gates else 0
                except ValueError:
                    current_idx = 0
                    
                self.operation = gates[current_idx]
                self.operation_step = 0
            
        self.last_trigger_val = trigger_val
        # --- End Gate Trigger ---

        if self.operation and self.operation_step < self.total_steps:
            # Operation in progress
            progress = self.operation_step / self.total_steps
            self._apply_gate(progress)
            self.operation_step += 1
            if self.operation_step >= self.total_steps:
                self.operation = None
        else:
            # Regular evolution
            self._update_dynamics()
        
        self.time += self.dt

    def get_output(self, port_name):
        if port_name == 'prob_density':
            # Output probability density: |Ï|Â²
            prob_density = np.abs(self.psi)**2
            max_val = np.max(prob_density)
            if max_val > 1e-9:
                return prob_density / max_val
            return prob_density
            
        elif port_name == 'phase_field':
            # Output normalized phase: [0, 1]
            phase = np.angle(self.psi)
            return (phase + np.pi) / (2 * np.pi)
            
        elif port_name == 'current_operation':
            # Output 1.0 if any gate is active
            return 1.0 if self.operation else 0.0
            
        return None
        
    def get_display_image(self):
        # Visualize probability density with phase color
        prob_density = np.abs(self.psi)**2
        phase = np.angle(self.psi)

        # Normalize amplitude and map phase to hue
        amp_norm = prob_density / (np.max(prob_density) + 1e-9)
        hue = ((np.angle(self.psi) + np.pi) / (2*np.pi) * 180).astype(np.uint8)
        sat = (amp_norm * 255).astype(np.uint8)
        val = (amp_norm * 255).astype(np.uint8)
        
        hsv = np.stack([hue, sat, val], axis=-1)
        rgb = cv2.cvtColor(hsv, cv2.COLOR_HSV2RGB)
        
        # Add operation indicator
        if self.operation:
            bar_color = (0, 0, 255) # Blue for Quantum
            if self.operation == 'hadamard': bar_color = (255, 165, 0) # Orange
            elif self.operation == 'x_gate': bar_color = (255, 0, 0) # Red
            elif self.operation == 'entanglement': bar_color = (0, 255, 0) # Green
            
            h, w = rgb.shape[:2]
            rgb[:3, :] = bar_color # Top status bar
            
        # Resize for display thumbnail (96x96)
        img_resized = cv2.resize(rgb, (96, 96), interpolation=cv2.INTER_LINEAR)
        img_resized = np.ascontiguousarray(img_resized)
        
        h, w = img_resized.shape[:2]
        return QtGui.QImage(img_resized.data, w, h, 3*w, QtGui.QImage.Format.Format_RGB888)

    def get_config_options(self):
        return [
            ("Resolution (NxN)", "size", self.size, None),
            ("Timestep (dt)", "dt", self.dt, None),
            ("Potential Strength", "potential_strength", self.potential_strength, None),
            ("Gate Duration (steps)", "total_steps", self.total_steps, None),
        ]

=== FILE: fractalropenode.py ===

"""
Fractal Rope Node - Implements Tim Palmer's geometric model of quantum reality.
Simulates a fractal helix bundle and strand selection during a measurement event.
Ported from palmers_rope.py.
Requires: pip install numpy
"""

import numpy as np
from PyQt6 import QtGui
import cv2
import sys
import os

import __main__
BaseNode = __main__.BaseNode
PA_INSTANCE = getattr(__main__, "PA_INSTANCE", None)
QtGui = __main__.QtGui


# --- Core Geometric Classes (from palmers_rope.py) ---

class FractalStrand:
    """A single strand in the fractal rope"""
    
    def __init__(self, base_trajectory, fractal_level=0, amplitude=1.0):
        self.base_trajectory = base_trajectory.astype(np.float32)
        self.fractal_level = fractal_level
        self.amplitude = amplitude
        self.sub_strands = []
        self.selected = False
        self.coherence = 1.0 # Stability metric

    # --- FIX: ADD MISSING METHOD ---
    def add_fractal_detail(self, n_sub_strands=3, detail_level=0.3):
        """Add fractal sub-structure to this strand (Helixes within helixes)"""
        if self.fractal_level < 2:  # Limit recursion depth for performance
            for i in range(n_sub_strands):
                # Create sub-trajectory wound around base trajectory
                sub_trajectory = self.create_sub_helix(i, n_sub_strands, detail_level)
                sub_strand = FractalStrand(sub_trajectory, 
                                         self.fractal_level + 1, 
                                         self.amplitude * detail_level)
                self.sub_strands.append(sub_strand)
                # Recursively add detail (Palmers' concept: Uncertainty = Geometric bundling)
                sub_strand.add_fractal_detail(n_sub_strands=2, detail_level=0.2) 
    # --- END FIX ---
    
    def create_sub_helix(self, index, total_strands, detail_level):
        """Create a helical sub-trajectory wound around the base"""
        t = np.linspace(0, 1, len(self.base_trajectory))
        
        phase = 2 * np.pi * index / total_strands
        frequency = 8 + 4 * self.fractal_level
        
        helix_x = detail_level * np.cos(frequency * 2 * np.pi * t + phase)
        helix_y = detail_level * np.sin(frequency * 2 * np.pi * t + phase)
        helix_z = detail_level * 0.5 * np.sin(frequency * 4 * np.pi * t + phase)
        
        sub_trajectory = self.base_trajectory.copy()
        sub_trajectory[:, 0] += helix_x
        sub_trajectory[:, 1] += helix_y
        sub_trajectory[:, 2] += helix_z
        
        return sub_trajectory.astype(np.float32)

class FractalRope:
    """The complete fractal rope structure"""
    
    def __init__(self, n_main_strands=6, length=40):
        self.n_main_strands = n_main_strands
        self.length = length
        self.main_strands = []
        self.selected_strand = None
        self.time = 0.0
        
        self.create_main_rope()
        
        # This loop now calls the fixed method
        for strand in self.main_strands:
            strand.add_fractal_detail()
    
    def create_main_rope(self):
        """Create the main helical rope structure"""
        t = np.linspace(0, 4*np.pi, self.length)
        
        centerline = np.array([
            t,
            2 * np.sin(t),
            2 * np.cos(t)
        ]).T
        
        for i in range(self.n_main_strands):
            phase = 2 * np.pi * i / self.n_main_strands
            radius = 1.5
            helix_freq = 3
            
            helix_x = radius * np.cos(helix_freq * t + phase)
            helix_y = radius * np.sin(helix_freq * t + phase)
            helix_z = 0.5 * np.sin(helix_freq * 2 * t + phase)
            
            main_trajectory = centerline.copy()
            main_trajectory[:, 0] += helix_x
            main_trajectory[:, 1] += helix_y
            main_trajectory[:, 2] += helix_z
            
            strand = FractalStrand(main_trajectory, fractal_level=0)
            self.main_strands.append(strand)
    
    def apply_measurement(self, selection_radius=2.0):
        """Apply measurement - select coherent strand cluster"""
        
        mp = np.array([
            np.random.uniform(5, 7), 
            np.random.uniform(-1, 1),
            np.random.uniform(-1, 1)
        ])
        
        selected_strands = []
        self.selected_strand = None

        for strand in self.main_strands:
            distances = np.linalg.norm(strand.base_trajectory - mp, axis=1)
            min_distance = np.min(distances)
            
            strand.selected = False
            
            if min_distance < selection_radius:
                strand.selected = True
                strand.coherence = 1.0 / (1.0 + min_distance)
                selected_strands.append(strand)
            else:
                strand.coherence = 0.05 # Decohered state
        
        if selected_strands:
            self.selected_strand = max(selected_strands, key=lambda s: s.coherence)
            
        return len(selected_strands) 

    def evolve(self, dt=0.1):
        """Evolve the rope structure"""
        self.time += dt
        
        for strand in self.main_strands:
            noise_amplitude = 0.01
            noise = np.random.normal(0, noise_amplitude, strand.base_trajectory.shape).astype(np.float32)
            strand.base_trajectory += noise
            
            if self.selected_strand is not strand:
                strand.coherence = max(0.01, strand.coherence * 0.95)

# --- The Main Node Class ---

class FractalRopeNode(BaseNode):
    NODE_CATEGORY = "Source"
    NODE_COLOR = QtGui.QColor(100, 100, 100) # Geometric Gray
    
    def __init__(self, n_strands=6, resolution=96, selection_radius=2.0):
        super().__init__()
        self.node_title = "Fractal Rope (Palmer)"
        
        self.inputs = {
            'measurement_trigger': 'signal'
        }
        self.outputs = {
            'measured_image': 'image',
            'coherence_out': 'signal',
            'uncertainty': 'signal' # Number of strands in the bundle
        }
        
        self.n_strands = int(n_strands)
        self.resolution = int(resolution)
        self.selection_radius = float(selection_radius)
        
        self.rope = FractalRope(n_main_strands=self.n_strands, length=40)
        self.last_trigger_val = 0.0
        self.last_uncertainty = float(self.n_strands)

    def step(self):
        # 1. Get inputs
        trigger_val = self.get_blended_input('measurement_trigger', 'sum') or 0.0
        
        # 2. Check for measurement trigger (rising edge)
        if trigger_val > 0.5 and self.last_trigger_val <= 0.5:
            num_selected = self.rope.apply_measurement(self.selection_radius)
            self.last_uncertainty = np.clip(num_selected / self.n_strands, 0.0, 1.0)
        else:
            self.rope.evolve()

        self.last_trigger_val = trigger_val

    def get_output(self, port_name):
        if port_name == 'coherence_out':
            if self.rope.selected_strand:
                return self.rope.selected_strand.coherence
            return 0.0
            
        elif port_name == 'uncertainty':
            return self.last_uncertainty
            
        elif port_name == 'measured_image':
            img = self._draw_cross_section()
            return img / 255.0 
            
        return None
        
    def _draw_cross_section(self):
        """Draws the cross-section visualization for the node's output port."""
        w, h = self.resolution, self.resolution
        img = np.zeros((h, w, 3), dtype=np.uint8)
        center = w // 2
        
        cross_section_x = 5.0 
        
        # Draw background uncertainty circle (faded)
        uncertainty_radius = int(self.last_uncertainty * center * 0.8)
        cv2.circle(img, (center, center), uncertainty_radius, (50, 50, 50), -1)

        for strand in self.rope.main_strands:
            x_coords = strand.base_trajectory[:, 0]
            closest_idx = np.argmin(np.abs(x_coords - cross_section_x))
            
            y = strand.base_trajectory[closest_idx, 1]
            z = strand.base_trajectory[closest_idx, 2]
            
            # Map YZ coordinates (range approx. [-4, 4]) to screen (0, w)
            y_screen = int(np.clip((y / 8.0 + 0.5) * w, 0, w-1))
            z_screen = int(np.clip((z / 8.0 + 0.5) * h, 0, h-1))
            
            # Draw strand (color based on coherence/selection)
            if strand.selected:
                color_val = int(strand.coherence * 255)
                color = (0, color_val, 255) # Cyan/Red for selected
                radius = 3
            else:
                color_val = int(strand.coherence * 255)
                color = (color_val, color_val, color_val) # Gray for decohered
                radius = 1
                
            cv2.circle(img, (y_screen, z_screen), radius, color, -1)

        if self.rope.selected_strand:
            y = self.rope.selected_strand.base_trajectory[closest_idx, 1]
            z = self.rope.selected_strand.base_trajectory[closest_idx, 2]
            y_screen = int(np.clip((y / 8.0 + 0.5) * w, 0, w-1))
            z_screen = int(np.clip((z / 8.0 + 0.5) * h, 0, h-1))
            cv2.circle(img, (y_screen, z_screen), 5, (255, 255, 255), 1) 

        return img

    def get_display_image(self):
        img_rgb = self._draw_cross_section()
        img_rgb = np.ascontiguousarray(img_rgb)
        
        h, w = img_rgb.shape[:2]
        return QtGui.QImage(img_rgb.data, w, h, 3*w, QtGui.QImage.Format.Format_BGR888)

    def get_config_options(self):
        return [
            ("Num Main Strands", "n_strands", self.n_strands, None),
            ("Selection Radius", "selection_radius", self.selection_radius, None),
        ]

=== FILE: fractalsteeringpilotnode.py ===

"""
Fractal Steering Pilot Node - Implements a feedback mechanism that analyzes the
complexity (contrast) of a fractal image and outputs a subtle steering vector
(X and Y nudges) designed to maximize the visible complexity.

Simulates the 'Fractal Surfer' honing in on a maximum information boundary.
"""

import numpy as np
from PyQt6 import QtGui
import cv2

import __main__
BaseNode = __main__.BaseNode
PA_INSTANCE = getattr(__main__, "PA_INSTANCE", None)
QtGui = __main__.QtGui

class FractalSteeringPilotNode(BaseNode):
    NODE_CATEGORY = "Transform"
    NODE_COLOR = QtGui.QColor(150, 100, 200) # Deep Steering Purple
    
    def __init__(self, nudge_factor=0.005, complexity_smoothing=0.9):
        super().__init__()
        self.node_title = "Fractal Steering Pilot"
        
        self.inputs = {
            'image_in': 'image',          # Current fractal image to analyze
            'steering_factor': 'signal'   # External control for nudge strength
        }
        self.outputs = {
            'x_nudge': 'signal',          # Nudge for X position
            'y_nudge': 'signal',          # Nudge for Y position
            'complexity': 'signal',       # Measured complexity (StDev)
        }
        
        self.nudge_factor = float(nudge_factor)
        self.complexity_smoothing = float(complexity_smoothing)
        
        # State tracking
        self.measured_complexity = 0.0
        self.last_nudge_x = 0.0
        self.last_nudge_y = 0.0

    def _measure_complexity(self, img):
        """Measures complexity using standard deviation (contrast)."""
        # Contrast (Standard Deviation) is an excellent, fast proxy for complexity.
        if img.size < 100: 
            return 0.0
        
        return np.std(img)

    def _calculate_steering_vector(self, complexity):
        """
        Calculates the steering vector based on complexity.
        Goal: Drift away from low-complexity areas, and drift randomly but slowly
        within high-complexity areas to explore boundaries.
        """
        
        # 1. Normalize complexity: Assume 0.3 is high complexity for a normalized image.
        target_complexity = 0.3 
        
        # 2. Steering based on perceived need:
        if complexity < target_complexity:
            # Low complexity (flat color): aggressively drift away from center
            # Direction vector: Random normalized direction
            angle = np.random.uniform(0, 2 * np.pi)
            base_nudge = self.nudge_factor * 2.0 # Higher speed to escape
        else:
            # High complexity (boundary): small, local exploration
            # Direction vector: Small random nudge
            angle = np.random.uniform(0, 2 * np.pi)
            base_nudge = self.nudge_factor * 0.5 # Slower speed to stick to boundary

        # 3. Apply steering factor and randomness
        nudge_x = base_nudge * np.cos(angle)
        nudge_y = base_nudge * np.sin(angle)
        
        return nudge_x, nudge_y

    def step(self):
        # 1. Get Inputs
        img_in = self.get_blended_input('image_in', 'mean')
        steering_factor_in = self.get_blended_input('steering_factor', 'sum') or 1.0
        
        if img_in is None or img_in.size == 0:
            return
        
        # Ensure image is grayscale (0-1)
        if img_in.ndim == 3:
             img_in = cv2.cvtColor(img_in.astype(np.float32), cv2.COLOR_BGR2GRAY)

        # 2. Measure Complexity
        new_complexity = self._measure_complexity(img_in)
        
        # Smooth the complexity metric to prevent chaotic jumps
        self.measured_complexity = (self.measured_complexity * self.complexity_smoothing +
                                    new_complexity * (1.0 - self.complexity_smoothing))

        # 3. Calculate Steering
        nudge_x, nudge_y = self._calculate_steering_vector(self.measured_complexity)
        
        # Apply external scaling factor
        self.last_nudge_x = nudge_x * steering_factor_in
        self.last_nudge_y = nudge_y * steering_factor_in


    def get_output(self, port_name):
        if port_name == 'x_nudge':
            return self.last_nudge_x
        elif port_name == 'y_nudge':
            return self.last_nudge_y
        elif port_name == 'complexity':
            # Normalize complexity to the 0-1 signal range
            return np.clip(self.measured_complexity * 4.0, 0.0, 1.0)
        return None
        
# In nodes/fractalsteeringpilotnode.py (Update get_display_image method, around line 124)

    def get_display_image(self):
        w, h = 96, 96
        # --- FIX: Change img initialization to 3 channels (RGB) ---
        img = np.zeros((h, w, 3), dtype=np.uint8) 
        # --- END FIX ---
        
        # 1. Visualize Learning Progress (Color represents Coupling Value)
        norm_coupling = self.measured_complexity * 255.0 * 2.0 
        comp_u8 = np.clip(norm_coupling, 0, 255).astype(np.uint8)
        
        # Green channel indicates high complexity, Red channel indicates low/escape
        color = (int(255 - comp_u8), int(comp_u8), 0) # BGR tuple with standard ints
        
        # This line was crashing:
        color = (int(255 - comp_u8), int(comp_u8), 0)
        
        # Draw arrow showing current nudge direction
        nudge_scale = 30
        end_x = int(w/2 + self.last_nudge_x * nudge_scale)
        end_y = int(h/2 + self.last_nudge_y * nudge_scale)
        
        # Draw the arrow in white
        cv2.arrowedLine(img, (w//2, h//2), (end_x, end_y), (255, 255, 255), 1)
        
        # Draw text in white
        cv2.putText(img, f"C: {self.measured_complexity:.2f}", (5, 15), 
                    cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255, 255, 255), 1, cv2.LINE_AA)
        
        img = np.ascontiguousarray(img)
        # We must return a QImage with 3 channels (Format_BGR888)
        return QtGui.QImage(img.data, w, h, 3*w, QtGui.QImage.Format.Format_BGR888)

    def get_config_options(self):
        return [
            ("Base Nudge Factor", "nudge_factor", self.nudge_factor, None),
            ("Complexity Smoothing", "complexity_smoothing", self.complexity_smoothing, None),
        ]

=== FILE: frameexporternode.py ===

#!/usr/bin/env python3
"""
Frame Exporter for Infinite Fractal Landscape
Add this to your Perception Lab to export high-quality frame sequences.

Usage:
1. Add this node to your workflow
2. Connect the fractal image output to this node's image input
3. Set export parameters in config
4. Run workflow - frames will be saved to disk

Commercial use: Export sequences for video editing or stock footage sales
"""

import numpy as np
import cv2
import os
from datetime import datetime
from pathlib import Path

import __main__
BaseNode = __main__.BaseNode
QtGui = __main__.QtGui

class FrameExporterNode(BaseNode):
    """Exports frames to disk for video production"""
    NODE_CATEGORY = "Output"
    NODE_COLOR = QtGui.QColor(255, 100, 100)
    
    def __init__(self, 
                 export_enabled=False,
                 output_dir="./fractal_export",
                 frame_prefix="fractal",
                 export_format="png",
                 export_every_n_frames=1,
                 max_frames=1000):
        super().__init__()
        self.node_title = "Frame Exporter"
        
        self.inputs = {
            'image': 'image',
            'trigger': 'signal'  # Set to 1.0 to enable export
        }
        self.outputs = {
            'frame_count': 'signal',
            'export_status': 'signal'
        }
        
        # Export settings
        self.export_enabled = bool(export_enabled)
        self.output_dir = str(output_dir)
        self.frame_prefix = str(frame_prefix)
        self.export_format = str(export_format)  # 'png', 'jpg', 'tiff'
        self.export_every_n_frames = int(export_every_n_frames)
        self.max_frames = int(max_frames)
        
        # State
        self.frame_counter = 0
        self.frames_exported = 0
        self.session_id = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.last_trigger = 0.0
        
        # Create output directory
        self.setup_output_dir()
        
    def setup_output_dir(self):
        """Create output directory structure"""
        session_dir = os.path.join(self.output_dir, self.session_id)
        Path(session_dir).mkdir(parents=True, exist_ok=True)
        self.session_dir = session_dir
        print(f"FrameExporter: Output directory: {self.session_dir}")
        
    def step(self):
        # Get input
        image = self.get_blended_input('image', 'max')
        trigger = self.get_blended_input('trigger', 'sum') or 0.0
        
        # Check if export should be enabled via trigger
        if trigger > 0.5 and self.last_trigger <= 0.5:
            self.export_enabled = not self.export_enabled
            print(f"FrameExporter: Export {'ENABLED' if self.export_enabled else 'DISABLED'}")
        self.last_trigger = trigger
        
        # Increment frame counter
        self.frame_counter += 1
        
        # Export if enabled and conditions met
        should_export = (
            self.export_enabled 
            and image is not None 
            and self.frame_counter % self.export_every_n_frames == 0
            and self.frames_exported < self.max_frames
        )
        
        if should_export:
            self.export_frame(image)
            
        # Output status
        self.set_output('frame_count', float(self.frame_counter))
        self.set_output('export_status', 1.0 if self.export_enabled else 0.0)
        
    def export_frame(self, image):
        """Save frame to disk"""
        try:
            # Generate filename
            filename = f"{self.frame_prefix}_{self.frames_exported:06d}.{self.export_format}"
            filepath = os.path.join(self.session_dir, filename)
            
            # Convert to uint8 if needed
            if image.dtype != np.uint8:
                if image.max() <= 1.0:
                    image = (image * 255).astype(np.uint8)
                else:
                    image = np.clip(image, 0, 255).astype(np.uint8)
            
            # Handle grayscale vs color
            if len(image.shape) == 2:
                # Grayscale - convert to BGR for color output
                image_bgr = cv2.cvtColor(image, cv2.COLOR_GRAY2BGR)
            elif image.shape[2] == 3:
                # Assume RGB, convert to BGR for OpenCV
                image_bgr = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)
            elif image.shape[2] == 4:
                # RGBA, convert to BGR
                image_bgr = cv2.cvtColor(image, cv2.COLOR_RGBA2BGR)
            else:
                image_bgr = image
            
            # Set quality based on format
            if self.export_format == 'jpg':
                cv2.imwrite(filepath, image_bgr, [cv2.IMWRITE_JPEG_QUALITY, 95])
            elif self.export_format == 'png':
                cv2.imwrite(filepath, image_bgr, [cv2.IMWRITE_PNG_COMPRESSION, 3])
            elif self.export_format == 'tiff':
                cv2.imwrite(filepath, image_bgr)
            else:
                cv2.imwrite(filepath, image_bgr)
            
            self.frames_exported += 1
            
            # Progress logging
            if self.frames_exported % 100 == 0:
                print(f"FrameExporter: {self.frames_exported} frames exported")
                
        except Exception as e:
            print(f"FrameExporter: Error exporting frame: {e}")


class VideoExporterNode(BaseNode):
    """Exports directly to video file using cv2.VideoWriter"""
    NODE_CATEGORY = "Output"
    NODE_COLOR = QtGui.QColor(255, 80, 80)
    
    def __init__(self,
                 export_enabled=False,
                 output_dir="./fractal_export",
                 filename="fractal_video",
                 fps=30,
                 codec='mp4v',
                 width=1920,
                 height=1080):
        super().__init__()
        self.node_title = "Video Exporter"
        
        self.inputs = {
            'image': 'image',
            'trigger': 'signal'
        }
        self.outputs = {
            'frame_count': 'signal',
            'recording': 'signal'
        }
        
        # Settings
        self.export_enabled = bool(export_enabled)
        self.output_dir = str(output_dir)
        self.filename = str(filename)
        self.fps = int(fps)
        self.codec = str(codec)
        self.width = int(width)
        self.height = int(height)
        
        # State
        self.writer = None
        self.frame_count = 0
        self.session_id = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.last_trigger = 0.0
        
        # Setup
        Path(self.output_dir).mkdir(parents=True, exist_ok=True)
        
    def start_recording(self):
        """Initialize video writer"""
        if self.writer is not None:
            self.stop_recording()
            
        output_path = os.path.join(
            self.output_dir,
            f"{self.filename}_{self.session_id}.mp4"
        )
        
        fourcc = cv2.VideoWriter_fourcc(*self.codec)
        self.writer = cv2.VideoWriter(
            output_path,
            fourcc,
            self.fps,
            (self.width, self.height)
        )
        
        if self.writer.isOpened():
            print(f"VideoExporter: Recording started: {output_path}")
            return True
        else:
            print(f"VideoExporter: Failed to open video writer")
            self.writer = None
            return False
            
    def stop_recording(self):
        """Finalize and close video file"""
        if self.writer is not None:
            self.writer.release()
            print(f"VideoExporter: Recording stopped. {self.frame_count} frames written.")
            self.writer = None
            self.frame_count = 0
            
    def step(self):
        # Get inputs
        image = self.get_blended_input('image', 'max')
        trigger = self.get_blended_input('trigger', 'sum') or 0.0
        
        # Toggle recording on trigger
        if trigger > 0.5 and self.last_trigger <= 0.5:
            if self.writer is None:
                self.start_recording()
            else:
                self.stop_recording()
        self.last_trigger = trigger
        
        # Write frame if recording
        if self.writer is not None and image is not None:
            try:
                # Resize to target resolution
                resized = cv2.resize(image, (self.width, self.height))
                
                # Convert to uint8 BGR
                if resized.dtype != np.uint8:
                    if resized.max() <= 1.0:
                        resized = (resized * 255).astype(np.uint8)
                    else:
                        resized = np.clip(resized, 0, 255).astype(np.uint8)
                        
                if len(resized.shape) == 2:
                    resized = cv2.cvtColor(resized, cv2.COLOR_GRAY2BGR)
                elif resized.shape[2] == 3:
                    resized = cv2.cvtColor(resized, cv2.COLOR_RGB2BGR)
                elif resized.shape[2] == 4:
                    resized = cv2.cvtColor(resized, cv2.COLOR_RGBA2BGR)
                    
                self.writer.write(resized)
                self.frame_count += 1
                
            except Exception as e:
                print(f"VideoExporter: Error writing frame: {e}")
                
        # Output status
        self.set_output('frame_count', float(self.frame_count))
        self.set_output('recording', 1.0 if self.writer is not None else 0.0)
        
    def cleanup(self):
        """Ensure video is finalized on node deletion"""
        self.stop_recording()


# Export both node classes
__all__ = ['FrameExporterNode', 'VideoExporterNode']


"""
USAGE EXAMPLES:

1. FRAME SEQUENCE EXPORT (for compositing):
   - Add FrameExporterNode to workflow
   - Connect fractal image -> FrameExporterNode.image
   - Set export_format='png' for lossless
   - Set export_every_n_frames=1 for every frame
   - Set max_frames=3000 for 100 seconds at 30fps
   - Connect trigger signal or manually set export_enabled=True

2. DIRECT VIDEO EXPORT (for quick sharing):
   - Add VideoExporterNode to workflow  
   - Connect fractal image -> VideoExporterNode.image
   - Set fps=60, width=1920, height=1080
   - Toggle recording with trigger signal
   - Video saves automatically when stopped

3. COMMERCIAL STOCK FOOTAGE:
   - Use FrameExporterNode with:
     * export_format='tiff' for maximum quality
     * Resolution set to 3840x2160 (4K)
     * Export 30 seconds = 900 frames at 30fps
   - Import sequence to video editor
   - Apply color grading
   - Export final at high bitrate
   - Upload to stock sites

4. REALTIME STREAMING:
   - Use VideoExporterNode
   - Set up OBS to capture the output folder
   - Stream the live generation process
   - Archive saves automatically

TO ADD TO YOUR PERCEPTION LAB:
1. Save this file as FrameExporterNode.py in your nodes directory
2. Restart Perception Lab
3. Nodes appear in "Output" category
4. Add to any workflow
"""

=== FILE: galaxy.py ===

"""
Galaxy Field Node - Creates spiral/galaxy patterns from signal inputs
Based on working galaxy.py physics with audio reactivity added
Requires: pip install torch
Place this file in the 'nodes' folder
"""

import numpy as np
from PyQt6 import QtGui
import cv2
import torch

import __main__
BaseNode = __main__.BaseNode
PA_INSTANCE = getattr(__main__, "PA_INSTANCE", None)
QtGui = __main__.QtGui

try:
    DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False
    print("Warning: GalaxyFieldNode requires 'torch'.")


class GalaxyFieldNode(BaseNode):
    NODE_CATEGORY = "Transform"
    NODE_COLOR = QtGui.QColor(150, 50, 150)  # Purple for galaxy
    
    def __init__(self, grid_size=128):
        super().__init__()
        self.node_title = "Galaxy Field"
        
        self.inputs = {
            'energy': 'signal',      # Drives field intensity
            'spin': 'signal',        # Creates rotation/spirals
            'seed_image': 'image'    # Optional: Seed the field with an image
        }
        self.outputs = {
            'field': 'image',        # Current field magnitude
            'memory': 'image',       # Persistent memory trace
            'total_energy': 'signal' # Field energy metric
        }
        
        if not TORCH_AVAILABLE:
            self.node_title = "Galaxy (No Torch!)"
            return
            
        self.grid_size = int(grid_size)
        self.dt = 0.015
        self.time = 0.0
        
        # Initialize fields on GPU
        self.psi = torch.zeros((self.grid_size, self.grid_size), 
                               dtype=torch.cfloat, device=DEVICE)
        self.psi_prev = torch.zeros_like(self.psi)
        self.memory = torch.zeros((self.grid_size, self.grid_size), 
                                  dtype=torch.float32, device=DEVICE)
        
        # Laplacian kernel
        self.laplace_kernel = torch.tensor(
            [[0, 1, 0], [1, -4, 1], [0, 1, 0]], 
            dtype=torch.float32, device=DEVICE
        ).unsqueeze(0).unsqueeze(0)
        
        # Seed with a spiral center
        self._initialize_spiral()
        
    def _initialize_spiral(self):
        """Seed the field with a spiral pattern"""
        Y, X = torch.meshgrid(
            torch.arange(self.grid_size, device=DEVICE), 
            torch.arange(self.grid_size, device=DEVICE), 
            indexing='ij'
        )
        cx, cy = self.grid_size // 2, self.grid_size // 2
        r = torch.sqrt((X - cx)**2 + (Y - cy)**2)
        theta = torch.atan2(Y - cy, X - cx)
        
        # Spiral seed - FIX: Create complex exponential properly
        phase = theta
        self.psi = torch.exp(-r**2 / 300.0) * (torch.cos(phase) + 1j * torch.sin(phase))
        self.psi_prev = self.psi.clone()
    
    def _laplacian(self, field):
        """Compute Laplacian using convolution"""
        real_part = torch.nn.functional.conv2d(
            field.real.unsqueeze(0).unsqueeze(0), 
            self.laplace_kernel, 
            padding=1
        ).squeeze()
        
        imag_part = torch.nn.functional.conv2d(
            field.imag.unsqueeze(0).unsqueeze(0), 
            self.laplace_kernel, 
            padding=1
        ).squeeze()
        
        return real_part + 1j * imag_part
    
    def _add_energy_pulse(self, energy_level):
        """Add energy to the field based on input signal"""
        if energy_level > 0.1:
            # Create a localized pulse at a random location
            Y, X = torch.meshgrid(
                torch.arange(self.grid_size, device=DEVICE), 
                torch.arange(self.grid_size, device=DEVICE), 
                indexing='ij'
            )
            
            # Pulse location (varies with time for variety)
            px = self.grid_size // 2 + int(30 * np.sin(self.time * 0.5))
            py = self.grid_size // 2 + int(30 * np.cos(self.time * 0.7))
            
            r = torch.sqrt((X - px)**2 + (Y - py)**2)
            pulse = energy_level * 2.0 * torch.exp(-r**2 / 100.0)
            
            # FIX: Create complex exponential properly for PyTorch
            phase = self.time * 3.0
            phase_complex = torch.cos(torch.tensor(phase, device=DEVICE)) + \
                           1j * torch.sin(torch.tensor(phase, device=DEVICE))
            
            self.psi += pulse * phase_complex
    
    def _add_spin_force(self, spin_strength):
        """Add rotational force to create spiral patterns"""
        if abs(spin_strength) > 0.1:
            Y, X = torch.meshgrid(
                torch.arange(self.grid_size, device=DEVICE), 
                torch.arange(self.grid_size, device=DEVICE), 
                indexing='ij'
            )
            cx, cy = self.grid_size // 2, self.grid_size // 2
            theta = torch.atan2(Y - cy, X - cx)
            
            # FIX: Rotational phase using cos + i*sin
            spin_phase = spin_strength * theta * 0.05
            phase_complex = torch.cos(spin_phase) + 1j * torch.sin(spin_phase)
            self.psi *= phase_complex
    
    def _seed_from_image(self, img):
        """Seed the field from an input image"""
        if img is None:
            return
            
        # Resize image to grid
        img_resized = cv2.resize(img, (self.grid_size, self.grid_size))
        
        # Convert to torch tensor
        img_torch = torch.from_numpy(img_resized).to(DEVICE, dtype=torch.float32)
        
        # FIX: Add to field as amplitude modulation with proper complex exponential
        phase = self.time
        phase_complex = torch.cos(torch.tensor(phase, device=DEVICE)) + \
                       1j * torch.sin(torch.tensor(phase, device=DEVICE))
        self.psi += (img_torch - 0.5) * 0.5 * phase_complex

    def step(self):
        if not TORCH_AVAILABLE:
            return
            
        # Get inputs
        energy = self.get_blended_input('energy', 'sum') or 0.0
        spin = self.get_blended_input('spin', 'sum') or 0.0
        seed_img = self.get_blended_input('seed_image', 'mean')
        
        # Add energy pulses
        self._add_energy_pulse(energy)
        
        # Add spin force
        self._add_spin_force(spin)
        
        # Seed from image (occasional)
        if seed_img is not None and np.random.rand() < 0.05:  # 5% chance per frame
            self._seed_from_image(seed_img)
        
        # --- Core Field Evolution (from galaxy.py) ---
        laplacian = self._laplacian(self.psi)
        
        # Wave equation with damping
        psi_new = (2 * self.psi - self.psi_prev + 
                   self.dt**2 * (1.2 * laplacian - 0.03 * self.psi))
        
        # Amplitude limiting (prevent blow-up)
        amp = torch.abs(psi_new)
        max_amp = 5.0
        mask = amp > max_amp
        psi_new[mask] = psi_new[mask] / amp[mask] * max_amp
        
        # Update memory (persistent trace)
        self.memory = 0.995 * self.memory + 0.005 * torch.abs(self.psi)**2
        
        # Update fields
        self.psi_prev = self.psi.clone()
        self.psi = psi_new
        
        self.time += self.dt

    def get_output(self, port_name):
        if port_name == 'field':
            # Return field magnitude
            field_cpu = torch.abs(self.psi).cpu().numpy().astype(np.float32)
            # Normalize
            max_val = field_cpu.max()
            if max_val > 1e-9:
                return field_cpu / max_val
            return field_cpu
            
        elif port_name == 'memory':
            # Return memory trace
            memory_cpu = self.memory.cpu().numpy().astype(np.float32)
            # Normalize
            max_val = memory_cpu.max()
            if max_val > 1e-9:
                return memory_cpu / max_val
            return memory_cpu
            
        elif port_name == 'total_energy':
            # Return total field energy
            return float(torch.sum(torch.abs(self.psi)**2).cpu().numpy())
            
        return None
        
    def get_display_image(self):
        # Visualize the memory field with a colormap
        memory_np = self.memory.cpu().numpy()
        
        # Normalize
        max_val = memory_np.max()
        if max_val > 1e-9:
            memory_norm = memory_np / max_val
        else:
            memory_norm = memory_np
            
        img_u8 = (memory_norm * 255).astype(np.uint8)
        
        # Apply magma colormap
        img_color = cv2.applyColorMap(img_u8, cv2.COLORMAP_MAGMA)
        
        img_color = np.ascontiguousarray(img_color)
        h, w = img_color.shape[:2]
        return QtGui.QImage(img_color.data, w, h, 3*w, QtGui.QImage.Format.Format_BGR888)

    def get_config_options(self):
        return [
            ("Grid Size (NxN)", "grid_size", self.grid_size, None),
        ]
    
    def randomize(self):
        """Re-initialize with a new spiral seed"""
        if TORCH_AVAILABLE:
            self._initialize_spiral()

=== FILE: heightmapflyernode.py ===

"""
HeightmapFlyerNode (Pseudo-3D "Mode 7" Renderer)

Takes a 2D image as a ground/heightmap and renders it
with a 3D perspective "fly-over" camera.
"""

import numpy as np
import cv2

# --- Magic import block ---
import __main__
BaseNode = __main__.BaseNode
QtGui = __main__.QtGui
# --------------------------

class HeightmapFlyerNode(BaseNode):
    """
    Simulates a 3D fly-over of a 2D heightmap image.
    """
    NODE_CATEGORY = "Generator"
    NODE_COLOR = QtGui.QColor(100, 150, 220) # Blue/Purple

    def __init__(self, size=256):
        super().__init__()
        self.node_title = "Heightmap Flyer"
        
        self.inputs = {
            'image_in': 'image',    # The ground texture
            'pitch': 'signal',      # 0 (top-down) to 1 (max perspective)
            'yaw': 'signal',        # -1 to 1 (rotation)
            'speed_y': 'signal',    # -1 to 1 (forward/back)
            'speed_x': 'signal',    # -1 to 1 (strafe left/right)
            'zoom': 'signal'        # 0 to 1 (altitude/scale)
        }
        self.outputs = {'image': 'image'}
        
        self.size = int(size)
        self.display_image = np.zeros((self.size, self.size, 3), dtype=np.float32)
        
        # Keep track of our "position" in the world
        self.scroll_x = 0.0
        self.scroll_y = 0.0

    def step(self):
        # --- 1. Get Control Signals ---
        pitch_in = self.get_blended_input('pitch', 'sum') or 0.2
        yaw_in = self.get_blended_input('yaw', 'sum') or 0.0
        speed_y_in = self.get_blended_input('speed_y', 'sum') or 0.0
        speed_x_in = self.get_blended_input('speed_x', 'sum') or 0.0
        zoom_in = self.get_blended_input('zoom', 'sum') or 0.5

        # --- 2. Get and Prepare Image ---
        img = self.get_blended_input('image_in', 'first')
        if img is None:
            # Use a simple checkerboard if no image is connected
            y, x = np.mgrid[0:self.size, 0:self.size]
            check = ((x // 32) + (y // 32)) % 2
            img = np.stack([check] * 3, axis=-1).astype(np.float32)
        
        if img.shape[0] != self.size or img.shape[1] != self.size:
            img = cv2.resize(img, (self.size, self.size), 
                             interpolation=cv2.INTER_LINEAR)
        if img.ndim == 2:
            img = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)
        
        # Ensure float32 in 0-1 range (fixes potential cvtColor errors)
        if img.dtype != np.float32:
            img = img.astype(np.float32)
        if img.max() > 1.0:
            img = img / 255.0
        
        img = np.clip(img, 0, 1)
        h, w = self.size, self.size

        # --- 3. Update Camera Position ---
        self.scroll_x += speed_x_in * 5.0
        self.scroll_y += speed_y_in * 5.0
        self.scroll_x %= w
        self.scroll_y %= h

        # --- 4. Build Transformation Matrices ---
        
        # a) Zoom (Altitude) and Translation (X/Y position)
        zoom_val = 1.0 + zoom_in * 2.0 # Scale from 1x to 3x

        # --- START FIX ---
        # We must use 3x3 matrices (homogeneous coords) to combine affine transforms.
        
        # M_scroll_zoom is (3, 3)
        M_scroll_zoom_3x3 = np.float32([
            [zoom_val, 0, self.scroll_x],
            [0, zoom_val, self.scroll_y],
            [0, 0, 1]
        ])
        
        # b) Yaw (Rotation)
        center = (w // 2, h // 2)
        angle_deg = yaw_in * 90.0
        
        # M_yaw_2x3 is (2, 3)
        M_yaw_2x3 = cv2.getRotationMatrix2D(center, angle_deg, 1.0)
        # M_yaw_3x3 is (3, 3)
        M_yaw_3x3 = np.vstack([M_yaw_2x3, [0, 0, 1]])
        
        # Combine affine transforms (scroll, zoom, yaw)
        # This is now a (3, 3) @ (3, 3) multiplication
        # The order matters: apply zoom/scroll first, THEN yaw
        M_affine_3x3 = M_yaw_3x3 @ M_scroll_zoom_3x3
        
        # Get the final (2, 3) matrix for warpAffine
        M_affine = M_affine_3x3[0:2, :]
        # --- END FIX ---
        
        # Apply affine transforms
        # BORDER_WRAP makes the world tile infinitely
        pre_transformed = cv2.warpAffine(img, M_affine, (w, h), 
                                         borderMode=cv2.BORDER_WRAP)
        
        # c) Pitch (Perspective)
        pitch_amount = np.clip(pitch_in, 0, 0.9) * (w / 2.2)
        
        src_pts = np.float32([
            [0, 0], [w - 1, 0],
            [w - 1, h - 1], [0, h - 1]
        ])
        
        dst_pts = np.float32([
            [pitch_amount, 0], [w - 1 - pitch_amount, 0],
            [w - 1, h - 1], [0, h - 1]
        ])
        
        M_perspective = cv2.getPerspectiveTransform(src_pts, dst_pts)
        
        # --- 5. Apply Final Transform ---
        self.display_image = cv2.warpPerspective(
            pre_transformed, M_perspective, (w, h), 
            borderMode=cv2.BORDER_CONSTANT, 
            borderValue=(0,0,0) # Fill horizon with black
        )

    def get_output(self, port_name):
        if port_name == 'image':
            return self.display_image
        return None

=== FILE: holoencodernode.py ===

"""
HoloEncoder Node (v4 - Fixed Outputs)
------------------
This node implements holographic/holographic-like compression,
converting a 2D image (spatial domain) into a 1D complex signal
(temporal/frequency domain). It can also decompress this signal
back into an image.

This is inspired by "Time-Domain Brain" concepts, where spatial
information might be encoded as a complex temporal pattern or
wave interference pattern for storage and broadcast.

FIX v4:
- `image_out` (blue port) now correctly outputs the reconstructed
  image when in 'Compress' mode, matching the internal display.
- `signal_out_real` (now orange port) is correctly typed as
  'spectrum' (a 1D float array) instead of 'signal' (a single float).
"""

import numpy as np
import cv2

import __main__
BaseNode = __main__.BaseNode
PA_INSTANCE = getattr(__main__, "PA_INSTANCE", None)
QtGui = __main__.QtGui

try:
    from scipy.fft import fft, ifft, fft2, ifft2, fftshift, ifftshift
    SCIPY_AVAILABLE = True
except ImportError:
    SCIPY_AVAILABLE = False
    print("Warning: HoloEncoderNode requires scipy.fft")

if QtGui is None:
    print("CRITICAL: HoloEncoderNode could not import QtGui from host.")


class HoloEncoderNode(BaseNode):
    NODE_CATEGORY = "Cognitive"
    NODE_COLOR = QtGui.QColor(220, 100, 100)  # Holographic Red
    
    def __init__(self, mode='Compress', compression_ratio=0.1, reference_phase_seed=42):
        super().__init__()
        self.node_title = "HoloEncoder"
        
        # Define port types. 'complex_spectrum' is a custom type
        # that the BaseNode will treat as "not 'signal'",
        # which is correct for handling arrays.
        self.inputs = {
            'image_in': 'image',
            'signal_in': 'complex_spectrum', 
        }
        
        self.outputs = {
            'image_out': 'image',
            'signal_out_complex': 'complex_spectrum', # The full complex signal
            # --- FIX: Changed port type from 'signal' to 'spectrum' ---
            # 'signal' is for single floats, 'spectrum' is for 1D float arrays.
            'signal_out_real': 'spectrum'            # The real magnitude for other nodes
        }
        
        if not SCIPY_AVAILABLE or QtGui is None:
            self.node_title = "HoloEncoder (ERROR)"
            self._error = True
            return
        self._error = False
            
        # --- Configurable Parameters ---
        self.mode = str(mode) # 'Compress' or 'Decompress'
        self.compression_ratio = float(compression_ratio)
        self.reference_phase_seed = int(reference_phase_seed)

        # --- Internal State ---
        self._last_seed = self.reference_phase_seed
        self.reference_phase_map = None
        self.input_shape = (64, 64) # Default
        
        self._update_reference_map() # Initialize the map
        
        # Buffers for display
        self.display_in = np.zeros((64, 64, 3), dtype=np.uint8)
        self.display_out = np.zeros((64, 64, 3), dtype=np.uint8)
        
        # Output buffers for ports
        self.signal_out_complex_buffer = None
        self.signal_out_real_buffer = None
        self.image_out_buffer = None

    def _update_reference_map(self):
        """
        Creates the complex reference wave based on the seed.
        This is the "holographic plate" or "interference key".
        """
        if self.input_shape is None:
            return
        # Use a fixed seed for a stable reference wave
        rng = np.random.default_rng(self.reference_phase_seed)
        phase_angles = rng.uniform(0, 2 * np.pi, self.input_shape)
        self.reference_phase_map = np.exp(1j * phase_angles).astype(np.complex64)
        self._last_seed = self.reference_phase_seed

    def _check_config_change(self, new_shape=None):
        """Check if we need to regenerate the reference map."""
        shape_changed = False
        if new_shape is not None and new_shape != self.input_shape:
            self.input_shape = new_shape
            shape_changed = True
            
        if self.reference_phase_seed != self._last_seed or shape_changed:
            self._update_reference_map()

    def _normalize_image_in(self, img_in):
        """Converts any input image to a 2D float (0-1) array."""
        if img_in.ndim == 3:
            img_in = np.mean(img_in, axis=2) # Convert to grayscale
        
        if img_in.dtype == np.uint8:
            img_float = img_in.astype(np.float32) / 255.0
        else:
            # Assumes it's a float array (e.g., from CorticalReconstruction)
            img_float = img_in.astype(np.float32)
            max_val = img_float.max()
            if max_val > 1e-6:
                img_float = (img_float - img_float.min()) / (max_val - img_float.min() + 1e-9)
            
        return np.clip(img_float, 0, 1)

    def step(self):
        if self._error: return
        
        if self.mode == 'Compress':
            self._step_compress()
        else:
            self._step_decompress()

    def _step_compress(self):
        # --- Mode: Image -> Signal ---
        self.node_title = "HoloEncoder (Compress)"
        image_in = self.get_blended_input('image_in', 'mean')
        if image_in is None:
            # --- FIX: Clear outputs if no input ---
            self.image_out_buffer = None
            self.signal_out_complex_buffer = None
            self.signal_out_real_buffer = None
            self.display_in = np.zeros_like(self.display_in)
            self.display_out = np.zeros_like(self.display_out)
            return

        # 1. Prepare Input Image
        img_float = self._normalize_image_in(image_in)
        self._check_config_change(img_float.shape)
        
        # Store for display
        self.display_in = cv2.cvtColor((img_float * 255).astype(np.uint8), cv2.COLOR_GRAY2RGB)

        # 2. Holographic Encoding (as per script)
        # Combine image amplitude with reference phase
        object_wave = img_float * self.reference_phase_map
        
        # Transform to frequency domain (the "hologram")
        hologram_freq = fftshift(fft2(object_wave))
        
        # 3. Compress
        # Keep only the central part of the spectrum
        h, w = hologram_freq.shape
        k = int(np.sqrt(h * w * self.compression_ratio))
        k = max(1, k) # Ensure at least 1
        
        start_h, end_h = (h - k) // 2, (h + k) // 2
        start_w, end_w = (w - k) // 2, (w + k) // 2
        
        compressed_spectrum = hologram_freq[start_h:end_h, start_w:end_w]
        
        # 4. Flatten to 1D Signal for output
        self.signal_out_complex_buffer = compressed_spectrum.flatten()
        # --- Create Real (Magnitude) version for other nodes ---
        self.signal_out_real_buffer = np.abs(self.signal_out_complex_buffer).astype(np.float32)
        
        # 5. Decompress for verification display AND output
        # --- FIX: Output the reconstructed image to the blue port ---
        self.image_out_buffer = self._decompress_signal(self.signal_out_complex_buffer, self.input_shape)
        self.display_out = cv2.cvtColor((self.image_out_buffer * 255).astype(np.uint8), cv2.COLOR_GRAY2RGB)
        
    def _step_decompress(self):
        # --- Mode: Signal -> Image ---
        self.node_title = "HoloEncoder (Decompress)"
        
        signal_in_list = self.get_blended_input('signal_in', 'raw_list') # Get the list of inputs
        if not signal_in_list:
             # --- FIX: Clear outputs if no input ---
            self.image_out_buffer = None
            self.signal_out_complex_buffer = None
            self.signal_out_real_buffer = None
            self.display_in = np.zeros_like(self.display_in)
            self.display_out = np.zeros_like(self.display_out)
            return
        signal_in = signal_in_list[0] # Get the first (and likely only) signal

        # 1. Check/update reference map
        # We need a target shape, use the last known shape or default
        self._check_config_change() 
        
        # 2. Decompress
        # Convert input signal (which might be float) to complex
        signal_in_complex = np.array(signal_in).astype(np.complex64)
        decomp_img = self._decompress_signal(signal_in_complex, self.input_shape)
        
        self.image_out_buffer = decomp_img # This is the main output
        self.signal_out_complex_buffer = None
        self.signal_out_real_buffer = None
        
        # 3. Prepare for display
        self.display_out = cv2.cvtColor((decomp_img * 255).astype(np.uint8), cv2.COLOR_GRAY2RGB)
        # Show the input signal's spectrum as "input"
        self.display_in = self._visualize_spectrum(signal_in_complex, self.input_shape)

    def _decompress_signal(self, signal, target_shape):
        """Internal decompression logic, usable by both modes."""
        h, w = target_shape
        
        # 1. Reconstruct Spectrum
        k_h = k_w = int(np.sqrt(signal.size))
        if k_h * k_w != signal.size: # Handle non-square
             k_h = k_w = int(np.floor(np.sqrt(signal.size)))
             if k_h * k_w == 0: return np.zeros(target_shape, dtype=np.float32) # Not enough data
             signal = signal[:k_h*k_w]
        
        compressed_spectrum = signal.reshape((k_h, k_w))
        
        full_spectrum = np.zeros(target_shape, dtype=np.complex64)
        start_h, end_h = (h - k_h) // 2, (h + k_h) // 2
        start_w, end_w = (w - k_w) // 2, (w + k_w) // 2
        
        # Handle cases where k is odd/even
        h_slice = slice(start_h, start_h + k_h)
        w_slice = slice(start_w, start_w + k_w)

        full_spectrum[h_slice, w_slice] = compressed_spectrum
        
        # 2. Inverse FFT
        reconstructed_wave = ifft2(ifftshift(full_spectrum))
        
        # 3. Decode with reference phase
        # This is the key: multiply by the conjugate of the reference
        reconstructed_image_complex = reconstructed_wave * np.conj(self.reference_phase_map)
        
        # 4. Take absolute value (amplitude)
        reconstructed_image = np.abs(reconstructed_image_complex)
        
        # Normalize for output
        max_val = reconstructed_image.max()
        if max_val > 1e-6:
            reconstructed_image = (reconstructed_image - reconstructed_image.min()) / (max_val - reconstructed_image.min())
            
        return np.clip(reconstructed_image, 0, 1).astype(np.float32)

    def _visualize_spectrum(self, signal, target_shape):
        """Helper for creating a displayable spectrum for Decompress mode."""
        h, w = target_shape
        k_h = k_w = int(np.sqrt(signal.size))
        if k_h * k_w != signal.size:
             k_h = k_w = int(np.floor(np.sqrt(signal.size)))
             if k_h*k_w == 0: return np.zeros((64,64,3), dtype=np.uint8)
             signal = signal[:k_h*k_w]
             
        spectrum = signal.reshape((k_h, k_w))
        
        full_spectrum_vis = np.zeros(target_shape, dtype=np.float32)
        start_h, end_h = (h - k_h) // 2, (h + k_h) // 2
        start_w, end_w = (w - k_w) // 2, (w + k_w) // 2
        
        # Handle cases where k is odd/even
        h_slice = slice(start_h, start_h + k_h)
        w_slice = slice(start_w, start_w + k_w)

        # Log magnitude for visualization
        log_mag = np.log1p(np.abs(spectrum))
        log_mag_norm = (log_mag - log_mag.min()) / (log_mag.max() - log_mag.min() + 1e-9)
        
        full_spectrum_vis[h_slice, w_slice] = log_mag_norm
        
        img_u8 = (full_spectrum_vis * 255).astype(np.uint8)
        return cv2.cvtColor(img_u8, cv2.COLOR_GRAY2RGB)

    def get_output(self, port_name):
        if self._error: return None
        if port_name == 'image_out':
            return self.image_out_buffer
        elif port_name == 'signal_out_complex':
            return self.signal_out_complex_buffer
        elif port_name == 'signal_out_real':
            return self.signal_out_real_buffer
        return None

    def get_display_image(self):
        if self._error: return None
        
        display_h = 128
        display_w = 256
        display = np.zeros((display_h, display_w, 3), dtype=np.uint8)
        
        # --- Left side: "Input" ---
        in_resized = cv2.resize(self.display_in, (display_h, display_h), interpolation=cv2.INTER_NEAREST)
        display[:, :display_h] = in_resized
        
        # --- Right side: "Output" ---
        out_resized = cv2.resize(self.display_out, (display_h, display_h), interpolation=cv2.INTER_NEAREST)
        display[:, display_w-display_h:] = out_resized
        
        # Add dividing line
        display[:, display_h-1:display_h+1] = [255, 255, 255]
        
        font = cv2.FONT_HERSHEY_SIMPLEX
        
        if self.mode == 'Compress':
            in_label = 'IN (Image)'
            out_label = 'OUT (Reconstructed)'
            info_text = f"COMPRESSING (Ratio: {self.compression_ratio:.2f})"
        else:
            in_label = 'IN (Spectrum)'
            out_label = 'OUT (Image)'
            info_text = "DECOMPRESSING"

        cv2.putText(display, in_label, (10, 15), font, 0.4, (255, 255, 255), 1, cv2.LINE_AA)
        cv2.putText(display, out_label, (display_h + 10, 15), font, 0.4, (255, 255, 255), 1, cv2.LINE_AA)
        cv2.putText(display, info_text, (10, display_h - 10), font, 0.4, (220, 100, 100), 1, cv2.LINE_AA)
        
        display = np.ascontiguousarray(display)
        return QtGui.QImage(display.data, display_w, display_h, 3*display_w, QtGui.QImage.Format.Format_RGB888)

    def get_config_options(self):
        return [
            ("Mode", "mode", self.mode, [
                ("Compress (Image->Signal)", "Compress"),
                ("Decompress (Signal->Image)", "Decompress")
            ]),
            ("Compression Ratio", "compression_ratio", self.compression_ratio, None),
            ("Reference Phase Seed", "reference_phase_seed", self.reference_phase_seed, None),
        ]

    # This is a special function to tell the host app how to handle array inputs
    def get_blended_input(self, port_name, blend_mode='sum'):
        # --- FIX: This method must be copied from the host BaseNode ---
        # --- so the node can correctly parse its own custom input types ---
        
        values = self.input_data.get(port_name, [])
        if not values:
            return None
            
        if blend_mode == 'raw_list':
            return values # Return the whole list of inputs

        # Check the type of the first item to decide the blend strategy
        first_val = values[0]
        
        if isinstance(first_val, (int, float)):
            # Handle simple signals (sum, mean, or first)
            if blend_mode == 'sum':
                return np.sum(values)
            elif blend_mode == 'mean':
                return np.mean(values)
            return values[0] # Default to 'first'

        elif isinstance(first_val, np.ndarray):
            # Handle array inputs (images, spectrums)
            
            # Check if it's complex
            if np.iscomplexobj(first_val):
                if blend_mode == 'mean':
                    # Safely average complex arrays
                    return np.mean([v for v in values if v is not None and v.size > 0], axis=0)
                return values[0] # Default to 'first'
            else:
                # Safely average real float/int arrays
                if blend_mode == 'mean':
                    return np.mean([v.astype(float) for v in values if v is not None and v.size > 0], axis=0)
                return values[0] # Default to 'first'
                
        # Default fallback for other types
        return values[0]

=== FILE: humanattractornode.py ===

"""
Human Attractor Node - A self-modifying strange loop
Models the recursive W â WÂ·Ï â W' cycle that might be consciousness/freedom.

Features:
- Internal W matrix that learns from experience
- Refractory periods (exhaustion, recovery)
- Pain from clarity (entropy cost of self-awareness)
- Attractor basins (habits, choices, learned patterns)
- Memory decay (forgetting, seizure-like resets)
- Attention (selective Ï sampling)
- Strange loop (self-modification based on self-observation)

Place this file in the 'nodes' folder as 'humanattractor.py'
"""

import numpy as np
from PyQt6 import QtGui
import cv2
from collections import deque
import math

import __main__
BaseNode = __main__.BaseNode
PA_INSTANCE = getattr(__main__, "PA_INSTANCE", None)
QtGui = __main__.QtGui


class HumanAttractorNode(BaseNode):
    NODE_CATEGORY = "Source"
    NODE_COLOR = QtGui.QColor(180, 60, 120)  # Deep human pink
    
    def __init__(self, 
                 w_size=8, 
                 learning_rate=0.01,
                 refractory_period=30,
                 pain_sensitivity=0.5):
        super().__init__()
        self.node_title = "Human Attractor"
        
        self.inputs = {
            'psi_external': 'signal',      # External world input
            'pain_stimulus': 'signal',      # Things that hurt
            'dopamine': 'signal',           # Reward signal
            'reset_trauma': 'signal'        # Seizure/trauma reset (>0.5 triggers)
        }
        
        self.outputs = {
            'consciousness': 'signal',      # Current WÂ·Ï projection
            'free_will_signal': 'signal',   # Measure of choice capacity
            'pain_level': 'signal',         # Current suffering
            'attractor_state': 'image',     # Visualization of W matrix
            'memory_trace': 'signal',       # Integrated experience
            'refractory': 'signal'          # Exhaustion level (0=ready, 1=exhausted)
        }
        
        # === Core Parameters ===
        self.w_size = int(w_size)
        self.learning_rate = float(learning_rate)
        self.refractory_max = int(refractory_period)
        self.pain_sensitivity = float(pain_sensitivity)
        
        # === The W Matrix (Your Neurons) ===
        # This is the learned projection operator
        self.W = np.random.randn(self.w_size, self.w_size) * 0.1
        self.W = (self.W + self.W.T) / 2  # Make symmetric (like Hebbian learning)
        
        # === Internal State ===
        self.psi_internal = np.random.randn(self.w_size) * 0.1  # Internal field
        self.consciousness_value = 0.0  # Current WÂ·Ï projection magnitude
        
        # Attractor basins (learned habits/patterns)
        self.attractors = []  # List of learned attractor states
        self._init_default_attractors()
        
        # === Refractory Period (Neuron Exhaustion) ===
        self.refractory_timer = 0  # Counts down from refractory_max
        self.dopamine_level = 0.5  # Current dopamine (motivation)
        self.exhaustion = 0.0  # 0=fresh, 1=depleted
        
        # === Pain and Clarity ===
        self.pain_level = 0.0  # Current suffering
        self.clarity_cost = 0.0  # Entropy cost of self-awareness
        
        # === Memory ===
        self.memory_trace = 0.0  # Integrated experience over time
        self.memory_buffer = deque(maxlen=100)  # Recent WÂ·Ï projections
        
        # === Free Will Measure ===
        self.choice_entropy = 0.0  # How many basins are available
        self.free_will_signal = 0.5
        
        # === Loop Iteration Counter ===
        self.loop_iterations = 0
        self.time = 0.0
        
    def _init_default_attractors(self):
        """Initialize with some basic attractor basins (like instincts)"""
        # Attractor 1: "Home/Safe" (low energy, coherent)
        home = np.zeros(self.w_size)
        home[0] = 1.0
        self.attractors.append({"state": home, "strength": 1.0, "name": "home"})
        
        # Attractor 2: "Explore/Novel" (high energy, chaotic)
        explore = np.random.randn(self.w_size) * 0.5
        self.attractors.append({"state": explore, "strength": 0.7, "name": "explore"})
        
        # Attractor 3: "Pain Avoidance" (negative gradient)
        avoid = -np.ones(self.w_size) * 0.3
        self.attractors.append({"state": avoid, "strength": 0.5, "name": "avoid"})
        
    def _project(self, psi):
        """
        The core operation: A[Ï] = W Â· Ï
        This is "being conscious of something"
        """
        projection = np.dot(self.W, psi)
        return projection
    
    def _measure_clarity_cost(self):
        """
        Self-awareness has an entropy cost.
        When you observe yourself (W projects WÂ·Ï), you pay for clarity.
        """
        # Entropy of W (how spread out is the projection?)
        eigenvalues = np.linalg.eigvalsh(self.W)
        eigenvalues = np.abs(eigenvalues) + 1e-10
        eigenvalues /= np.sum(eigenvalues)
        
        entropy = -np.sum(eigenvalues * np.log(eigenvalues + 1e-10))
        
        # High entropy = diffuse awareness = low cost
        # Low entropy = focused awareness = high cost (hurts to see clearly)
        clarity_cost = 1.0 / (entropy + 1e-3)
        
        return clarity_cost
    
    def _find_nearest_attractor(self, state):
        """
        Which learned basin is this state closest to?
        Returns: (attractor_index, distance)
        """
        min_dist = float('inf')
        nearest_idx = 0
        
        for i, attr in enumerate(self.attractors):
            dist = np.linalg.norm(state - attr["state"])
            if dist < min_dist:
                min_dist = dist
                nearest_idx = i
        
        return nearest_idx, min_dist
    
    def _measure_free_will(self):
        """
        How much choice do you have?
        Free will = number of accessible attractor basins
        
        If only one basin is accessible â no freedom (deterministic)
        If many basins are accessible â freedom (choice)
        """
        current_state = self.psi_internal
        
        # Count how many attractors are within reach
        accessible = 0
        for attr in self.attractors:
            dist = np.linalg.norm(current_state - attr["state"])
            # If distance < threshold and you have energy â accessible
            if dist < 2.0 and self.dopamine_level > 0.3 and self.refractory_timer == 0:
                accessible += 1
        
        # Entropy of choice (more options = more freedom)
        if accessible > 1:
            # Shannon entropy of uniform distribution over choices
            choice_entropy = np.log(accessible)
        else:
            choice_entropy = 0.0
        
        # Normalize to [0, 1]
        max_entropy = np.log(len(self.attractors))
        free_will = choice_entropy / (max_entropy + 1e-9)
        
        return free_will
    
    def _learn_from_experience(self, psi_external, dopamine):
        """
        The strange loop: W modifies itself based on WÂ·Ï projection.
        Hebbian learning: "Neurons that fire together, wire together"
        """
        if self.refractory_timer > 0:
            return  # Can't learn during refractory period
        
        # Project current state
        projection = self._project(self.psi_internal)
        
        # Learning rule: ÎW â Ï â Ï (outer product)
        # Modulated by dopamine (reward) and pain (punishment)
        learning_signal = dopamine - self.pain_level * 0.5
        
        # Hebbian update
        dW = np.outer(self.psi_internal, self.psi_internal) * learning_signal * self.learning_rate
        
        # Anti-Hebbian if painful (unlearn)
        if self.pain_level > 0.7:
            dW *= -0.5
        
        self.W += dW
        
        # Keep W bounded
        self.W = np.clip(self.W, -2.0, 2.0)
        
        # Re-symmetrize (maintain structure)
        self.W = (self.W + self.W.T) / 2
        
    def _create_new_attractor(self):
        """
        When you do something novel repeatedly, it becomes a new habit.
        This is how "free" choices become deterministic patterns.
        """
        current_state = self.psi_internal.copy()
        
        # Check if this is actually novel (far from existing attractors)
        _, min_dist = self._find_nearest_attractor(current_state)
        
        if min_dist > 1.5 and len(self.attractors) < 10:
            # Create new attractor
            new_attractor = {
                "state": current_state,
                "strength": 0.3,  # Start weak
                "name": f"learned_{len(self.attractors)}"
            }
            self.attractors.append(new_attractor)
    
    def _pull_toward_attractor(self):
        """
        Like gravity: current state is pulled toward nearest basin.
        This is how habits constrain freedom.
        """
        nearest_idx, dist = self._find_nearest_attractor(self.psi_internal)
        
        if dist < 3.0:  # Within gravitational range
            attractor = self.attractors[nearest_idx]
            
            # Pull strength proportional to basin depth
            pull_strength = attractor["strength"] * 0.1
            
            # Stronger pull when exhausted (default to habits)
            pull_strength *= (1.0 + self.exhaustion)
            
            # Apply pull
            direction = attractor["state"] - self.psi_internal
            self.psi_internal += direction * pull_strength
            
            # Strengthen this attractor (the more you use it, the deeper it gets)
            attractor["strength"] = min(2.0, attractor["strength"] + 0.001)
    
    def _handle_refractory(self):
        """
        Refractory period: after intense activity, neurons need rest.
        During this time, learning is disabled, free will is reduced.
        """
        if self.refractory_timer > 0:
            self.refractory_timer -= 1
            self.exhaustion = self.refractory_timer / self.refractory_max
            
            # During refractory, default to strongest attractor (habits)
            if self.exhaustion > 0.7:
                strongest = max(self.attractors, key=lambda a: a["strength"])
                pull = strongest["state"] - self.psi_internal
                self.psi_internal += pull * 0.2  # Strong pull
        else:
            self.exhaustion = 0.0
    
    def _trigger_refractory(self):
        """
        Intense activity (high consciousness, high pain) â exhaustion
        """
        # High consciousness = intense projection
        intensity = abs(self.consciousness_value)
        
        # Pain amplifies exhaustion
        intensity += self.pain_level * 2.0
        
        # Random threshold with hysteresis
        if intensity > 2.0 and np.random.rand() < 0.05:
            self.refractory_timer = self.refractory_max
            # Lose some dopamine
            self.dopamine_level *= 0.7
    
    def _handle_trauma_reset(self, trauma_signal):
        """
        Seizure/trauma: reset internal state, lose recent memory.
        Like waking up in the ambulance: "what happened?"
        """
        if trauma_signal > 0.5:
            # Reset psi_internal (lose current thought)
            self.psi_internal = np.random.randn(self.w_size) * 0.1
            
            # Clear recent memory
            self.memory_buffer.clear()
            
            # Damage W slightly (some neural connections lost)
            noise = np.random.randn(self.w_size, self.w_size) * 0.05
            self.W += noise
            self.W = (self.W + self.W.T) / 2
            
            # Reset exhaustion
            self.refractory_timer = 0
            self.exhaustion = 0.0
            
            # Pain from confusion
            self.pain_level = 0.8
    
    def step(self):
        self.time += 1.0 / 30.0  # Assume 30 FPS
        self.loop_iterations += 1
        
        # === Get Inputs ===
        psi_external = self.get_blended_input('psi_external', 'sum') or 0.0
        pain_stimulus = self.get_blended_input('pain_stimulus', 'sum') or 0.0
        dopamine = self.get_blended_input('dopamine', 'sum')
        if dopamine is None:
            dopamine = 0.5 + 0.1 * np.sin(self.time * 0.5)  # Default oscillation
        trauma_signal = self.get_blended_input('reset_trauma', 'sum') or 0.0
        
        # === Handle Trauma/Seizure ===
        self._handle_trauma_reset(trauma_signal)
        
        # === Internal Dynamics ===
        # Natural drift (internal thoughts)
        self.psi_internal += np.random.randn(self.w_size) * 0.02
        
        # External influence (world affects internal state)
        # But only if paying attention (not exhausted)
        attention_strength = (1.0 - self.exhaustion) * 0.1
        self.psi_internal[0] += psi_external * attention_strength
        
        # === The Projection: Consciousness = W Â· Ï ===
        projection = self._project(self.psi_internal)
        self.consciousness_value = np.mean(projection)  # Scalar measure
        
        # === Memory Integration ===
        self.memory_buffer.append(self.consciousness_value)
        if len(self.memory_buffer) > 0:
            self.memory_trace = np.mean(list(self.memory_buffer))
        
        # === Pain ===
        # Pain from external stimulus
        self.pain_level = pain_stimulus * self.pain_sensitivity
        
        # Pain from clarity (entropy cost of self-awareness)
        self.clarity_cost = self._measure_clarity_cost()
        self.pain_level += self.clarity_cost * 0.1
        
        # Pain decays slowly
        self.pain_level *= 0.95
        self.pain_level = np.clip(self.pain_level, 0.0, 1.0)
        
        # === Attractor Dynamics ===
        self._pull_toward_attractor()
        
        # === Free Will Measurement ===
        self.free_will_signal = self._measure_free_will()
        
        # === The Strange Loop: W Modifies Itself ===
        self._learn_from_experience(psi_external, dopamine)
        
        # Create new attractors from novel patterns
        if self.loop_iterations % 100 == 0 and dopamine > 0.6:
            self._create_new_attractor()
        
        # === Refractory Period ===
        self._handle_refractory()
        self._trigger_refractory()
        
        # === Dopamine Dynamics ===
        # Slowly return to baseline
        self.dopamine_level = 0.9 * self.dopamine_level + 0.1 * dopamine
        self.dopamine_level = np.clip(self.dopamine_level, 0.0, 1.0)
        
        # === Normalize Internal State ===
        norm = np.linalg.norm(self.psi_internal)
        if norm > 5.0:
            self.psi_internal /= norm / 5.0
    
    def get_output(self, port_name):
        if port_name == 'consciousness':
            return self.consciousness_value
        
        elif port_name == 'free_will_signal':
            return self.free_will_signal
        
        elif port_name == 'pain_level':
            return self.pain_level
        
        elif port_name == 'attractor_state':
            return self._generate_w_visualization()
        
        elif port_name == 'memory_trace':
            return self.memory_trace
        
        elif port_name == 'refractory':
            return self.exhaustion
        
        return None
    
    def _generate_w_visualization(self):
        """
        Visualize the W matrix (your neural structure)
        """
        # Normalize W for display
        W_norm = self.W - self.W.min()
        W_norm /= (W_norm.max() + 1e-9)
        
        # Resize for visibility
        W_display = cv2.resize(W_norm.astype(np.float32), (64, 64), interpolation=cv2.INTER_NEAREST)
        
        return W_display
    
    def get_display_image(self):
        # Create a composite visualization
        h, w = 128, 128
        img = np.zeros((h, w, 3), dtype=np.uint8)
        
        # Background: W matrix structure
        W_vis = self._generate_w_visualization()
        W_vis_u8 = (W_vis * 255).astype(np.uint8)
        W_vis_color = cv2.applyColorMap(W_vis_u8, cv2.COLORMAP_VIRIDIS)
        W_vis_color = cv2.resize(W_vis_color, (w, h))
        img = W_vis_color
        
        # Overlay: Current attractor basin (white dots)
        for i, attr in enumerate(self.attractors):
            x = int((i / len(self.attractors)) * w)
            y = int(h - attr["strength"] * 30)
            color = (255, 255, 255) if i == self._find_nearest_attractor(self.psi_internal)[0] else (100, 100, 100)
            cv2.circle(img, (x, y), 3, color, -1)
        
        # Status text
        font = cv2.FONT_HERSHEY_SIMPLEX
        
        # Consciousness level
        cv2.putText(img, f"C: {self.consciousness_value:.2f}", (5, 15), font, 0.3, (255, 255, 255), 1)
        
        # Free will
        cv2.putText(img, f"FW: {self.free_will_signal:.2f}", (5, 30), font, 0.3, (0, 255, 0), 1)
        
        # Pain
        if self.pain_level > 0.3:
            cv2.putText(img, f"Pain: {self.pain_level:.2f}", (5, 45), font, 0.3, (0, 0, 255), 1)
        
        # Refractory indicator
        if self.refractory_timer > 0:
            cv2.putText(img, "REFRACTORY", (5, h-5), font, 0.3, (255, 100, 0), 1)
            # Progress bar
            bar_width = int((1.0 - self.exhaustion) * (w - 10))
            cv2.rectangle(img, (5, h-15), (5 + bar_width, h-10), (255, 100, 0), -1)
        
        img = np.ascontiguousarray(img)
        return QtGui.QImage(img.data, w, h, 3*w, QtGui.QImage.Format.Format_BGR888)
    
    def get_config_options(self):
        return [
            ("W Matrix Size", "w_size", self.w_size, None),
            ("Learning Rate", "learning_rate", self.learning_rate, None),
            ("Refractory Period", "refractory_max", self.refractory_max, None),
            ("Pain Sensitivity", "pain_sensitivity", self.pain_sensitivity, None),
        ]

=== FILE: iht_attractor_w.py ===

"""
IHT Attractor W-Matrix Node - The learned holographic decoder
Implements trainable complex linear mapping W that projects
high-dimensional quantum states onto stable classical attractors.

Place this file in the 'nodes' folder
"""

import numpy as np
from PyQt6 import QtGui
import cv2

import __main__
BaseNode = __main__.BaseNode
PA_INSTANCE = getattr(__main__, "PA_INSTANCE", None)

class IHTAttractorWNode(BaseNode):
    NODE_CATEGORY = "Transform"
    NODE_COLOR = QtGui.QColor(200, 50, 150)  # Magenta for attractor
    
    def __init__(self, hidden_dim=128, mapping_type='Learned'):
        super().__init__()
        self.node_title = "IHT W-Matrix"
        
        self.inputs = {
            'phase_field': 'image',     # Input quantum state
            'train_signal': 'signal'    # Trigger training steps
        }
        
        self.outputs = {
            'projected_field': 'image',     # W * Ï
            'attractor_image': 'image',     # Visualization of W structure
            'projection_quality': 'signal'   # How well it projects
        }
        
        self.hidden_dim = int(hidden_dim)
        self.mapping_type = mapping_type
        
        # The W matrix (complex)
        self.W = None
        self.last_input_shape = None
        
        # Training state
        self.training_mode = False
        self.learning_rate = 0.001
        self.loss_history = []
        
        # Outputs
        self.projected = None
        self.quality = 0.0
        
    def _init_W(self, input_size):
        """Initialize W matrix based on mapping type"""
        if self.mapping_type == 'Identity':
            # Baseline: just pass through
            self.W = np.eye(input_size, dtype=np.complex64)
            
        elif self.mapping_type == 'Random':
            # Random orthonormal (delocalized)
            real_part = np.random.randn(input_size, input_size)
            imag_part = np.random.randn(input_size, input_size)
            W_rand = real_part + 1j * imag_part
            
            # Orthonormalize via QR decomposition
            Q, R = np.linalg.qr(W_rand)
            self.W = Q.astype(np.complex64)
            
        elif self.mapping_type == 'Learned':
            # Start with identity + small noise
            self.W = np.eye(input_size, dtype=np.complex64)
            noise_scale = 0.01
            self.W += (np.random.randn(input_size, input_size) + 
                      1j * np.random.randn(input_size, input_size)) * noise_scale
            
    def _apply_W(self, psi_flat):
        """Apply W matrix to flattened complex field"""
        if self.W is None or self.W.shape[0] != len(psi_flat):
            self._init_W(len(psi_flat))
            
        return np.dot(self.W, psi_flat)
        
    def _compute_loss(self, psi_projected, psi_original):
        """Loss = negative coherence of projection"""
        # We want high coherence (phase alignment)
        coherence = np.abs(np.sum(psi_projected)) / (np.sum(np.abs(psi_projected)) + 1e-9)
        return -coherence  # Maximize coherence = minimize negative coherence
        
    def _gradient_step(self, psi_flat):
        """Simple gradient descent on W"""
        # Forward pass
        projected = self._apply_W(psi_flat)
        loss = self._compute_loss(projected, psi_flat)
        
        # Numerical gradient (finite differences)
        epsilon = 1e-5
        grad_W = np.zeros_like(self.W)
        
        # Only update a small random subset for speed
        n_samples = min(100, self.W.size)
        idx_i = np.random.randint(0, self.W.shape[0], n_samples)
        idx_j = np.random.randint(0, self.W.shape[1], n_samples)
        
        for i, j in zip(idx_i, idx_j):
            # Real part
            self.W[i, j] += epsilon
            proj_plus = self._apply_W(psi_flat)
            loss_plus = self._compute_loss(proj_plus, psi_flat)
            self.W[i, j] -= epsilon
            
            grad_W[i, j] = (loss_plus - loss) / epsilon
            
        # Update W
        self.W -= self.learning_rate * grad_W
        
        # Normalize rows to maintain stability
        for i in range(self.W.shape[0]):
            norm = np.linalg.norm(self.W[i, :])
            if norm > 1e-9:
                self.W[i, :] /= norm
                
        self.loss_history.append(float(loss))
        
    def step(self):
        phase_field = self.get_blended_input('phase_field', 'mean')
        train_signal = self.get_blended_input('train_signal', 'sum')
        
        if phase_field is None:
            return
            
        # Convert RGB phase field back to complex
        # (This is a simplification - in real use, we'd pass complex directly)
        if phase_field.ndim == 3:
            # Assume grayscale for now
            amp = np.mean(phase_field, axis=2)
        else:
            amp = phase_field
            
        h, w = amp.shape
        
        # Create complex field (amplitude only for now)
        psi_2d = amp.astype(np.complex64)
        psi_flat = psi_2d.flatten()
        
        # Training mode
        if train_signal is not None and train_signal > 0.5:
            self.training_mode = True
            self._gradient_step(psi_flat)
        else:
            self.training_mode = False
            
        # Apply W
        projected_flat = self._apply_W(psi_flat)
        self.projected = projected_flat.reshape(h, w)
        
        # Compute quality metric
        coherence = np.abs(np.sum(projected_flat)) / (np.sum(np.abs(projected_flat)) + 1e-9)
        self.quality = float(coherence)
        
    def get_output(self, port_name):
        if port_name == 'projected_field':
            if self.projected is None:
                return None
            # Return amplitude as image
            amp = np.abs(self.projected)
            amp_norm = amp / (amp.max() + 1e-9)
            return amp_norm.astype(np.float32)
            
        elif port_name == 'attractor_image':
            # Visualize W structure (first few rows)
            if self.W is None:
                return np.zeros((64, 64), dtype=np.float32)
                
            # Take a square subset
            n = min(64, self.W.shape[0])
            W_sub = self.W[:n, :n]
            
            # Show amplitude
            amp = np.abs(W_sub)
            amp_norm = amp / (amp.max() + 1e-9)
            return amp_norm.astype(np.float32)
            
        elif port_name == 'projection_quality':
            return self.quality
            
        return None
        
    def get_display_image(self):
        w_vis = self.get_output('attractor_image')
        if w_vis is None:
            return None
            
        img_u8 = (w_vis * 255).astype(np.uint8)
        
        # Add training indicator
        if self.training_mode:
            img_u8[:5, :] = 255  # White bar at top
            
        img_u8 = np.ascontiguousarray(img_u8)
        h, w = img_u8.shape
        return QtGui.QImage(img_u8.data, w, h, w, QtGui.QImage.Format.Format_Grayscale8)
        
    def get_config_options(self):
        return [
            ("Mapping Type", "mapping_type", self.mapping_type, [
                ("Identity (Baseline)", "Identity"),
                ("Random (Delocalized)", "Random"),
                ("Learned (Optimized)", "Learned")
            ]),
            ("Learning Rate", "learning_rate", self.learning_rate, None),
        ]

=== FILE: iht_phase_field.py ===

"""
IHT Phase Field Node - The fundamental quantum substrate
Implements complex Bloch-sphere cellular automaton with:
- Unitary evolution (Division/branching)
- Dissipative coupling (Dilution/decoherence)
- Attractor alignment

Place this file in the 'nodes' folder
"""

import numpy as np
from PyQt6 import QtGui
import cv2

import __main__
BaseNode = __main__.BaseNode
PA_INSTANCE = getattr(__main__, "PA_INSTANCE", None)

class IHTPhaseFieldNode(BaseNode):
    NODE_CATEGORY = "Source"
    NODE_COLOR = QtGui.QColor(100, 50, 200)  # Deep purple for quantum
    
    def __init__(self, grid_size=64):
        super().__init__()
        self.node_title = "IHT Phase Field"
        
        self.inputs = {
            'dilution': 'signal',      # Î³ parameter (0-1)
            'alignment': 'signal',      # Î· parameter for attractor
            'perturbation': 'image'     # External disturbance
        }
        
        self.outputs = {
            'phase_field': 'image',     # Complex field visualization
            'coherence': 'signal',      # Global phase coherence
            'constraint_density': 'image',  # Ï_C for gravity coupling
            'participation_ratio': 'signal'  # PR metric
        }
        
        self.N = int(grid_size)
        
        # Physics parameters
        self.alpha = 0.1  # Diffusion strength (division)
        self.gamma = 0.05  # Base dilution rate
        self.eta = 0.1     # Attractor alignment strength
        
        # Complex phase field (Bloch sphere states)
        self.psi = np.random.randn(self.N, self.N).astype(np.complex64)
        self.psi += 1j * np.random.randn(self.N, self.N).astype(np.complex64)
        
        # Normalize initially
        norm = np.sqrt(np.sum(np.abs(self.psi)**2))
        if norm > 1e-9:
            self.psi /= norm
            
        # Attractor state (will be learned or set)
        self.attractor = np.zeros_like(self.psi)
        self._init_simple_attractor()
        
        # Metrics
        self.coherence_value = 1.0
        self.constraint_density = np.zeros((self.N, self.N), dtype=np.float32)
        self.pr_value = 0.0
        
    def _init_simple_attractor(self):
        """Initialize a simple Gaussian attractor"""
        y, x = np.ogrid[-self.N//2:self.N//2, -self.N//2:self.N//2]
        r2 = x*x + y*y
        self.attractor = np.exp(-r2 / (2 * (self.N/8)**2)).astype(np.complex64)
        self.attractor /= np.sqrt(np.sum(np.abs(self.attractor)**2))
        
    def _unitary_step(self):
        """Division: Quantum branching via discrete Laplacian"""
        # FFT-based diffusion (periodic boundary)
        psi_fft = np.fft.fft2(self.psi)
        
        # Frequency coordinates
        kx = np.fft.fftfreq(self.N).reshape(-1, 1)
        ky = np.fft.fftfreq(self.N).reshape(1, -1)
        k2 = kx**2 + ky**2
        
        # Diffusion in Fourier space
        psi_fft *= np.exp(-self.alpha * k2)
        
        self.psi = np.fft.ifft2(psi_fft)
        
    def _dilution_step(self):
        """Dilution: Decoherence/normalization"""
        self.psi *= (1.0 - self.gamma)
        
    def _attractor_step(self):
        """Attractor alignment: Projection toward learned state"""
        # Spatial localization (Gaussian window around center)
        y, x = np.ogrid[-self.N//2:self.N//2, -self.N//2:self.N//2]
        r2 = x*x + y*y
        lambda_x = np.exp(-r2 / (2 * (self.N/4)**2))
        
        # Project toward attractor
        error = self.psi - self.attractor
        self.psi -= self.eta * lambda_x * error
        
    def _compute_metrics(self):
        """Compute coherence, PR, and constraint density"""
        # Global phase coherence
        total_amp = np.sum(np.abs(self.psi))
        phase_sum = np.sum(self.psi)
        self.coherence_value = np.abs(phase_sum) / (total_amp + 1e-9)
        
        # Participation Ratio
        amp2 = np.abs(self.psi)**2
        amp4 = amp2**2
        sum_amp2 = np.sum(amp2)
        sum_amp4 = np.sum(amp4)
        if sum_amp4 > 1e-12:
            self.pr_value = (sum_amp2**2) / sum_amp4
        else:
            self.pr_value = 0.0
            
        # Constraint density (where amplitude is localized)
        self.constraint_density = np.abs(self.psi)**2
        
    def step(self):
        # Get control parameters
        dilution_in = self.get_blended_input('dilution', 'sum')
        alignment_in = self.get_blended_input('alignment', 'sum')
        
        if dilution_in is not None:
            # Map from [-1,1] to [0, 0.2]
            self.gamma = np.clip((dilution_in + 1.0) / 2.0 * 0.2, 0.0, 0.2)
            
        if alignment_in is not None:
            # Map from [-1,1] to [0, 0.5]
            self.eta = np.clip((alignment_in + 1.0) / 2.0 * 0.5, 0.0, 0.5)
            
        # External perturbation
        perturb = self.get_blended_input('perturbation', 'mean')
        if perturb is not None:
            perturb_resized = cv2.resize(perturb, (self.N, self.N))
            # Add as phase modulation
            self.psi *= np.exp(1j * perturb_resized * np.pi)
            
        # Run physics steps
        self._unitary_step()
        self._dilution_step()
        self._attractor_step()
        
        # Periodic renormalization
        norm = np.sqrt(np.sum(np.abs(self.psi)**2))
        if norm > 1e-9:
            self.psi /= norm
            
        self._compute_metrics()
        
    def get_output(self, port_name):
        if port_name == 'phase_field':
            # Visualize as amplitude with phase hue
            amp = np.abs(self.psi)
            phase = np.angle(self.psi)
            
            # Normalize amplitude
            amp_norm = amp / (amp.max() + 1e-9)
            
            # Map phase to hue (0-180 for OpenCV HSV)
            hue = ((phase + np.pi) / (2*np.pi) * 180).astype(np.uint8)
            sat = (amp_norm * 255).astype(np.uint8)
            val = (amp_norm * 255).astype(np.uint8)
            
            hsv = np.stack([hue, sat, val], axis=-1)
            rgb = cv2.cvtColor(hsv, cv2.COLOR_HSV2RGB)
            
            return rgb.astype(np.float32) / 255.0
            
        elif port_name == 'coherence':
            return self.coherence_value
            
        elif port_name == 'constraint_density':
            return self.constraint_density
            
        elif port_name == 'participation_ratio':
            return self.pr_value
            
        return None
        
    def get_display_image(self):
        # Show phase field
        rgb_out = self.get_output('phase_field')
        if rgb_out is None:
            return None
            
        rgb_u8 = (rgb_out * 255).astype(np.uint8)
        
        # Add coherence bar at bottom
        bar_h = 5
        coherence_color = int(self.coherence_value * 255)
        rgb_u8[-bar_h:, :] = [coherence_color, coherence_color, 0]
        
        rgb_u8 = np.ascontiguousarray(rgb_u8)
        h, w = rgb_u8.shape[:2]
        return QtGui.QImage(rgb_u8.data, w, h, 3*w, QtGui.QImage.Format.Format_RGB888)
        
    def get_config_options(self):
        return [
            ("Grid Size", "N", self.N, None),
            ("Diffusion (Î±)", "alpha", self.alpha, None),
        ]

=== FILE: imageprocessornode.py ===

"""
Image Processor Node (FIXED)
--------------------
A simple utility node to adjust the brightness and contrast of an
incoming image stream.

- 'Brightness' adds or subtracts from all pixel values.
- 'Contrast' multiplies the pixel values relative to the midpoint (0.5).

FIX v2: This version preserves the input data type and dimensions 
(e.g., 2D float) for its 'image_out' port, which fixes compatibility
with nodes that expect a specific format (like Scalogram).

Place this file in the 'nodes' folder
"""

import numpy as np
import cv2

import __main__
BaseNode = __main__.BaseNode
PA_INSTANCE = getattr(__main__, "PA_INSTANCE", None)
QtGui = __main__.QtGui

if QtGui is None:
    print("CRITICAL: ImageProcessorNode could not import QtGui from host.")

class ImageProcessorNode(BaseNode):
    NODE_CATEGORY = "Filter"
    NODE_COLOR = QtGui.QColor(150, 150, 150)  # Neutral Gray
    
    def __init__(self, brightness=0.0, contrast=1.0):
        super().__init__()
        self.node_title = "Image Processor"
        
        self.inputs = {
            'image_in': 'image',
        }
        
        self.outputs = {
            'image_out': 'image',
            'brightness_signal': 'signal',
            'contrast_signal': 'signal'
        }
        
        if QtGui is None:
            self.node_title = "Image Processor (ERROR)"
            self._error = True
            return
        self._error = False
            
        # --- Configurable Parameters ---
        self.brightness = float(brightness)
        self.contrast = float(contrast)

        # --- Internal State ---
        self.processed_image = None # This will hold the format-preserved image
        self.display_in_rgb = np.zeros((64, 64, 3), dtype=np.uint8) # For "Before" display
        self.display_out_rgb = np.zeros((64, 64, 3), dtype=np.uint8) # For "After" display


    def step(self):
        if self._error: return
            
        # --- 1. Get Input Image ---
        img_in = self.get_blended_input('image_in', 'mean')
        
        if img_in is None:
            return
            
        # --- 2. Store original properties ---
        original_dtype = img_in.dtype
        
        # --- 3. Convert to Float (0.0 - 1.0) for processing ---
        if original_dtype == np.uint8:
            img_float = img_in.astype(np.float32) / 255.0
        else:
            # Assumes it's a float array (e.g., from CorticalReconstruction)
            img_float = img_in.astype(np.float32) 
            
        # --- 4. Apply Brightness & Contrast ---
        # Formula: new_val = (old_val - 0.5) * contrast + 0.5 + brightness
        
        # Apply contrast
        processed_float = (img_float - 0.5) * self.contrast + 0.5
        
        # Apply brightness
        processed_float = processed_float + (self.brightness / 100.0) # Brightness as -100 to 100
        
        # Clip values to valid 0.0 - 1.0 range
        np.clip(processed_float, 0.0, 1.0, out=processed_float)
        
        # --- 5. Convert back to original format for OUTPUT port ---
        if original_dtype == np.uint8:
            self.processed_image = (processed_float * 255).astype(np.uint8)
        else:
            # IMPORTANT: Keep it as float if it came in as float
            self.processed_image = processed_float.astype(original_dtype)
            
        # --- 6. Create separate uint8 RGB versions for DISPLAY ---
        
        # Create "Before" display
        if img_float.ndim == 2:
            before_u8 = (np.clip(img_float, 0, 1) * 255).astype(np.uint8)
            self.display_in_rgb = cv2.cvtColor(before_u8, cv2.COLOR_GRAY2RGB)
        elif img_float.shape[2] == 3:
            before_u8 = (np.clip(img_float, 0, 1) * 255).astype(np.uint8)
            self.display_in_rgb = before_u8
        
        # Create "After" display
        if processed_float.ndim == 2:
            after_u8 = (np.clip(processed_float, 0, 1) * 255).astype(np.uint8)
            self.display_out_rgb = cv2.cvtColor(after_u8, cv2.COLOR_GRAY2RGB)
        elif processed_float.shape[2] == 3:
            after_u8 = (np.clip(processed_float, 0, 1) * 255).astype(np.uint8)
            self.display_out_rgb = after_u8
        
        
    def get_output(self, port_name):
        if self._error: return None
        if port_name == 'image_out':
            return self.processed_image
        elif port_name == 'brightness_signal':
            return self.brightness
        elif port_name == 'contrast_signal':
            return self.contrast
        return None

    def get_display_image(self):
        if self._error: return None
        if self.processed_image is None: return None

        # Create a side-by-side "Before" and "After"
        display_h = 128
        display_w = 256
        display = np.zeros((display_h, display_w, 3), dtype=np.uint8)
        
        # --- Left side: "Before" (Input) ---
        before_resized = cv2.resize(self.display_in_rgb, (display_h, display_h), interpolation=cv2.INTER_NEAREST)
        display[:, :display_h] = before_resized
        
        # --- Right side: "After" (Processed Output) ---
        after_resized = cv2.resize(self.display_out_rgb, (display_h, display_h), interpolation=cv2.INTER_NEAREST)
        display[:, display_w-display_h:] = after_resized
        
        # Add dividing line
        display[:, display_h-1:display_h+1] = [255, 255, 255]
        
        # Add labels
        font = cv2.FONT_HERSHEY_SIMPLEX
        cv2.putText(display, 'IN', (10, 15), font, 0.4, (255, 255, 255), 1, cv2.LINE_AA)
        cv2.putText(display, 'OUT', (display_h + 10, 15), font, 0.4, (255, 255, 255), 1, cv2.LINE_AA)

        # Add current values
        b_text = f"B: {self.brightness:.1f}"
        c_text = f"C: {self.contrast:.2f}"
        cv2.putText(display, b_text, (10, display_h - 10), font, 0.4, (200, 200, 200), 1, cv2.LINE_AA)
        cv2.putText(display, c_text, (display_h + 10, display_h - 10), font, 0.4, (200, 200, 200), 1, cv2.LINE_AA)
        
        display = np.ascontiguousarray(display)
        return QtGui.QImage(display.data, display_w, display_h, 3*display_w, QtGui.QImage.Format.Format_RGB888)

    def get_config_options(self):
        # Config options: [("Display Name", "variable_name", current_value, options_list)]
        # For sliders, options_list is None
        return [
            ("Brightness", "brightness", self.brightness, None),
            ("Contrast", "contrast", self.contrast, None),
        ]

=== FILE: img2moire.py ===

"""
Antti's Image-to-MoirÃ© Node
Applies a signal-controlled band-pass filter in the frequency domain
to isolate specific spatial frequencies, creating MoirÃ©-like patterns.
Inspired by the FFT->filter->IFFT logic in sigh_image.py.
Place this file in the 'nodes' folder
"""

import numpy as np
from PyQt6 import QtGui
import cv2

import sys
import os
# --- This is the new, correct block ---
import __main__
BaseNode = __main__.BaseNode
PA_INSTANCE = getattr(__main__, "PA_INSTANCE", None)
# ------------------------------------

try:
    from scipy.fft import fft2, ifft2, fftshift, fftfreq
    SCIPY_FFT_AVAILABLE = True
except ImportError:
    SCIPY_FFT_AVAILABLE = False
    print("Warning: ImageMoireNode requires 'scipy'.")
    print("Please run: pip install scipy")

class ImageMoireNode(BaseNode):
    NODE_CATEGORY = "Transform"
    NODE_COLOR = QtGui.QColor(220, 120, 40) # Transform Orange
    
    def __init__(self, resolution=128):
        super().__init__()
        self.node_title = "Image to MoirÃ©"
        
        self.inputs = {
            'image': 'image',
            'peak_freq': 'signal',  # Controls center of frequency band (0 to 1)
            'bandwidth': 'signal' # Controls width of frequency band (0 to 1)
        }
        self.outputs = {'image': 'image'}
        
        self.resolution = int(resolution)
        self.peak_freq = 0.1  # Default peak frequency
        self.bandwidth = 0.1  # Default bandwidth
        
        self.output_image = np.zeros((self.resolution, self.resolution), dtype=np.float32)
        
        # Pre-calculate the frequency grid
        self._k_magnitude = self._create_frequency_grid(self.resolution)
        
        if not SCIPY_FFT_AVAILABLE:
            self.node_title = "MoirÃ© (No SciPy!)"

    def _create_frequency_grid(self, n):
        """Creates a centered grid of frequency magnitudes."""
        freq_x = fftshift(fftfreq(n))
        freq_y = fftshift(fftfreq(n))
        fx, fy = np.meshgrid(freq_x, freq_y)
        k_magnitude = np.sqrt(fx**2 + fy**2)
        # Normalize from [0, 0.707] to [0, 1]
        return k_magnitude / 0.707

    def step(self):
        if not SCIPY_FFT_AVAILABLE:
            return

        input_img = self.get_blended_input('image', 'mean')
        
        # Get control signals, mapping from [-1, 1] to [0, 1]
        peak_signal = self.get_blended_input('peak_freq', 'sum')
        bw_signal = self.get_blended_input('bandwidth', 'sum')
        
        # Use signal if connected, else use internal config
        # Map signal from [-1, 1] to [0, 1], or use config [0, 1]
        peak = (peak_signal + 1.0) / 2.0 if peak_signal is not None else self.peak_freq
        bw = (bw_signal + 1.0) / 2.0 if bw_signal is not None else self.bandwidth
        
        if input_img is None:
            self.output_image *= 0.95 # Fade to black
            return
            
        try:
            # Resize image to target resolution
            img_resized = cv2.resize(input_img, (self.resolution, self.resolution),
                                     interpolation=cv2.INTER_AREA)
            
            # --- 1. FFT ---
            field_fft = fftshift(fft2(img_resized))
            
            # --- 2. Create Filter Mask ---
            # Map bandwidth from [0, 1] to a small, usable range
            bw_scaled = bw * 0.05 + 0.005 # e.g., 0.005 to 0.055
            
            # Create a Gaussian ring (band-pass filter)
            distance_from_peak = np.abs(self._k_magnitude - peak)
            filter_mask = np.exp(-(distance_from_peak**2) / (2 * bw_scaled**2))
            
            # --- 3. Apply Filter ---
            filtered_fft = field_fft * filter_mask
            
            # --- 4. IFFT ---
            result = ifft2(filtered_fft) # Already shifted
            result_real = np.abs(result) # Use magnitude
            
            # Normalize for output
            r_min, r_max = result_real.min(), result_real.max()
            if (r_max - r_min) > 1e-9:
                self.output_image = (result_real - r_min) / (r_max - r_min)
            else:
                self.output_image.fill(0.0)
                                           
        except Exception as e:
            print(f"Image MoirÃ© Error: {e}")
            self.output_image *= 0.95

    def get_output(self, port_name):
        if port_name == 'image':
            return self.output_image
        return None
        
    def get_display_image(self):
        img_u8 = (np.clip(self.output_image, 0, 1) * 255).astype(np.uint8)
        img_u8 = np.ascontiguousarray(img_u8)
        return QtGui.QImage(img_u8.data, self.resolution, self.resolution, self.resolution, QtGui.QImage.Format.Format_Grayscale8)

    def get_config_options(self):
        return [
            ("Resolution", "resolution", self.resolution, None),
            ("Peak Freq (0-1)", "peak_freq", self.peak_freq, None),
            ("Bandwidth (0-1)", "bandwidth", self.bandwidth, None),
        ]

=== FILE: instantonfieldnode.py ===

"""
InstantonFieldNode

Simulates a continuous field dynamic based on the "action integral"
S[Ï] = â« dâ´x [Â½(âÎ¼Ï)Â² + V(Ï)]. It accumulates a field 'Ï' based
on an input potential 'V(Ï)' and a beta-field catalyst.
"""

import numpy as np
import cv2

# --- Magic import block ---
import __main__
BaseNode = __main__.BaseNode
QtGui = __main__.QtGui
# --------------------------

class InstantonFieldNode(BaseNode):
    """
    Generates 'instantons' by accumulating a field in a potential.
    """
    NODE_CATEGORY = "Generator"
    NODE_COLOR = QtGui.QColor(100, 200, 250) # Sky Blue

    def __init__(self, size=128):
        super().__init__()
        self.node_title = "Instanton Field"
        
        self.inputs = {
            'potential_in': 'image', # V(Ï) - The landscape
            'beta_field': 'image',   # Î²-parameter field (catalyst)
            'diffusion': 'signal',   # (âÎ¼Ï)Â² - Smoothing/kinetic term
            'decay': 'signal'        # 0-1, how fast the field fades
        }
        self.outputs = {
            'field_out': 'image',      # The raw, continuous field Ï
            'instanton_viz': 'image'   # Thresholded "instantons"
        }
        
        self.size = int(size)
        
        # The field Ï, initialized as float32 for safety
        self.field = np.zeros((self.size, self.size), dtype=np.float32)

    def _prepare_image(self, img, default_val=0.0):
        """Helper to resize, format, and handle missing images."""
        if img is None:
            return np.full((self.size, self.size), default_val, dtype=np.float32)
        
        # Ensure float32 in 0-1 range
        if img.dtype != np.float32:
            img = img.astype(np.float32)
        if img.max() > 1.0:
            img = img / 255.0
            
        img_resized = cv2.resize(img, (self.size, self.size), 
                                 interpolation=cv2.INTER_LINEAR)
        
        if img_resized.ndim == 3:
            img_gray = cv2.cvtColor(img_resized, cv2.COLOR_RGB2GRAY)
        else:
            img_gray = img_resized
            
        return np.clip(img_gray, 0, 1)

    def step(self):
        # --- 1. Get Inputs ---
        
        # [FIX 1: Logic Error] Use 'potential_in', not 'image_in'
        potential = 1.0 - self._prepare_image(
            self.get_blended_input('potential_in', 'first'), 
            default_val=1.0
        )
        
        beta_field = self._prepare_image(
            self.get_blended_input('beta_field', 'first'), 
            default_val=1.0
        )
        
        # Get standard Python floats (which are 64-bit)
        diffusion = self.get_blended_input('diffusion', 'sum') or 0.1
        decay = self.get_blended_input('decay', 'sum') or 0.05
        
        # --- 2. Simulate the Field ---
        
        # [FIX 2: Crash Fix]
        # Force self.field to be float32 *before* passing to OpenCV.
        # This fixes the crash if it was upcast to float64 on the previous frame.
        laplacian = cv2.Laplacian(self.field.astype(np.float32), cv2.CV_32F, ksize=3)
        
        # S[Ï] = â« dâ´x [Â½(âÎ¼Ï)Â² + V(Ï)]
        # All math here will be upcast to float64, which is fine
        new_field = (self.field * (1.0 - np.clip(decay, 0, 1))) + \
                     (laplacian * np.clip(diffusion, 0, 1)) + \
                     (potential * beta_field * 0.1) # 0.1 is a 'learning rate'
                     
        # Clamp to prevent runaway values
        new_field = np.clip(new_field, 0, 1)
        
        # [FIX 3: Prevent Future Crashes]
        # Store the result as float32, so it's correct for the *next* frame
        self.field = new_field.astype(np.float32)

    def get_output(self, port_name):
        if port_name == 'field_out':
            return self.field # Return the raw 0-1 float field
            
        elif port_name == 'instanton_viz':
            # Threshold the field to see the "instantons"
            _ , binary = cv2.threshold(self.field, 0.5, 1.0, cv2.THRESH_BINARY)
            
            # Apply colormap to make it look cool
            img_u8 = (binary * 255).astype(np.uint8)
            img_color = cv2.applyColorMap(img_u8, cv2.COLORMAP_MAGMA)
            return img_color.astype(np.float32) / 255.0
            
        return None

    def get_display_image(self):
        # By default, display the 'instanton_viz' output
        return self.get_output('instanton_viz')

=== FILE: instantontrainnode.py ===

"""
Instanton Train Node - Simulates topological solitons and quantum tunneling events
Models instantons as localized spacetime events that mediate vacuum transitions.

Based on instanton theory from QFT:
- Instantons are classical solutions to equations of motion in imaginary time
- They represent tunneling events between different vacuum states
- Have finite action and create a "train" of events in spacetime

Place this file in the 'nodes' folder
Requires: pip install scipy
"""

import numpy as np
from PyQt6 import QtGui
import cv2
import time

import __main__
BaseNode = __main__.BaseNode
PA_INSTANCE = getattr(__main__, "PA_INSTANCE", None)

try:
    from scipy.ndimage import gaussian_filter
    SCIPY_AVAILABLE = True
except ImportError:
    SCIPY_AVAILABLE = False
    print("Warning: InstantonTrainNode requires 'scipy'.")


class Instanton:
    """
    Represents a single instanton event - a localized spacetime bubble.
    """
    def __init__(self, position, size, strength, vacuum_state):
        self.position = np.array(position, dtype=np.float32)  # (x, y, t)
        self.size = float(size)  # Instanton radius
        self.strength = float(strength)  # Action/coupling strength
        self.vacuum_state = int(vacuum_state)  # Which vacuum (0 or 1)
        self.age = 0.0
        self.lifetime = np.random.uniform(10, 30)  # How long it persists
        self.velocity = np.random.randn(2) * 0.1  # Drift velocity
        
    def profile(self, x, y):
        """
        Calculate instanton profile at position (x, y).
        Uses the standard instanton solution profile.
        """
        dx = x - self.position[0]
        dy = y - self.position[1]
        r_squared = dx**2 + dy**2
        
        # Standard instanton profile: ÏÂ² / (rÂ² + ÏÂ²)
        # where Ï is the instanton size
        rho_squared = self.size**2
        profile = rho_squared / (r_squared + rho_squared)
        
        # Modulate by age (fade in/out)
        age_factor = 1.0
        if self.age < 5:
            age_factor = self.age / 5.0  # Fade in
        elif self.age > self.lifetime - 5:
            age_factor = (self.lifetime - self.age) / 5.0  # Fade out
        
        return profile * self.strength * age_factor
    
    def update(self, dt, grid_size):
        """Update instanton position and age."""
        self.age += dt
        
        # Drift in spacetime
        self.position[0] += self.velocity[0] * dt
        self.position[1] += self.velocity[1] * dt
        
        # Wrap around boundaries
        self.position[0] %= grid_size[0]
        self.position[1] %= grid_size[1]
        
        return self.age < self.lifetime  # Return True if still alive


class InstantonTrainNode(BaseNode):
    NODE_CATEGORY = "Source"
    NODE_COLOR = QtGui.QColor(150, 50, 150)  # Deep purple for quantum
    
    def __init__(self, grid_size=96, max_instantons=20):
        super().__init__()
        self.node_title = "Instanton Train"
        
        self.inputs = {
            'tunneling_rate': 'signal',      # Controls spawn rate
            'coupling_strength': 'signal',    # Controls instanton strength
            'vacuum_bias': 'signal',          # Bias toward vacuum 0 or 1
            'perturbation': 'image',          # External field perturbation
            'reset': 'signal'
        }
        
        self.outputs = {
            'vacuum_field': 'image',          # Current vacuum state field
            'action_density': 'image',        # Topological action density
            'tunneling_events': 'signal',     # Number of active instantons
            'winding_number': 'signal',       # Topological charge
            'vacuum_0_density': 'signal',     # Density in vacuum 0
            'vacuum_1_density': 'signal',     # Density in vacuum 1
            'average_action': 'signal'        # Average instanton action
        }
        
        if not SCIPY_AVAILABLE:
            self.node_title = "Instanton (No SciPy!)"
            return
        
        # Grid parameters
        self.grid_size = (int(grid_size), int(grid_size))
        self.max_instantons = int(max_instantons)
        
        # Physical parameters
        self.tunneling_rate = 0.1  # Base rate of instanton creation
        self.coupling_strength = 1.0
        self.vacuum_bias = 0.0  # -1 to 1, bias toward vacuum 0 or 1
        
        # State fields
        self.vacuum_field = np.zeros(self.grid_size, dtype=np.float32)  # -1 to 1
        self.action_density = np.zeros(self.grid_size, dtype=np.float32)
        
        # Instanton collection
        self.instantons = []
        
        # Metrics
        self.winding_number = 0.0
        self.vacuum_0_density = 0.5
        self.vacuum_1_density = 0.5
        self.average_action = 0.0
        
        # Time tracking
        self.time = 0.0
        self.last_spawn_time = 0.0
        self.dt = 0.1
        
        # Initialize with random vacuum configuration
        self._initialize_vacuum()
    
    def _initialize_vacuum(self):
        """Initialize the vacuum field with a smooth random configuration."""
        # Start with random noise
        noise = np.random.randn(*self.grid_size)
        # Smooth it to create domain structure
        self.vacuum_field = gaussian_filter(noise, sigma=5.0)
        # Normalize to [-1, 1]
        vmin, vmax = self.vacuum_field.min(), self.vacuum_field.max()
        if vmax > vmin:
            self.vacuum_field = 2.0 * (self.vacuum_field - vmin) / (vmax - vmin) - 1.0
    
    def _spawn_instanton(self):
        """Create a new instanton event."""
        if len(self.instantons) >= self.max_instantons:
            return
        
        # Random position
        position = np.array([
            np.random.uniform(0, self.grid_size[0]),
            np.random.uniform(0, self.grid_size[1]),
            self.time
        ])
        
        # Size varies (smaller = more localized, higher action)
        size = np.random.uniform(3.0, 8.0)
        
        # Strength proportional to coupling
        strength = self.coupling_strength * np.random.uniform(0.8, 1.2)
        
        # Vacuum state based on bias
        if np.random.random() < (self.vacuum_bias + 1.0) / 2.0:
            vacuum_state = 1
        else:
            vacuum_state = 0
        
        instanton = Instanton(position, size, strength, vacuum_state)
        self.instantons.append(instanton)
    
    def _update_instantons(self):
        """Update all instantons and remove dead ones."""
        alive_instantons = []
        
        for inst in self.instantons:
            if inst.update(self.dt, self.grid_size):
                alive_instantons.append(inst)
        
        self.instantons = alive_instantons
    
    def _compute_vacuum_field(self):
        """Compute the vacuum field from all active instantons."""
        # Start with the base field (slowly decays toward zero)
        self.vacuum_field *= 0.99
        
        # Add bias drift
        self.vacuum_field += self.vacuum_bias * 0.01
        
        # Create coordinate grids
        x = np.arange(self.grid_size[0])
        y = np.arange(self.grid_size[1])
        X, Y = np.meshgrid(x, y, indexing='ij')
        
        # Add contribution from each instanton
        for inst in self.instantons:
            profile = inst.profile(X, Y)
            
            # Instantons flip the vacuum locally
            if inst.vacuum_state == 1:
                self.vacuum_field += profile
            else:
                self.vacuum_field -= profile
        
        # Clamp to valid range
        self.vacuum_field = np.clip(self.vacuum_field, -1.0, 1.0)
    
    def _compute_action_density(self):
        """
        Compute the action density (topological charge density).
        This measures local field gradients - where tunneling is occurring.
        """
        # Calculate gradient magnitude
        grad_x = np.roll(self.vacuum_field, -1, axis=0) - np.roll(self.vacuum_field, 1, axis=0)
        grad_y = np.roll(self.vacuum_field, -1, axis=1) - np.roll(self.vacuum_field, 1, axis=1)
        
        # Action density ~ gradient squared (kinetic term)
        self.action_density = grad_x**2 + grad_y**2
        
        # Add potential term (double-well potential)
        # V(Ï) = (ÏÂ² - 1)Â² has minima at Ï = Â±1 (two vacua)
        potential = (self.vacuum_field**2 - 1.0)**2
        self.action_density += potential * 0.5
        
        # Smooth for visualization
        self.action_density = gaussian_filter(self.action_density, sigma=1.0)
    
    def _compute_winding_number(self):
        """
        Compute topological winding number (topological charge).
        This counts the net number of vacuum transitions.
        """
        # Simple approximation: count domain walls
        # A domain wall is where the field crosses zero
        zero_crossings_x = np.sum(self.vacuum_field[:-1, :] * self.vacuum_field[1:, :] < 0)
        zero_crossings_y = np.sum(self.vacuum_field[:, :-1] * self.vacuum_field[:, 1:] < 0)
        
        # Winding number is proportional to number of crossings
        self.winding_number = (zero_crossings_x + zero_crossings_y) / 100.0
    
    def _compute_vacuum_densities(self):
        """Calculate the fraction of space in each vacuum."""
        # Vacuum 0 is where field < 0, Vacuum 1 is where field > 0
        self.vacuum_0_density = np.sum(self.vacuum_field < 0) / self.vacuum_field.size
        self.vacuum_1_density = np.sum(self.vacuum_field > 0) / self.vacuum_field.size
    
    def _compute_average_action(self):
        """Calculate average instanton action."""
        if len(self.instantons) > 0:
            total_action = sum(inst.strength * (inst.size**2) for inst in self.instantons)
            self.average_action = total_action / len(self.instantons)
        else:
            self.average_action = 0.0
    
    def step(self):
        if not SCIPY_AVAILABLE:
            return
        
        # Get control inputs
        tunneling_in = self.get_blended_input('tunneling_rate', 'sum')
        coupling_in = self.get_blended_input('coupling_strength', 'sum')
        bias_in = self.get_blended_input('vacuum_bias', 'sum')
        perturbation = self.get_blended_input('perturbation', 'mean')
        reset_sig = self.get_blended_input('reset', 'sum')
        
        # Handle reset
        if reset_sig is not None and reset_sig > 0.5:
            self._reset()
            return
        
        # Update parameters from inputs
        if tunneling_in is not None:
            # Map [-1, 1] to [0, 0.5]
            self.tunneling_rate = (tunneling_in + 1.0) / 2.0 * 0.5
        
        if coupling_in is not None:
            # Map [-1, 1] to [0.5, 2.0]
            self.coupling_strength = 0.5 + (coupling_in + 1.0) / 2.0 * 1.5
        
        if bias_in is not None:
            # Direct mapping [-1, 1]
            self.vacuum_bias = np.clip(bias_in, -1.0, 1.0)
        
        # Apply external perturbation
        if perturbation is not None:
            perturb_resized = cv2.resize(perturbation, 
                                        (self.grid_size[1], self.grid_size[0]),
                                        interpolation=cv2.INTER_AREA)
            # Perturbation nudges the vacuum field
            self.vacuum_field += (perturb_resized - 0.5) * 0.1
            self.vacuum_field = np.clip(self.vacuum_field, -1.0, 1.0)
        
        # Decide whether to spawn a new instanton
        spawn_probability = self.tunneling_rate * self.dt
        if np.random.random() < spawn_probability:
            self._spawn_instanton()
        
        # Update all instantons
        self._update_instantons()
        
        # Compute the vacuum field
        self._compute_vacuum_field()
        
        # Compute action density
        self._compute_action_density()
        
        # Compute metrics
        self._compute_winding_number()
        self._compute_vacuum_densities()
        self._compute_average_action()
        
        # Advance time
        self.time += self.dt
    
    def _reset(self):
        """Reset the simulation."""
        self.instantons = []
        self._initialize_vacuum()
        self.action_density = np.zeros(self.grid_size, dtype=np.float32)
        self.time = 0.0
        self.winding_number = 0.0
    
    def get_output(self, port_name):
        if port_name == 'vacuum_field':
            # Normalize to [0, 1] for output
            return (self.vacuum_field + 1.0) / 2.0
        
        elif port_name == 'action_density':
            # Normalize action density
            if self.action_density.max() > 1e-9:
                return self.action_density / self.action_density.max()
            return self.action_density
        
        elif port_name == 'tunneling_events':
            return float(len(self.instantons))
        
        elif port_name == 'winding_number':
            return self.winding_number
        
        elif port_name == 'vacuum_0_density':
            return self.vacuum_0_density
        
        elif port_name == 'vacuum_1_density':
            return self.vacuum_1_density
        
        elif port_name == 'average_action':
            return self.average_action
        
        return None
    
    def get_display_image(self):
        # Create RGB visualization
        img = np.zeros((*self.grid_size, 3), dtype=np.float32)
        
        # Red channel: Vacuum 1 regions (positive field)
        img[:, :, 0] = np.clip(self.vacuum_field, 0, 1)
        
        # Blue channel: Vacuum 0 regions (negative field)
        img[:, :, 2] = np.clip(-self.vacuum_field, 0, 1)
        
        # Green channel: Action density (tunneling events)
        action_norm = self.action_density / (self.action_density.max() + 1e-9)
        img[:, :, 1] = action_norm * 0.8
        
        # Draw instanton centers
        for inst in self.instantons:
            x, y = int(inst.position[0]), int(inst.position[1])
            if 0 <= x < self.grid_size[0] and 0 <= y < self.grid_size[1]:
                # Bright spot at instanton center
                size = max(1, int(inst.size / 2))
                x_min, x_max = max(0, x-size), min(self.grid_size[0], x+size)
                y_min, y_max = max(0, y-size), min(self.grid_size[1], y+size)
                
                if inst.vacuum_state == 1:
                    img[x_min:x_max, y_min:y_max, 0] = 1.0  # Red for vacuum 1
                else:
                    img[x_min:x_max, y_min:y_max, 2] = 1.0  # Blue for vacuum 0
        
        # Convert to uint8
        img_u8 = (np.clip(img, 0, 1) * 255).astype(np.uint8)
        
        # Resize to thumbnail
        img_resized = cv2.resize(img_u8, (96, 96), interpolation=cv2.INTER_LINEAR)
        img_resized = np.ascontiguousarray(img_resized)
        
        h, w = img_resized.shape[:2]
        return QtGui.QImage(img_resized.data, w, h, 3*w, QtGui.QImage.Format.Format_RGB888)
    
    def get_config_options(self):
        return [
            ("Grid Size", "grid_size", self.grid_size[0], None),
            ("Max Instantons", "max_instantons", self.max_instantons, None),
        ]

=== FILE: interactivesignalnode.py ===

"""
Interactive Signal Node - Outputs a value that can be changed
with on-screen + and - buttons.

Place this file in the 'nodes' folder
"""

import numpy as np
from PyQt6 import QtGui
from PIL import Image, ImageDraw, ImageFont
import os

import __main__
BaseNode = __main__.BaseNode
PA_INSTANCE = getattr(__main__, "PA_INSTANCE", None)

class InteractiveSignalNode(BaseNode):
    NODE_CATEGORY = "Source"
    NODE_COLOR = QtGui.QColor(40, 120, 80) # Source Green
    
    def __init__(self, value=1.0):
        super().__init__()
        self.node_title = "Interactive Signal"
        self.outputs = {'signal': 'signal'}
        
        # This attribute MUST be named 'zoom_factor'
        # for the host (perception_lab_host.py) to draw the +/ - buttons.
        self.zoom_factor = float(value)
        
        # Try to load a font for display
        try:
            self.font = ImageFont.load_default(size=14)
        except IOError:
            self.font = None

    def step(self):
        # The host application modifies self.zoom_factor directly
        # when the +/ - buttons are clicked.
        pass
        
    def get_output(self, port_name):
        if port_name == 'signal':
            # Output the current value
            return self.zoom_factor
        return None
        
    def get_display_image(self):
        w, h = 64, 32  # Small and wide
        img = np.zeros((h, w, 3), dtype=np.uint8)
        img_pil = Image.fromarray(img)
        draw = ImageDraw.Draw(img_pil)
        
        # Display the current value
        text = f"{self.zoom_factor:.3f}"
        
        if self.zoom_factor > 1.0:
            text_color = (100, 255, 100) # Green
        elif self.zoom_factor < 1.0:
            text_color = (255, 100, 100) # Red
        else:
            text_color = (200, 200, 200) # Gray
        
        try:
            bbox = draw.textbbox((0, 0), text, font=self.font)
            text_w = bbox[2] - bbox[0]
            text_h = bbox[3] - bbox[1]
            x = (w - text_w) / 2
            y = (h - text_h) / 2
        except Exception:
            x, y = 5, 5 # Fallback
            
        draw.text((x, y), text, fill=text_color, font=self.font)
        
        img_final = np.array(img_pil)
        img_final = np.ascontiguousarray(img_final)
        return QtGui.QImage(img_final.data, w, h, 3*w, QtGui.QImage.Format.Format_RGB888)

    def get_config_options(self):
        # Allow setting the initial value
        return [
            ("Initial Value", "zoom_factor", self.zoom_factor, None)
        ]

=== FILE: largemoirefield.py ===

"""
Large Moire Field Node - The "Eye" and "V1" of the Attentional Field Computer.
Encodes a webcam image into a single-channel "fast field" using a 
convolutional network and holographic (wave) evolution.

This is a simplified version of SensoryEncoderNode, focused only on 
generating the visual field and motion signal, without X/Y tracking.

Ported from afc6.py
Requires: pip install torch numpy
"""

import numpy as np
from PyQt6 import QtGui
import cv2
import sys
import os
import time 

import __main__
BaseNode = __main__.BaseNode
PA_INSTANCE = getattr(__main__, "PA_INSTANCE", None)
QtGui = __main__.QtGui

# --- Dependency Check ---
try:
    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False
    print("Warning: LargeMoireFieldNode requires 'torch'.")
    print("Please run: pip install torch")

# Use GPU if available
try:
    DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    TORCH_DTYPE = torch.float16 if DEVICE.type == "cuda" else torch.float32
except Exception:
    DEVICE = torch.device("cpu")
    TORCH_DTYPE = torch.float32

# --- Core Architectural Components (from afc6.py) ---

class HolographicField(nn.Module):
    """A field that evolves based on wave dynamics. (afc6.py)"""
    def __init__(self, dimensions=(64, 64), num_channels=1):
        super().__init__()
        self.dimensions = dimensions
        self.damping_map = nn.Parameter(torch.full((1, num_channels, *dimensions), 0.02, dtype=torch.float32))
        
        k_freq = [torch.fft.fftfreq(n, d=1 / n) for n in dimensions]
        k_grid = torch.meshgrid(*k_freq, indexing='ij')
        k2 = sum(k ** 2 for k in k_grid)
        self.register_buffer('k2', k2)

    def evolve(self, field_state, steps=1):
        """Evolve the field state using spectral methods."""
        field_fft = torch.fft.fft2(field_state)
        decay = torch.exp(-self.k2.unsqueeze(0).unsqueeze(0) * F.softplus(self.damping_map))
        for _ in range(steps):
            field_fft *= decay
        return torch.fft.ifft2(field_fft).real

class SensoryEncoder(nn.Module):
    """The 'Eye' and 'V1'. Encodes images to a single-channel fast field. (afc6.py)"""
    def __init__(self, field_dims=(64, 64)):
        super().__init__()
        self.field = HolographicField(field_dims, num_channels=1)
        self.image_to_drive = nn.Sequential(
            nn.Conv2d(3, 16, 5, stride=2, padding=2), nn.GELU(),
            nn.Conv2d(16, 1, 3, padding=1),
            nn.AdaptiveAvgPool2d(field_dims)
        )
        self.gamma_freq = 7.5
        self.receptive_threshold = 0.0

    def get_gamma_phase(self):
        return (time.time() * self.gamma_freq * 2 * np.pi) % (2 * np.pi)

    def is_receptive_phase(self, phase):
        return np.cos(phase) > self.receptive_threshold

    def forward(self, image_tensor):
        drive_pattern = self.image_to_drive(image_tensor)
        fast_pattern = self.field.evolve(drive_pattern, steps=5)
        phase = self.get_gamma_phase()
        receptive = self.is_receptive_phase(phase)
        return fast_pattern, phase, receptive

# --- The Main Node Class ---

class LargeMoireFieldNode(BaseNode):
    NODE_CATEGORY = "Transform"
    NODE_COLOR = QtGui.QColor(100, 50, 200) # Deep purple
    
    def __init__(self, size=64):
        super().__init__()
        self.node_title = "Large Moire Field"
        
        self.inputs = {'image_in': 'image'}
        self.outputs = {
            'fast_field': 'image',    # The 64x64 evolved pattern
            'motion_signal': 'signal',# A signal representing change/motion
            'gamma_phase': 'signal',  # The internal clock signal
            'is_receptive': 'signal'  # The 1.0/0.0 gate signal
        }
        
        if not TORCH_AVAILABLE:
            self.node_title = "Moire Field (No Torch!)"
            return
            
        self.size = int(size)
        
        # 1. Initialize the PyTorch model
        self.model = SensoryEncoder(field_dims=(self.size, self.size)).to(DEVICE)
        self.model.eval() # Set to evaluation mode
        
        # 2. Internal state
        self.fast_field_data = np.zeros((self.size, self.size), dtype=np.float32)
        self.last_fast_field = torch.zeros(1, 1, self.size, self.size, device=DEVICE)
        self.motion_value = 0.0
        self.gamma_phase = 0.0
        self.is_receptive = 0.0

    @torch.no_grad() # Disable gradient calculations for speed
    def step(self):
        if not TORCH_AVAILABLE:
            return
            
        # 1. Get input image
        img_in = self.get_blended_input('image_in', 'mean')
        
        if img_in is None:
            # Evolve the last known field if no new input
            self.model.field.evolve(self.last_fast_field, steps=1)
            self.fast_field_data *= 0.95 # Fade out
            return
            
        # 2. Pre-process image for the model
        if img_in.ndim == 2: # Grayscale
            img_in = cv2.cvtColor(img_in.astype(np.float32), cv2.COLOR_GRAY2RGB)
        
        img_tensor = torch.from_numpy(img_in).permute(2, 0, 1).unsqueeze(0)
        img_tensor = (img_tensor * 2.0 - 1.0).to(DEVICE)

        # 3. Run the model (forward pass)
        fast_pattern_tensor, phase, receptive = self.model(img_tensor)
        
        # 4. Calculate Motion
        motion_diff = torch.abs(fast_pattern_tensor - self.last_fast_field).mean()
        self.motion_value = motion_diff.item() * 100.0 
        
        # 5. Store outputs
        self.fast_field_data = fast_pattern_tensor.cpu().squeeze().numpy()
        self.last_fast_field = fast_pattern_tensor.detach()
        self.gamma_phase = (phase / (2 * np.pi)) * 2.0 - 1.0 
        self.is_receptive = 1.0 if receptive else 0.0

    def get_output(self, port_name):
        if port_name == 'fast_field':
            # Normalize for visualization
            max_val = np.max(self.fast_field_data)
            min_val = np.min(self.fast_field_data)
            range_val = max_val - min_val
            if range_val > 1e-9:
                return (self.fast_field_data - min_val) / range_val
            return self.fast_field_data
            
        elif port_name == 'motion_signal':
            return self.motion_value
        elif port_name == 'gamma_phase':
            return self.gamma_phase
        elif port_name == 'is_receptive':
            return self.is_receptive
        return None
        
    def get_display_image(self):
        # Display the fast field
        img_data = self.get_output('fast_field')
        if img_data is None: 
            return None
            
        img_u8 = (np.clip(img_data, 0, 1) * 255).astype(np.uint8)
        
        # Apply colormap (Inferno, as in afc6.py)
        img_color = cv2.applyColorMap(img_u8, cv2.COLORMAP_INFERNO)
        
        # Add gate status bar
        if self.is_receptive:
            cv2.rectangle(img_color, (0, 0), (self.size, 5), (0, 255, 0), -1) # Green
        else:
            cv2.rectangle(img_color, (0, 0), (self.size, 5), (0, 0, 255), -1) # Red
        
        img_color = np.ascontiguousarray(img_color)
        h, w = img_color.shape[:2]
        return QtGui.QImage(img_color.data, w, h, 3*w, QtGui.QImage.Format.Format_BGR888)

    def get_config_options(self):
        return [
            ("Resolution (NxN)", "size", self.size, None),
        ]

=== FILE: latentannealernode.py ===

"""
LatentAnnealerNode - Applies diffusion (noise) and an external force vector
to the VAE's latent code.

Requires: numpy
Place this file in the 'nodes' folder as latentannealernode.py
"""

import numpy as np

# --- CRITICAL IMPORT BLOCK (your system's convention) ---
import __main__
BaseNode = __main__.BaseNode
# --------------------------------------------------------

class LatentAnnealerNode(BaseNode):
    
    # We use parent_node=None to avoid the TypeError we just fixed
    def __init__(self, parent_node=None): 
        super().__init__() 
        self.title = "Latent Annealer"
        self.parent_node = parent_node # Keep a reference if needed later
        
        # Inputs: Latent from the VAE, and a Force Vector from the ForceFieldNode
        self.add_input("Latent Input", "LATENT")
        self.add_input("Force Input", "LATENT_VECTOR", can_connect=True) 
        
        # Output: The modified latent code
        self.add_output("Latent Output", "LATENT")
        
        # Parameters for control
        self.add_float_input("Diffusion/Noise", 0.0, 0.0, 5.0, 0.1, "diffusion_changed")
        self.add_float_input("Random Seed", 1234.0, 1.0, 99999.0, 1.0, "seed_changed")
        
        # Internal state
        self.current_diffusion = 0.2 
        self.current_seed = 1234.0
        self.latent_output = np.zeros(16, dtype=np.float32) 
        np.random.seed(int(self.current_seed))

    def diffusion_changed(self, value):
        self.current_diffusion = value
        
    def seed_changed(self, value):
        self.current_seed = value
        np.random.seed(int(self.current_seed))

    def get_output(self, slot):
        if slot == 0:
            return self.latent_output
        return None

    def process(self):
        # 1. Get Latent Input from VAE (Node 2, VAE A)
        latent_in_np = self.get_input_value(0)
        if latent_in_np is None:
            # If no input, just pass the last output or a zero vector
            return

        # Ensure the output is the same shape as the input latent code
        if self.latent_output.shape != latent_in_np.shape:
             self.latent_output = np.zeros_like(latent_in_np)

        # 2. Annealing (Adding Gaussian Noise for Exploration)
        # Use the latent_in_np as the base
        noise = np.random.normal(0.0, self.current_diffusion, size=latent_in_np.shape).astype(np.float32)
        latent_annealed = latent_in_np + noise
        
        # 3. Attractor Stabilization (Adding Force Vector)
        force_np = self.get_input_value(1)
        if force_np is not None and force_np.shape == latent_annealed.shape:
            # Add the force vector (pulling the state towards the attractor)
            latent_annealed += force_np

        # 4. Set Output
        self.latent_output = latent_annealed

=== FILE: latentdecoder.py ===

"""
Latent Assembler Node (v2 - Corrected)
Collects individual signal inputs and assembles them into a latent vector (spectrum).
This node ONLY assembles. It does NOT decode.

The 'latent_out' port (orange) should be connected back to the 'latent_in'
port of the RealVAENode to be decoded by the TRAINED model.
"""

import numpy as np
import cv2

# --- CRITICAL IMPORT BLOCK ---
import __main__
BaseNode = __main__.BaseNode
QtGui = __main__.QtGui
# -----------------------------

class LatentAssemblerNode(BaseNode):
    """
    Assembles multiple signal inputs into a single latent vector (spectrum).
    Can also passthrough a spectrum and modify specific components.
    """
    NODE_CATEGORY = "Utility"
    NODE_COLOR = QtGui.QColor(150, 150, 150)
    
    def __init__(self, latent_dim=16):
        super().__init__()
        self.node_title = "Latent Assembler"
        
        self.latent_dim = int(latent_dim)
        
        # Create inputs: one for each latent dimension
        self.inputs = {
            'latent_base': 'spectrum',  # Optional base
        }
        for i in range(self.latent_dim):
            self.inputs[f'in_{i}'] = 'signal'
        
        self.outputs = {
            'latent_out': 'spectrum',
            # --- REMOVED 'image_out' ---
        }
        
        self.latent_vector = np.zeros(self.latent_dim, dtype=np.float32)

    def step(self):
        # Start with base latent if provided
        base = self.get_blended_input('latent_base', 'first')
        
        if base is not None:
            # Use base as starting point
            if len(base) >= self.latent_dim:
                self.latent_vector = base[:self.latent_dim].astype(np.float32)
            else:
                # Pad if base is too short
                self.latent_vector = np.zeros(self.latent_dim, dtype=np.float32)
                self.latent_vector[:len(base)] = base.astype(np.float32)
        else:
            # Start from zeros
            self.latent_vector = np.zeros(self.latent_dim, dtype=np.float32)
        
        # Override with individual signal inputs (if connected)
        for i in range(self.latent_dim):
            signal_val = self.get_blended_input(f'in_{i}', 'sum')
            if signal_val is not None:
                self.latent_vector[i] = float(signal_val)
    
    def get_output(self, port_name):
        if port_name == 'latent_out':
            return self.latent_vector
        return None
    
    def get_display_image(self):
        w, h = 256, 128
        img = np.zeros((h, w, 3), dtype=np.uint8)
        
        bar_width = max(1, w // self.latent_dim)
        
        # Normalize for display
        val_max = np.abs(self.latent_vector).max()
        if val_max < 1e-6: 
            val_max = 1.0
        
        for i, val in enumerate(self.latent_vector):
            x = i * bar_width
            norm_val = val / val_max
            bar_h = int(abs(norm_val) * (h/2 - 10))
            y_base = h // 2
            
            if val >= 0:
                color = (0, int(255 * abs(norm_val)), 0) # Green
                cv2.rectangle(img, (x, y_base-bar_h), (x+bar_width-1, y_base), color, -1)
            else:
                color = (0, 0, int(255 * abs(norm_val))) # Red
                cv2.rectangle(img, (x, y_base), (x+bar_width-1, y_base+bar_h), color, -1)
            
            # Label every 4th
            if i % 4 == 0:
                cv2.putText(img, str(i), (x+2, h-5), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.3, (255,255,255), 1)
        
        # Baseline
        cv2.line(img, (0, h//2), (w, h//2), (100,100,100), 1)
        
        # Status
        cv2.putText(img, f"Dim: {self.latent_dim}", (5, 15), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255,255,0), 1)
        
        return QtGui.QImage(img.data, w, h, 3*w, QtGui.QImage.Format.Format_RGB888)
    
    def get_config_options(self):
        return [
            ("Latent Dim", "latent_dim", self.latent_dim, None)
        ]

=== FILE: latentexplorernode.py ===

"""
Latent Explorer Node - Manipulate individual PCA coefficients
Explore what each principal component controls in your visual space
"""

import numpy as np
from PyQt6 import QtGui
import cv2

import __main__
BaseNode = __main__.BaseNode
QtGui = __main__.QtGui

class LatentExplorerNode(BaseNode):
    """
    Interactive manipulation of PCA latent codes.
    Add/subtract individual principal components to see what they control.
    """
    NODE_CATEGORY = "AI / Physics"
    NODE_COLOR = QtGui.QColor(220, 120, 180)
    
    def __init__(self, num_controls=8):
        super().__init__()
        self.node_title = "Latent Explorer"
        
        self.inputs = {
            'latent_in': 'spectrum',
            'pc0_mod': 'signal',  # Modulation for PC0
            'pc1_mod': 'signal',
            'pc2_mod': 'signal',
            'pc3_mod': 'signal',
            'pc4_mod': 'signal',
            'pc5_mod': 'signal',
            'pc6_mod': 'signal',
            'pc7_mod': 'signal',
            'global_scale': 'signal',  # Scale all modifications
            'reset': 'signal'  # Reset to original
        }
        self.outputs = {
            'latent_out': 'spectrum',
            'delta': 'spectrum',  # The modification vector
            'magnitude': 'signal'  # How much we've changed
        }
        
        self.num_controls = int(num_controls)
        
        # State
        self.latent_original = None
        self.latent_modified = None
        self.delta_vector = None
        self.magnitude = 0.0
        
        # Internal modulation values (for display when no signal input)
        self.internal_mods = np.zeros(8)
        
    def step(self):
        # Get inputs
        latent_in = self.get_blended_input('latent_in', 'first')
        global_scale = self.get_blended_input('global_scale', 'sum')
        if global_scale is None:
            global_scale = 1.0
            
        reset_signal = self.get_blended_input('reset', 'sum') or 0.0
        
        if latent_in is None:
            return
            
        # Store original
        if self.latent_original is None or reset_signal > 0.5:
            self.latent_original = latent_in.copy()
            
        # Get modulation values for each PC
        mods = []
        for i in range(min(self.num_controls, len(latent_in))):
            mod_signal = self.get_blended_input(f'pc{i}_mod', 'sum')
            if mod_signal is not None:
                mods.append(mod_signal * global_scale)
                self.internal_mods[i] = mod_signal
            else:
                mods.append(0.0)
                
        # Create delta vector
        self.delta_vector = np.zeros_like(latent_in)
        for i, mod in enumerate(mods):
            self.delta_vector[i] = mod * 2.0  # Scale for visibility
            
        # Apply modifications
        self.latent_modified = self.latent_original + self.delta_vector
        
        # Calculate magnitude of change
        self.magnitude = np.linalg.norm(self.delta_vector)
        
    def get_output(self, port_name):
        if port_name == 'latent_out':
            return self.latent_modified
        elif port_name == 'delta':
            return self.delta_vector
        elif port_name == 'magnitude':
            return self.magnitude
        return None
        
    def get_display_image(self):
        """
        Visualize:
        - Top: Original latent code (gray)
        - Middle: Delta vector (colored by +/-)
        - Bottom: Modified latent code
        """
        img = np.zeros((256, 256, 3), dtype=np.uint8)
        
        if self.latent_original is None:
            cv2.putText(img, "Waiting for input...", (10, 128), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255,255,255), 1)
            return QtGui.QImage(img.data, 256, 256, 256*3, QtGui.QImage.Format.Format_RGB888)
            
        latent_dim = len(self.latent_original)
        bar_width = max(1, 256 // latent_dim)
        
        # Helper function to draw latent code
        def draw_code(code, y_offset, color_fn):
            code_norm = code.copy()
            code_max = np.abs(code_norm).max()
            if code_max > 1e-6:
                code_norm = code_norm / code_max
                
            for i, val in enumerate(code_norm):
                x = i * bar_width
                h = int(abs(val) * 64)
                y_base = y_offset + 64
                
                if val >= 0:
                    y_start = y_base - h
                    y_end = y_base
                else:
                    y_start = y_base
                    y_end = y_base + h
                    
                color = color_fn(i, val)
                cv2.rectangle(img, (x, y_start), (x+bar_width-1, y_end), color, -1)
                
            # Draw baseline
            cv2.line(img, (0, y_offset+64), (256, y_offset+64), (100,100,100), 1)
            
        # Draw original (top section)
        draw_code(self.latent_original, 0, lambda i, v: (150, 150, 150))
        
        # Draw delta (middle section) - colored by sign
        def delta_color(i, val):
            if i < self.num_controls:
                # Controlled PCs: red for negative, green for positive
                if val > 0:
                    return (0, int(255 * abs(val)), 0)
                else:
                    return (0, 0, int(255 * abs(val)))
            else:
                return (100, 100, 100)  # Uncontrolled PCs
                
        draw_code(self.delta_vector, 64, delta_color)
        
        # Draw modified (bottom section) - highlight active PCs
        def modified_color(i, val):
            if i < self.num_controls and abs(self.delta_vector[i]) > 0.01:
                # Active PC: bright cyan
                return (255, 255, 0)
            else:
                # Inactive: white
                return (200, 200, 200)
                
        draw_code(self.latent_modified, 128, modified_color)
        
        # Labels
        cv2.putText(img, "ORIG", (5, 15), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255,255,255), 1)
        cv2.putText(img, "DELTA", (5, 79), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255,255,255), 1)
        cv2.putText(img, "MOD", (5, 143), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255,255,255), 1)
        
        # Magnitude indicator
        mag_text = f"||Î||={self.magnitude:.3f}"
        cv2.putText(img, mag_text, (5, 250), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255,255,0), 1)
        
        return QtGui.QImage(img.data, 256, 256, 256*3, QtGui.QImage.Format.Format_RGB888)
        
    def get_config_options(self):
        return [
            ("Num Controls", "num_controls", self.num_controls, None)
        ]

=== FILE: livingorganismnode.py ===

"""
Living Organism Node - A unified "living system" simulation with:
- A non-linear wave field (the "environment")
- 12 Homeostatic Cognitive Units (HCUs) forming a "soft organism"
- An MTX bus for agent communication

Ported from h_cu_life.py
Requires: pip install numpy
"""

import numpy as np
from PyQt6 import QtGui
import cv2
import math
import random
import sys
import os

import __main__
BaseNode = __main__.BaseNode
PA_INSTANCE = getattr(__main__, "PA_INSTANCE", None)
QtGui = __main__.QtGui

# --- Simulation Parameters (from h_cu_life.py) ---
GRID = 96                  # Smaller grid for performance
DT = 0.12                  
C = 0.85                   
DAMP = 0.015               
NONLIN = 0.18              
NOISE_AMP = 0.0007         
NUM_HCU = 12               
RING = True                
SPRING_K = 0.12            
SPRING_REST = 8.0          # Adjusted for smaller grid
SPACING_REPULSION = 150.0  
HCU_SENSE_SIGMA = 3.0      
HCU_STAMP = 0.012          
HCU_MOVE_GAIN = 0.85       
HCU_NOISE = 0.35           
HCU_TARGET_AMP = 0.30      
HCU_BASE_FREQ = 1.6        
BUS_MAX = 60               

# Prebuild a small Gaussian stamp used by HCUs
def gaussian_stamp(radius=7, sigma=HCU_SENSE_SIGMA):
    r = int(radius)
    y, x = np.mgrid[-r:r+1, -r:r+1]
    g = np.exp(-(x**2 + y**2)/(2*sigma**2))
    g /= g.sum()
    return g.astype(np.float32)

STAMP = gaussian_stamp(7, HCU_SENSE_SIGMA)

def splat(field, x, y, amp):
    """Add a Gaussian blob to the field at (x,y) with amplitude amp."""
    h, w = field.shape
    r = STAMP.shape[0]//2
    xi, yi = int(x), int(y)
    x0, x1 = max(0, xi-r), min(w, xi+r+1)
    y0, y1 = max(0, yi-r), min(h, yi+r+1)
    sx0, sx1 = r-(xi-x0), r+(x1-xi)
    sy0, sy1 = r-(yi-y0), r+(y1-yi)
    if x0 < x1 and y0 < y1:
        field[y0:y1, x0:x1] += amp * STAMP[sy0:sy1, sx0:sx1]

# --- Core Simulation Classes (from h_cu_life.py) ---

class HCU:
    """Homeostatic Cognitive Unit with internal Hopf oscillator."""
    def __init__(self, x, y, idx):
        self.x = float(x); self.y = float(y)
        self.vx = 0.0; self.vy = 0.0
        self.idx = idx
        self.z = complex(np.random.uniform(-0.1,0.1), np.random.uniform(-0.1,0.1))
        self.mu = 1.0
        self.omega = np.random.uniform(0.8, 1.2)*HCU_BASE_FREQ
        self.energy = 0.0
        self.energy_smooth = 0.0
        self.last_token = None
        self.token_clock = 0.0

    def hopf_step(self, u, dt):
        z = self.z
        r2 = (z.real*z.real + z.imag*z.imag)
        dz = complex(self.mu - r2, self.omega) * z + u
        z = z + dz*dt
        self.z = z

    def sense(self, field):
        h, w = field.shape
        xi, yi = int(self.x), int(self.y)
        r = STAMP.shape[0]//2
        x0, x1 = max(0, xi-r), min(w, xi+r+1)
        y0, y1 = max(0, yi-r), min(h, yi+r+1)
        sx0, sx1 = r-(xi-x0), r+(x1-xi)
        sy0, sy1 = r-(yi-y0), r+(y1-yi)
        
        patch = field[y0:y1, x0:x1]
        mask = STAMP[sy0:sy1, sx0:sx1]
        val = float((patch * mask).sum())
        
        gx = float((field[yi, (xi+1)%w] - field[yi, (xi-1)%w]) * 0.5)
        gy = float((field[(yi+1)%h, xi] - field[(yi-1)%h, xi]) * 0.5)
        return val, gx, gy

    def act(self, field, dt, bus):
        val, gx, gy = self.sense(field)
        r = abs(self.z)
        amp_err = (HCU_TARGET_AMP - r)
        u = complex(val*0.8, amp_err*0.6)
        self.hopf_step(u, dt)

        energy = abs(amp_err) + 0.3*math.sqrt(gx*gx + gy*gy)
        self.energy = energy
        self.energy_smooth = 0.92*self.energy_smooth + 0.08*energy

        self.vx += (-gx * HCU_MOVE_GAIN + np.random.randn()*HCU_NOISE) * dt
        self.vy += (-gy * HCU_MOVE_GAIN + np.random.randn()*HCU_NOISE) * dt
        self.vx *= 0.96; self.vy *= 0.96

        self.x = (self.x + self.vx) % field.shape[1]
        self.y = (self.y + self.vy) % field.shape[0]

        token = None
        if self.energy_smooth < 0.12:
            splat(field, self.x, self.y, +HCU_STAMP)
            token = 'l3' # focus
        elif self.energy_smooth > 0.28:
            splat(field, self.x, self.y, -HCU_STAMP)
            token = 'h0' # novelty
        else:
            token = 's1' # scan

        if token == self.last_token:
            self.token_clock += dt
        else:
            if self.last_token is not None and self.token_clock > 0.12:
                bus.append((self.idx, self.last_token, self.token_clock))
            self.last_token = token
            self.token_clock = 0.0
        return token

class World:
    """The simulation world, containing the field and agents"""
    def __init__(self, size):
        self.size = size
        self.phi = np.zeros((size, size), dtype=np.float32)
        self.phi_prev = np.zeros((size, size), dtype=np.float32)
        self.field_noise_on = True
        self.bus = []
        self.time = 0.0

        self.agents = []
        cx, cy = size//2, size//2
        for i in range(NUM_HCU):
            angle = (i / NUM_HCU) * 2 * math.pi
            r = size * 0.2
            self.agents.append(HCU(cx + r * math.cos(angle), cy + r * math.sin(angle), i))
        
        self.springs = []
        for i in range(NUM_HCU):
            j = (i + 1) % NUM_HCU if RING else i + 1
            if j < NUM_HCU:
                self.springs.append((self.agents[i], self.agents[j]))

    def step_field(self, dt):
        lap = (np.roll(self.phi, 1, 0) + np.roll(self.phi, -1, 0) +
               np.roll(self.phi, 1, 1) + np.roll(self.phi, -1, 1) - 4*self.phi)
        
        nonlinear_force = NONLIN * (self.phi - self.phi**3)
        phi_dot = (self.phi - self.phi_prev) / dt
        force = C*C * lap - DAMP * phi_dot + nonlinear_force

        phi_new = 2*self.phi - self.phi_prev + force * dt*dt
        self.phi_prev, self.phi = self.phi, phi_new
        
        if self.field_noise_on:
            self.phi += (np.random.randn(self.size, self.size) * NOISE_AMP).astype(np.float32)

    def step_agents(self, dt):
        for a, b in self.springs:
            dx, dy = b.x - a.x, b.y - a.y
            dist = math.hypot(dx, dy) + 1e-6
            force_mag = SPRING_K * (dist - SPRING_REST)
            fx, fy = force_mag * dx / dist, force_mag * dy / dist
            a.vx += fx; a.vy += fy
            b.vx -= fx; b.vy -= fy
        
        for i, a in enumerate(self.agents):
            for j in range(i + 1, len(self.agents)):
                b = self.agents[j]
                dx, dy = b.x - a.x, b.y - a.y
                dist_sq = dx*dx + dy*dy + 1e-6
                if dist_sq < (SPRING_REST * 2.5)**2:
                    force_mag = SPACING_REPULSION / dist_sq
                    fx, fy = force_mag * dx / math.sqrt(dist_sq), force_mag * dy / math.sqrt(dist_sq)
                    a.vx -= fx; a.vy -= fy
                    b.vx += fx; b.vy += fy

        self.bus.clear()
        for agent in self.agents:
            agent.act(self.phi, dt, self.bus)
    
    def step(self, dt):
        self.time += dt
        self.step_field(dt)
        self.step_agents(dt)


# --- The Main Node Class ---

class LivingOrganismNode(BaseNode):
    NODE_CATEGORY = "Source"
    NODE_COLOR = QtGui.QColor(20, 150, 150) # Biological Teal
    
    def __init__(self, size=96, steps_per_frame=2):
        super().__init__()
        self.node_title = "Living Organism (HCU)"
        
        self.inputs = {
            'noise_toggle': 'signal', # > 0.5 = noise ON
            'guidance_pulse': 'signal' # > 0.5 = inject guidance
        }
        self.outputs = {
            'field_image': 'image',   # The main wave field (phi)
            'avg_energy': 'signal',   # Average energy of all agents
            'bus_activity': 'signal'  # Number of MTX tokens this frame
        }
        
        self.size = int(size)
        self.steps_per_frame = int(steps_per_frame)
        
        # Initialize simulation
        self.world = World(size=self.size)
        self.last_guidance_trigger = 0.0

    def step(self):
        # 1. Handle Inputs
        noise_sig = self.get_blended_input('noise_toggle', 'sum')
        if noise_sig is not None:
            self.world.field_noise_on = (noise_sig > 0.5)
            
        guidance_sig = self.get_blended_input('guidance_pulse', 'sum')
        if guidance_sig is not None and guidance_sig > 0.5 and self.last_guidance_trigger <= 0.5:
            # Inject a global "thought" (guidance)
            rand_agent = random.choice(self.world.agents)
            self.world.bus.append((-1, 'h0', 0.5)) # -1 for global source
            splat(self.world.phi, rand_agent.x, rand_agent.y, -HCU_STAMP * 5)
        self.last_guidance_trigger = guidance_sig or 0.0

        # 2. Run simulation steps
        for _ in range(self.steps_per_frame):
            self.world.step(DT)

    def get_output(self, port_name):
        if port_name == 'field_image':
            # Normalize phi field [-0.4, 0.4] to [0, 1]
            return np.clip((self.world.phi + 0.4) / 0.8, 0.0, 1.0)
            
        elif port_name == 'avg_energy':
            # Average homeostatic energy of all agents
            if self.world.agents:
                return np.mean([a.energy_smooth for a in self.world.agents])
            return 0.0
            
        elif port_name == 'bus_activity':
            # Number of MTX tokens generated this frame
            return float(len(self.world.bus))
            
        return None
        
    def get_display_image(self):
        # Get the field image
        img_data = self.get_output('field_image')
        if img_data is None: return None
        
        img_u8 = (img_data * 255).astype(np.uint8)
        
        # Apply colormap (Viridis, as in screenshot)
        img_color = cv2.applyColorMap(img_u8, cv2.COLORMAP_VIRIDIS)
        
        # Draw the organism (agents and springs)
        for a, b in self.world.springs:
            pt1 = (int(a.x), int(a.y))
            pt2 = (int(b.x), int(b.y))
            cv2.line(img_color, pt1, pt2, (255, 255, 255), 1, cv2.LINE_AA)
            
        for a in self.world.agents:
            pt = (int(a.x), int(a.y))
            # Determine color based on internal state
            if a.last_token == 'l3': color = (0, 255, 0) # Green (focus)
            elif a.last_token == 'h0': color = (0, 0, 255) # Red (novelty)
            else: color = (255, 0, 0) # Blue (scan)
            
            cv2.circle(img_color, pt, 3, color, -1)
            cv2.circle(img_color, pt, 3, (255, 255, 255), 1)
        
        img_color = np.ascontiguousarray(img_color)
        h, w = img_color.shape[:2]
        return QtGui.QImage(img_color.data, w, h, 3*w, QtGui.QImage.Format.Format_BGR888)

    def get_config_options(self):
        return [
            ("Resolution (NxN)", "size", self.size, None),
            ("Sim Steps / Frame", "steps_per_frame", self.steps_per_frame, None),
        ]

=== FILE: lobe_emergence_node.py ===

"""
Lobe Emergence Node - Demonstrates how brain lobes emerge from W-matrix optimization
Shows the 'ghost cortex' - spatial localization of frequency filters through learning.

This node bridges:
- IHT Phase Field (quantum substrate)
- W Matrix (holographic decoder)
- Brain Lobes (emergent spatial structure)

Key insight: Lobes aren't designed - they EMERGE from optimization.

Place this file in the 'nodes' folder
"""

import numpy as np
from PyQt6 import QtGui
import cv2

import __main__
BaseNode = __main__.BaseNode
PA_INSTANCE = getattr(__main__, "PA_INSTANCE", None)
QtGui = __main__.QtGui

try:
    from scipy.fft import fft, ifft, fft2, ifft2
    from scipy.ndimage import gaussian_filter
    SCIPY_AVAILABLE = True
except ImportError:
    SCIPY_AVAILABLE = False
    print("Warning: LobeEmergenceNode requires scipy")

class LobeEmergenceNode(BaseNode):
    NODE_CATEGORY = "Analysis"
    NODE_COLOR = QtGui.QColor(200, 100, 200)  # Purple for emergence
    
    def __init__(self, grid_size=24, learning_rate=0.01, damage_location='None', initialization='Random'):
        super().__init__()
        self.node_title = "Lobe Emergence"
        self.initialization = initialization
        
        self.inputs = {
            'phase_field': 'image',        # Input quantum state
            'train_signal': 'signal',      # Trigger training
            'damage_amount': 'signal',     # How much damage to apply
        }
        
        self.outputs = {
            'ghost_cortex': 'image',           # 2D frequency map (the "lobes")
            'lobe_structure': 'image',         # Segmented lobe regions
            'emergence_metric': 'signal',       # How separated are lobes?
            'theta_lobe': 'image',             # Individual lobe outputs
            'alpha_lobe': 'image',
            'gamma_lobe': 'image',
            'cross_frequency_leakage': 'signal'
        }
        
        if not SCIPY_AVAILABLE:
            self.node_title = "Lobe Emergence (No SciPy!)"
            return
        
        self.grid_size = int(grid_size)
        self.learning_rate = float(learning_rate)
        self.damage_location = damage_location
        
        # The W matrix (complex) - starts random, will develop structure
        self.W = None
        self.training_steps = 0
        
        # State trackers for config changes
        self._last_init_mode = self.initialization
        self._last_grid_size = self.grid_size
        
        # --- FIX: Moved this block *before* _init_W() is called ---
        # Frequency bands (Hz equivalents in normalized units)
        self.freq_bands = {
            'theta': (0.05, 0.15),   # Low frequency
            'alpha': (0.15, 0.30),   # Mid frequency
            'gamma': (0.50, 0.90)    # High frequency
        }
        # --- END FIX ---
        
        self._init_W() # Build the W matrix
        
        # Throttle updates
        self.steps_since_last_visual_update = 0
        self.visual_update_interval = 5  # Only update visualization every N training steps
        
        # Outputs
        self.ghost_cortex_img = np.zeros((self.grid_size, self.grid_size, 3), dtype=np.float32)
        self.lobe_structure_img = np.zeros((self.grid_size, self.grid_size, 3), dtype=np.float32)
        self.emergence_score = 0.0
        self.leakage_score = 0.0
        
        # Lobe-specific outputs
        self.theta_lobe_img = np.zeros((self.grid_size, self.grid_size), dtype=np.float32)
        self.alpha_lobe_img = np.zeros((self.grid_size, self.grid_size), dtype=np.float32)
        self.gamma_lobe_img = np.zeros((self.grid_size, self.grid_size), dtype=np.float32)
        
    def _init_W(self):
        """Initialize W matrix with small random complex values"""
        
        # Update trackers when W is (re)built
        self._last_init_mode = self.initialization
        self._last_grid_size = self.grid_size
        self.training_steps = 0
        
        n = self.grid_size * self.grid_size
        
        if self.initialization == 'Random':
            # --- FIX: Start with pure noise, not a structured identity matrix ---
            # Pure random (slow to converge)
            noise_scale = 0.05 
            real_noise = np.random.randn(n, n) * noise_scale
            imag_noise = np.random.randn(n, n) * noise_scale
            self.W = (real_noise + 1j * imag_noise).astype(np.complex64)
            # --- END FIX ---
            
        elif self.initialization == 'Frequency-Biased':
            # Pre-bias W to prefer spatial frequency separation
            self.W = np.zeros((n, n), dtype=np.complex64)
            
            for i in range(n):
                y_i = i // self.grid_size
                x_i = i % self.grid_size
                
                if y_i < self.grid_size // 3:
                    freq_preference = 'theta'
                    phase_offset = 0.0
                elif y_i < 2 * self.grid_size // 3:
                    freq_preference = 'alpha'
                    phase_offset = np.pi / 3
                else:
                    freq_preference = 'gamma'
                    phase_offset = 2 * np.pi / 3
                
                for j in range(n):
                    y_j = j // self.grid_size
                    x_j = j % self.grid_size
                    dist = np.sqrt((x_i - x_j)**2 + (y_i - y_j)**2)
                    
                    if freq_preference == 'theta':
                        strength = np.exp(-dist / 8.0)
                        freq_mod = np.cos(dist * 0.2 + phase_offset)
                    elif freq_preference == 'alpha':
                        strength = np.exp(-dist / 5.0)
                        freq_mod = np.cos(dist * 0.5 + phase_offset)
                    else:  # gamma
                        strength = np.exp(-dist / 3.0)
                        freq_mod = np.cos(dist * 1.0 + phase_offset)
                    
                    self.W[i, j] = strength * freq_mod * (1.0 + 0.1j)
            
            noise_scale = 0.01
            self.W += (np.random.randn(n, n) + 1j * np.random.randn(n, n)) * noise_scale
            
            # --- FIX: Moved this loop inside the 'Frequency-Biased' block ---
            # It should not run for the 'Random' mode.
            # Encourage spatial locality
            for i in range(n):
                y_i = i // self.grid_size
                x_i = i % self.grid_size
                for j in range(n):
                    y_j = j // self.grid_size
                    x_j = j % self.grid_size
                    dist = np.sqrt((x_i - x_j)**2 + (y_i - y_j)**2)
                    if dist < 5.0:
                        self.W[i, j] += 0.1 * np.exp(-dist / 2.0)
            # --- END FIX ---

        # --- FIX: The "Encourage spatial locality" loop was here and has been moved. ---
        
        # Immediately compute the visual state after init
        self.ghost_cortex_img = self._compute_ghost_cortex(self.W)
        self.lobe_structure_img = self._segment_lobes(self.ghost_cortex_img)
        self.emergence_score = self._compute_emergence_metric(self.ghost_cortex_img)
        self.leakage_score = self._compute_cross_frequency_leakage(self.ghost_cortex_img)
        
    def _apply_damage(self, W, damage_amount):
        """Apply damage to specific lobe region"""
        if self.damage_location == 'None' or damage_amount < 0.01:
            return W
        
        h, w = self.grid_size, self.grid_size
        W_damaged = W.copy()
        
        damage_masks = {
            'theta': self._get_region_mask(0, 0, h//2, w//2),
            'alpha': self._get_region_mask(0, w//2, h//2, w),
            'gamma': self._get_region_mask(h//2, 0, h, w//2),
        }
        
        if self.damage_location in damage_masks:
            mask_flat = damage_masks[self.damage_location].flatten()
            for i in range(len(mask_flat)):
                if mask_flat[i]:
                    noise = (np.random.randn(W.shape[1]) + 1j * np.random.randn(W.shape[1])) * damage_amount * 0.3
                    W_damaged[i, :] += noise.astype(np.complex64)
                    W_damaged[i, :] *= (1.0 - damage_amount * 0.5)
        return W_damaged
    
    def _get_region_mask(self, y_start, x_start, y_end, x_end):
        mask = np.zeros((self.grid_size, self.grid_size), dtype=bool)
        mask[y_start:y_end, x_start:x_end] = True
        return mask
    

    def _compute_ghost_cortex(self, W):
        h, w = self.grid_size, self.grid_size
        ghost_cortex = np.zeros((h, w, 3), dtype=np.float32)
        test_signals = {}
        
        for freq_name, (low, high) in self.freq_bands.items():
            center_freq = (low + high) / 2.0
            y_coords, x_coords = np.meshgrid(np.arange(h), np.arange(w), indexing='ij')
            spatial_wave = np.sin(x_coords * center_freq * np.pi + y_coords * center_freq * np.pi * 0.7)
            test_signals[freq_name] = spatial_wave.flatten().astype(np.complex64)
        
        for i in range(h):
            for j in range(w):
                idx = i * w + j
                if idx >= W.shape[0]: continue
                W_row = W[idx, :]
                
                theta_response = np.abs(np.dot(W_row, test_signals['theta']))
                alpha_response = np.abs(np.dot(W_row, test_signals['alpha']))
                gamma_response = np.abs(np.dot(W_row, test_signals['gamma']))
                
                total = theta_response + alpha_response + gamma_response + 1e-9
                ghost_cortex[i, j, 0] = theta_response / total
                ghost_cortex[i, j, 1] = alpha_response / total
                ghost_cortex[i, j, 2] = gamma_response / total
        
        for c in range(3):
            ghost_cortex[:, :, c] = gaussian_filter(ghost_cortex[:, :, c], sigma=1.0)
        
        return ghost_cortex
    
    def _segment_lobes(self, ghost_cortex):
        dominant = np.argmax(ghost_cortex, axis=2)
        segmented = np.zeros_like(ghost_cortex)
        
        theta_mask = (dominant == 0)
        segmented[theta_mask] = [1.0, 0.0, 0.0]
        
        alpha_mask = (dominant == 1)
        segmented[alpha_mask] = [0.0, 1.0, 0.0]
        
        gamma_mask = (dominant == 2)
        segmented[gamma_mask] = [0.0, 0.0, 1.0]
        
        self.theta_lobe_img = theta_mask.astype(np.float32)
        self.alpha_lobe_img = alpha_mask.astype(np.float32)
        self.gamma_lobe_img = gamma_mask.astype(np.float32)
        
        return segmented
    
    def _compute_emergence_metric(self, ghost_cortex):
        r_var = np.var(ghost_cortex[:, :, 0])
        g_var = np.var(ghost_cortex[:, :, 1])
        b_var = np.var(ghost_cortex[:, :, 2])
        separation = (r_var + g_var + b_var) / 3.0
        separation = np.tanh(separation * 20.0)
        return float(separation)
    
    def _compute_cross_frequency_leakage(self, ghost_cortex):
        dominant = np.argmax(ghost_cortex, axis=2)
        h, w = ghost_cortex.shape[:2]
        leakage_sum = 0.0
        for i in range(h):
            for j in range(w):
                dom_idx = dominant[i, j]
                dom_power = ghost_cortex[i, j, dom_idx]
                other_power = 1.0 - dom_power
                leakage_sum += other_power
        leakage = leakage_sum / (h * w)
        return float(leakage)
    
    def _train_W_step(self, phase_field):
        """
        One gradient descent step to train W. (STABLE VERSION)
        """
        try:
            if phase_field.ndim == 3:
                phase_field = np.mean(phase_field, axis=2)
            
            phase_resized = cv2.resize(phase_field, (self.grid_size, self.grid_size))
            
            if not np.all(np.isfinite(phase_resized)):
                return 
                
            psi_flat = phase_resized.flatten().astype(np.complex64)
            
            psi_norm = np.linalg.norm(psi_flat)
            if psi_norm > 1e-6:
                psi_flat = psi_flat / psi_norm
            else:
                return 

            output = np.dot(self.W, psi_flat)
            
            output_norm = np.linalg.norm(output)
            if output_norm > 1e-6:
                output = output / output_norm
            else:
                output = np.zeros_like(output)

            n_updates = 50
            rows_updated = set() 

            for _ in range(n_updates):
                i_out = np.random.randint(0, self.grid_size)
                j_out = np.random.randint(0, self.grid_size)
                i_in = np.random.randint(0, self.grid_size)
                j_in = np.random.randint(0, self.grid_size)
                
                out_idx = i_out * self.grid_size + j_out
                in_idx = i_in * self.grid_size + j_in
                
                spatial_dist = np.sqrt((i_out - i_in)**2 + (j_out - j_in)**2)
                
                if spatial_dist < 10.0:
                    correlation = output[out_idx] * np.conj(psi_flat[in_idx])
                    
                    MAX_CORR_MAG = 100.0
                    corr_mag = np.abs(correlation)
                    if corr_mag > MAX_CORR_MAG:
                        correlation = correlation * (MAX_CORR_MAG / corr_mag)
                    
                    locality_factor = np.exp(-spatial_dist / 3.0)
                    safe_learning_rate = self.learning_rate * 0.01 
                    update_val = safe_learning_rate * correlation * locality_factor

                    if np.isfinite(update_val):
                        self.W[out_idx, in_idx] += update_val
                        rows_updated.add(out_idx)
            
            for idx in rows_updated:
                row_norm = np.linalg.norm(self.W[idx, :])
                if row_norm > 1.5: 
                    self.W[idx, :] /= row_norm
            
            MAX_W_MAGNITUDE = 5.0 
            np.clip(self.W.real, -MAX_W_MAGNITUDE, MAX_W_MAGNITUDE, out=self.W.real)
            np.clip(self.W.imag, -MAX_W_MAGNITUDE, MAX_W_MAGNITUDE, out=self.W.imag)

            self.training_steps += 1

        except Exception as e:
            print(f"CRITICAL ERROR in _train_W_step, resetting W: {e}")
            self._init_W()
    
    
    def step(self):
        if not SCIPY_AVAILABLE:
            return
        
        current_grid_size = int(self.grid_size)
        if (self.initialization != self._last_init_mode or 
            current_grid_size != self._last_grid_size):
            
            print(f"Config changed! Re-initializing W with mode: {self.initialization}")
            self.grid_size = current_grid_size 
            self._init_W() 
            return 
        
        phase_field = self.get_blended_input('phase_field', 'mean')
        train_signal = self.get_blended_input('train_signal', 'sum')
        
        if phase_field is None:
            phase_field = np.random.rand(self.grid_size, self.grid_size).astype(np.float32)
        
        if train_signal is not None and train_signal > 0.5:
            self._train_W_step(phase_field)
    
    def get_output(self, port_name):
        # --- NEW: Re-compute visuals on-demand when output is requested ---
        # This ensures outputs are always fresh, even if the node isn't training
        damage_amount = self.get_blended_input('damage_amount', 'sum')
        damage_amount = np.clip((damage_amount or 0.0) + 1.0, 0, 2.0) / 2.0
        W_current = self._apply_damage(self.W, damage_amount)
        
        # We need to re-compute these here to update the outputs
        ghost_cortex_img = self._compute_ghost_cortex(W_current)
        lobe_structure_img = self._segment_lobes(ghost_cortex_img)
        emergence_score = self._compute_emergence_metric(ghost_cortex_img)
        leakage_score = self._compute_cross_frequency_leakage(ghost_cortex_img)
        # --- END NEW ---

        if port_name == 'ghost_cortex':
            return ghost_cortex_img
        elif port_name == 'lobe_structure':
            return lobe_structure_img
        elif port_name == 'emergence_metric':
            return emergence_score
        elif port_name == 'theta_lobe':
            return self.theta_lobe_img # This is set by _segment_lobes
        elif port_name == 'alpha_lobe':
            return self.alpha_lobe_img
        elif port_name == 'gamma_lobe':
            return self.gamma_lobe_img
        elif port_name == 'cross_frequency_leakage':
            return leakage_score
        return None
    
    def get_display_image(self):
        """
        This function now re-computes the visualization every frame.
        """
        if not SCIPY_AVAILABLE:
            return None
        
        # --- NEW: Re-compute visuals every single frame ---
        damage_amount = self.get_blended_input('damage_amount', 'sum')
        damage_amount = np.clip((damage_amount or 0.0) + 1.0, 0, 2.0) / 2.0
        
        # Apply damage to W *for this frame only*
        W_current = self._apply_damage(self.W, damage_amount)
        
        # Compute ghost cortex (frequency map)
        self.ghost_cortex_img = self._compute_ghost_cortex(W_current)
        
        # Segment into discrete lobes
        self.lobe_structure_img = self._segment_lobes(self.ghost_cortex_img)
        
        # Compute metrics
        self.emergence_score = self._compute_emergence_metric(self.ghost_cortex_img)
        self.leakage_score = self._compute_cross_frequency_leakage(self.ghost_cortex_img)
        # --- END NEW ---
        
        # Create a detailed visualization
        display_h = 256
        display_w = 512
        display = np.zeros((display_h, display_w, 3), dtype=np.uint8)
        
        # Left side: Ghost cortex (smooth frequency map)
        ghost_resized = cv2.resize(self.ghost_cortex_img, (display_w//2, display_h))
        ghost_u8 = (np.clip(ghost_resized, 0, 1) * 255).astype(np.uint8)
        display[:, :display_w//2] = ghost_u8
        
        # Right side: Segmented lobes (discrete regions)
        lobe_resized = cv2.resize(self.lobe_structure_img, (display_w//2, display_h))
        lobe_u8 = (np.clip(lobe_resized, 0, 1) * 255).astype(np.uint8)
        display[:, display_w//2:] = lobe_u8
        
        # Add dividing line
        display[:, display_w//2-1:display_w//2+1] = [255, 255, 255]
        
        # Add labels
        font = cv2.FONT_HERSHEY_SIMPLEX
        
        # Left label
        cv2.putText(display, 'GHOST CORTEX', (10, 20), font, 0.5, (0, 0, 0), 3, cv2.LINE_AA)
        cv2.putText(display, 'GHOST CORTEX', (10, 20), font, 0.5, (255, 255, 255), 1, cv2.LINE_AA)
        
        # Right label
        cv2.putText(display, 'LOBES', (display_w//2 + 10, 20), font, 0.5, (0, 0, 0), 3, cv2.LINE_AA)
        cv2.putText(display, 'LOBES', (display_w//2 + 10, 20), font, 0.5, (255, 255, 255), 1, cv2.LINE_AA)
        
        # Add training step counter
        step_text = f"Training: {self.training_steps}"
        cv2.putText(display, step_text, (10, display_h - 10), font, 0.4, (0, 0, 0), 2, cv2.LINE_AA)
        cv2.putText(display, step_text, (10, display_h - 10), font, 0.4, (200, 200, 200), 1, cv2.LINE_AA)
        
        # Add emergence metric
        emergence_text = f"Emergence: {self.emergence_score:.2f}"
        cv2.putText(display, emergence_text, (10, display_h - 30), font, 0.4, (0, 0, 0), 2, cv2.LINE_AA)
        cv2.putText(display, emergence_text, (10, display_h - 30), font, 0.4, (0, 255, 0), 1, cv2.LINE_AA)
        
        # Add leakage metric (warning if high)
        leakage_text = f"Leakage: {self.leakage_score:.2f}"
        leakage_color = (0, 0, 255) if self.leakage_score > 0.3 else (200, 200, 200)
        cv2.putText(display, leakage_text, (10, display_h - 50), font, 0.4, (0, 0, 0), 2, cv2.LINE_AA)
        cv2.putText(display, leakage_text, (10, display_h - 50), font, 0.4, leakage_color, 1, cv2.LINE_AA)
        
        # Add legend (bottom right)
        legend_x = display_w//2 + 10
        legend_y = display_h - 60
        
        cv2.rectangle(display, (legend_x, legend_y), (legend_x + 20, legend_y + 10), (255, 0, 0), -1)
        cv2.putText(display, 'Theta (4-8Hz)', (legend_x + 25, legend_y + 8), font, 0.3, (255, 255, 255), 1, cv2.LINE_AA)
        
        cv2.rectangle(display, (legend_x, legend_y + 15), (legend_x + 20, legend_y + 25), (0, 255, 0), -1)
        cv2.putText(display, 'Alpha (8-13Hz)', (legend_x + 25, legend_y + 23), font, 0.3, (255, 255, 255), 1, cv2.LINE_AA)
        
        cv2.rectangle(display, (legend_x, legend_y + 30), (legend_x + 20, legend_y + 40), (0, 0, 255), -1)
        cv2.putText(display, 'Gamma (30-100Hz)', (legend_x + 25, legend_y + 38), font, 0.3, (255, 255, 255), 1, cv2.LINE_AA)
        
        # Add damage indicator if present
        if self.damage_location != 'None':
            damage_text = f"DAMAGED: {self.damage_location.upper()}"
            cv2.putText(display, damage_text, (display_w//2 + 10, display_h - 10), font, 0.4, (0, 0, 0), 3, cv2.LINE_AA)
            cv2.putText(display, damage_text, (display_w//2 + 10, display_h - 10), font, 0.4, (0, 0, 255), 1, cv2.LINE_AA)
        
        display = np.ascontiguousarray(display)
        return QtGui.QImage(display.data, display_w, display_h, 3*display_w, QtGui.QImage.Format.Format_RGB888)
    
    def get_config_options(self):
        return [
            ("Initialization", "initialization", self.initialization, [
                ("Random (Slow)", "Random"),
                ("Frequency-Biased (Fast)", "Frequency-Biased")
            ]),
            ("Grid Size", "grid_size", self.grid_size, None),
            ("Learning Rate", "learning_rate", self.learning_rate, None),
            ("Damage Location", "damage_location", self.damage_location, [
                ("None (Healthy)", "None"),
                ("Theta Lobe", "theta"),
                ("Alpha Lobe", "alpha"),
                ("Gamma Lobe", "gamma")
            ]),
        ]

=== FILE: loopattractornode.py ===

"""
Loop Attractor Node - A chaotic system with self-sustaining oscillations
Place this file in the 'nodes' folder as 'loopattractornode.py'
"""

import numpy as np
from PyQt6 import QtGui
import cv2

import __main__
BaseNode = __main__.BaseNode
PA_INSTANCE = getattr(__main__, "PA_INSTANCE", None)
QtGui = __main__.QtGui

class LoopAttractorNode(BaseNode):
    NODE_CATEGORY = "Source"
    NODE_COLOR = QtGui.QColor(180, 60, 120)
    
    def __init__(self, dt=0.01, a=10.0, b=8/3, c=28.0):
        super().__init__()
        self.node_title = "Loop Attractor"
        
        self.inputs = {
            'perturbation': 'signal',
            'parameter_a': 'signal',
            'parameter_c': 'signal',
            'reset': 'signal'
        }
        
        self.outputs = {
            'x_out': 'signal',
            'y_out': 'signal',
            'z_out': 'signal',
            'phase_image': 'image',
            'energy': 'signal'
        }
        
        self.dt = float(dt)
        self.a = float(a)
        self.b = float(b)
        self.c = float(c)
        
        self.x = 1.0
        self.y = 1.0
        self.z = 1.0
        
        self.history_len = 500
        self.history_x = np.zeros(self.history_len, dtype=np.float32)
        self.history_y = np.zeros(self.history_len, dtype=np.float32)
        self.history_z = np.zeros(self.history_len, dtype=np.float32)
        
        self.loop_phase = 0.0
        self.loop_amplitude = 1.0
        self.last_reset = 0.0
        
    def loop_dynamics(self, x, y, z, perturbation=0.0):
        dx = self.a * (y - x)
        dy = x * (self.c - z) - y
        dz = x * y - self.b * z
        
        loop_force = 0.5 * np.sin(self.loop_phase)
        dx += loop_force * y
        dy += loop_force * (-x)
        dx += perturbation
        
        self.loop_phase += 0.05 * np.sqrt(x*x + y*y + z*z + 0.01)
        self.loop_phase = self.loop_phase % (2 * np.pi)
        
        return dx, dy, dz
    
    def runge_kutta_4(self, x, y, z, perturbation=0.0):
        dx1, dy1, dz1 = self.loop_dynamics(x, y, z, perturbation)
        
        dx2, dy2, dz2 = self.loop_dynamics(
            x + 0.5*self.dt*dx1,
            y + 0.5*self.dt*dy1,
            z + 0.5*self.dt*dz1,
            perturbation
        )
        
        dx3, dy3, dz3 = self.loop_dynamics(
            x + 0.5*self.dt*dx2,
            y + 0.5*self.dt*dy2,
            z + 0.5*self.dt*dz2,
            perturbation
        )
        
        dx4, dy4, dz4 = self.loop_dynamics(
            x + self.dt*dx3,
            y + self.dt*dy3,
            z + self.dt*dz3,
            perturbation
        )
        
        new_x = x + (self.dt / 6.0) * (dx1 + 2*dx2 + 2*dx3 + dx4)
        new_y = y + (self.dt / 6.0) * (dy1 + 2*dy2 + 2*dy3 + dy4)
        new_z = z + (self.dt / 6.0) * (dz1 + 2*dz2 + 2*dz3 + dz4)
        
        return new_x, new_y, new_z
    
    def randomize(self):
        self.x = np.random.uniform(-5, 5)
        self.y = np.random.uniform(-5, 5)
        self.z = np.random.uniform(0, 30)
        self.loop_phase = np.random.uniform(0, 2*np.pi)
        self.history_x.fill(0)
        self.history_y.fill(0)
        self.history_z.fill(0)
        
    def step(self):
        perturbation = self.get_blended_input('perturbation', 'sum') or 0.0
        param_a = self.get_blended_input('parameter_a', 'sum')
        param_c = self.get_blended_input('parameter_c', 'sum')
        reset_sig = self.get_blended_input('reset', 'sum') or 0.0
        
        if reset_sig > 0.5 and self.last_reset <= 0.5:
            self.randomize()
        self.last_reset = reset_sig
        
        if param_a is not None:
            self.a = 10.0 + param_a * 5.0
        if param_c is not None:
            self.c = 30.0 + param_c * 10.0
        
        perturbation *= 5.0
        
        self.x, self.y, self.z = self.runge_kutta_4(self.x, self.y, self.z, perturbation)
        
        max_val = 100.0
        if abs(self.x) > max_val or abs(self.y) > max_val or abs(self.z) > max_val:
            self.randomize()
        
        self.history_x[:-1] = self.history_x[1:]
        self.history_x[-1] = self.x
        
        self.history_y[:-1] = self.history_y[1:]
        self.history_y[-1] = self.y
        
        self.history_z[:-1] = self.history_z[1:]
        self.history_z[-1] = self.z
        
    def get_output(self, port_name):
        if port_name == 'x_out':
            return np.tanh(self.x / 10.0)
        elif port_name == 'y_out':
            return np.tanh(self.y / 10.0)
        elif port_name == 'z_out':
            return np.tanh(self.z / 20.0)
        elif port_name == 'energy':
            return np.sqrt(self.x**2 + self.y**2 + self.z**2) / 30.0
        elif port_name == 'phase_image':
            return self.generate_phase_image()
        return None
    
    def generate_phase_image(self):
        w, h = 96, 96
        img = np.zeros((h, w), dtype=np.float32)
        
        if len(self.history_x) == 0:
            return img
        
        x_min, x_max = self.history_x.min(), self.history_x.max()
        y_min, y_max = self.history_y.min(), self.history_y.max()
        
        x_range = x_max - x_min + 1e-9
        y_range = y_max - y_min + 1e-9
        
        margin = 8
        x_coords = ((self.history_x - x_min) / x_range * (w - 2*margin) + margin).astype(int)
        y_coords = ((self.history_y - y_min) / y_range * (h - 2*margin) + margin).astype(int)
        
        y_coords = h - 1 - y_coords
        
        x_coords = np.clip(x_coords, 0, w-1)
        y_coords = np.clip(y_coords, 0, h-1)
        
        for i in range(1, len(x_coords)):
            intensity = i / len(x_coords)
            img[y_coords[i], x_coords[i]] = intensity
        
        img = cv2.GaussianBlur(img, (3, 3), 0)
        
        return img
        
    def get_display_image(self):
        phase_img = self.generate_phase_image()
        
        img_u8 = (np.clip(phase_img, 0, 1) * 255).astype(np.uint8)
        
        img_color = cv2.applyColorMap(img_u8, cv2.COLORMAP_PLASMA)
        
        w, h = 96, 96
        x_min, x_max = self.history_x.min(), self.history_x.max()
        y_min, y_max = self.history_y.min(), self.history_y.max()
        x_range = x_max - x_min + 1e-9
        y_range = y_max - y_min + 1e-9
        
        margin = 8
        curr_x = int((self.x - x_min) / x_range * (w - 2*margin) + margin)
        curr_y = int((self.y - y_min) / y_range * (h - 2*margin) + margin)
        curr_y = h - 1 - curr_y
        
        curr_x = np.clip(curr_x, 0, w-1)
        curr_y = np.clip(curr_y, 0, h-1)
        
        cv2.circle(img_color, (curr_x, curr_y), 3, (255, 255, 255), -1)
        
        center = (w - 12, 12)
        radius = 8
        angle = int(np.degrees(self.loop_phase))
        cv2.ellipse(img_color, center, (radius, radius), 0, 0, angle, (0, 255, 255), 2)
        
        img_color = np.ascontiguousarray(img_color)
        h, w = img_color.shape[:2]
        return QtGui.QImage(img_color.data, w, h, 3*w, QtGui.QImage.Format.Format_BGR888)
    
    def get_config_options(self):
        return [
            ("Time Step (dt)", "dt", self.dt, None),
            ("Parameter A (speed)", "a", self.a, None),
            ("Parameter B (dissipation)", "b", self.b, None),
            ("Parameter C (size)", "c", self.c, None),
        ]

=== FILE: math_node.py ===

"""
Math Nodes - An expanded library of nodes for signal math, logic, and boolean operations
Place this file in the 'nodes' folder
"""

import numpy as np
from PyQt6 import QtGui
from PIL import Image, ImageDraw, ImageFont
import math

# --- !! CRITICAL IMPORT BLOCK !! ---
import __main__
BaseNode = __main__.BaseNode
QtGui = __main__.QtGui
PA_INSTANCE = getattr(__main__, "PA_INSTANCE", None)
# -----------------------------------

class SignalMathNode(BaseNode):
    """Performs a mathematical operation on two input signals."""
    NODE_CATEGORY = "Transform"
    NODE_COLOR = QtGui.QColor(220, 120, 40) # Transform Orange
    
    def __init__(self, operation='add'):
        super().__init__()
        self.node_title = "Signal Math"
        self.inputs = {'A': 'signal', 'B': 'signal'}
        self.outputs = {'result': 'signal'}
        
        self.operation = operation
        self.result = 0.0
        self.last_a = 0.0
        self.last_b = 0.0

    def step(self):
        # Use last known value if an input is disconnected
        a = self.get_blended_input('A', 'sum')
        b = self.get_blended_input('B', 'sum')
        
        if a is None: a = self.last_a
        else: self.last_a = a
        
        if b is None: b = self.last_b
        else: self.last_b = b
        
        if self.operation == 'add':
            self.result = a + b
        elif self.operation == 'subtract':
            self.result = a - b
        elif self.operation == 'multiply':
            self.result = a * b
        elif self.operation == 'divide':
            if abs(b) < 1e-6:
                self.result = 0.0
            else:
                self.result = a / b
        elif self.operation == 'pow':
            try:
                # Use numpy for safer power calculation
                self.result = np.nan_to_num(math.pow(a, b))
            except (ValueError, OverflowError):
                self.result = 0.0 # Handle complex results or overflow
        elif self.operation == 'min':
            self.result = min(a, b)
        elif self.operation == 'max':
            self.result = max(a, b)
        elif self.operation == 'avg':
            self.result = (a + b) / 2.0
        
    def get_output(self, port_name):
        if port_name == 'result':
            return self.result
        return None
        
    def get_display_image(self):
        w, h = 64, 64
        img = np.zeros((h, w), dtype=np.uint8)
        
        op_symbol = {
            'add': '+', 'subtract': '-', 'multiply': 'Ã', 'divide': 'Ã·',
            'pow': '^', 'min': 'min', 'max': 'max', 'avg': 'avg'
        }.get(self.operation, '?')
        
        # --- FIX: Ensure self.result is a single float before formatting ---
        display_result = self.result
        if isinstance(self.result, np.ndarray) and self.result.size > 0:
            display_result = self.result.flat[0]
            
        text = f"A {op_symbol} B\n= {display_result:.2f}"
        
        img_pil = Image.fromarray(img)
        draw = ImageDraw.Draw(img_pil)
        
        try:
            font = ImageFont.load_default()
        except IOError:
            font = None 
            
        draw.text((5, 20), text, fill=255, font=font)
        
        img = np.array(img_pil)
        img = np.ascontiguousarray(img)
        return QtGui.QImage(img.data, w, h, w, QtGui.QImage.Format.Format_Grayscale8)

    def get_config_options(self):
        return [
            ("Operation", "operation", self.operation, [
                ("Add (A + B)", "add"),
                ("Subtract (A - B)", "subtract"),
                ("Multiply (A Ã B)", "multiply"),
                ("Divide (A Ã· B)", "divide"),
                ("Power (A ^ B)", "pow"),
                ("Min(A, B)", "min"),
                ("Max(A, B)", "max"),
                ("Average", "avg")
            ])
        ]

class SignalLogicNode(BaseNode):
    """
    Outputs one of two signals based on a test condition.
    (If Test > Threshold, output if_true, else output if_false)
    """
    NODE_CATEGORY = "Transform"
    NODE_COLOR = QtGui.QColor(220, 120, 40) # Transform Orange
    
    def __init__(self, threshold=0.5, condition='>'):
        super().__init__()
        self.node_title = "Signal Logic (If/Else)"
        self.inputs = {'test': 'signal', 'if_true': 'signal', 'if_false': 'signal'}
        self.outputs = {'result': 'signal'}
        
        self.threshold = float(threshold)
        self.condition = condition
        self.result = 0.0
        self.last_true = 0.0
        self.last_false = 0.0
        self.condition_met = False

    def step(self):
        test_val = self.get_blended_input('test', 'sum') or 0.0
        if_true_val = self.get_blended_input('if_true', 'sum')
        if_false_val = self.get_blended_input('if_false', 'sum')
        
        if if_true_val is not None: self.last_true = if_true_val
        if if_false_val is not None: self.last_false = if_false_val
        
        self.condition_met = False
        if self.condition == '>':
            self.condition_met = test_val > self.threshold
        elif self.condition == '<':
            self.condition_met = test_val < self.threshold
        elif self.condition == '==':
            self.condition_met = abs(test_val - self.threshold) < 1e-6
        elif self.condition == '>=':
            self.condition_met = test_val >= self.threshold
        elif self.condition == '<=':
            self.condition_met = test_val <= self.threshold
        elif self.condition == '!=':
            self.condition_met = abs(test_val - self.threshold) > 1e-6
            
        self.result = self.last_true if self.condition_met else self.last_false

    def get_output(self, port_name):
        if port_name == 'result':
            return self.result
        return None
        
    def get_display_image(self):
        w, h = 64, 64
        # Use RGB for color
        img = np.zeros((h, w, 3), dtype=np.uint8)
        
        if self.condition_met:
            img[10:h-10, 10:w-10] = (60, 220, 60) # Green
            text = "TRUE"
        else:
            img[10:h-10, 10:w-10] = (220, 60, 60) # Red
            text = "FALSE"
            
        img_pil = Image.fromarray(img)
        draw = ImageDraw.Draw(img_pil)
        
        try:
            font = ImageFont.load_default()
        except IOError:
            font = None
            
        condition_text = f"Test {self.condition} {self.threshold}"
        draw.text((5, 2), condition_text, fill=(255,255,255), font=font)
        draw.text((18, 28), text, fill=(255,255,255), font=font)
        
        img = np.array(img_pil)
        img = np.ascontiguousarray(img)
        return QtGui.QImage(img.data, w, h, 3*w, QtGui.QImage.Format.Format_RGB888)

    def get_config_options(self):
        return [
            ("Condition", "condition", self.condition, [
                ("Greater Than (>)", ">"),
                ("Less Than (<)", "<"),
                ("Equals (==)", "=="),
                ("Not Equal (!=)", "!="),
                ("Greater/Equal (>=)", ">="),
                ("Less/Equal (<=)", "<="),
            ]),
            ("Threshold", "threshold", self.threshold, None)
        ]

class SignalBooleanNode(BaseNode):
    """
    Performs boolean logic on two signals (A > thresh, B > thresh).
    Outputs 1.0 for True, 0.0 for False.
    """
    NODE_CATEGORY = "Transform"
    NODE_COLOR = QtGui.QColor(220, 120, 40) # Transform Orange
    
    def __init__(self, operation='and', threshold=0.0):
        super().__init__()
        self.node_title = "Signal Boolean"
        self.inputs = {'A': 'signal', 'B': 'signal'}
        self.outputs = {'result': 'signal'}
        
        self.operation = operation
        self.threshold = float(threshold)
        self.result = 0.0
        self.last_a = 0.0
        self.last_b = 0.0

    def step(self):
        a = self.get_blended_input('A', 'sum')
        b = self.get_blended_input('B', 'sum')
        
        if a is None: a = self.last_a
        else: self.last_a = a
        
        if b is None: b = self.last_b
        else: self.last_b = b
        
        # Convert signals to boolean based on threshold
        a_true = (a > self.threshold)
        b_true = (b > self.threshold)
        
        res_bool = False
        if self.operation == 'and':
            res_bool = a_true and b_true
        elif self.operation == 'or':
            res_bool = a_true or b_true
        elif self.operation == 'xor':
            res_bool = a_true ^ b_true
        elif self.operation == 'not':
            res_bool = not a_true  # Only uses input A
        elif self.operation == 'nand':
            res_bool = not (a_true and b_true)
        elif self.operation == 'nor':
            res_bool = not (a_true or b_true)
        elif self.operation == 'xnor':
            res_bool = not (a_true ^ b_true)
            
        self.result = 1.0 if res_bool else 0.0

    def get_output(self, port_name):
        if port_name == 'result':
            return self.result
        return None
        
    def get_display_image(self):
        w, h = 64, 64
        img = np.zeros((h, w), dtype=np.uint8)
        
        op_str = self.operation.upper()
        if op_str == 'NOT':
            text = f"NOT A\n= {self.result:.1f}"
        else:
            text = f"A {op_str} B\n= {self.result:.1f}"
        
        img_pil = Image.fromarray(img)
        draw = ImageDraw.Draw(img_pil)
        
        try:
            font = ImageFont.load_default()
        except IOError:
            font = None 
            
        draw.text((5, 20), text, fill=255, font=font)
        
        img = np.array(img_pil)
        img = np.ascontiguousarray(img)
        return QtGui.QImage(img.data, w, h, w, QtGui.QImage.Format.Format_Grayscale8)

    def get_config_options(self):
        return [
            ("Operation", "operation", self.operation, [
                ("AND", "and"),
                ("OR", "or"),
                ("XOR", "xor"),
                ("NOT (A only)", "not"),
                ("NAND", "nand"),
                ("NOR", "nor"),
                ("XNOR", "xnor"),
            ]),
            ("Boolean Threshold", "threshold", self.threshold, None)
        ]


=== FILE: media_source.py ===

"""
Media Source Node - Provides webcam or microphone input
Place this file in the 'nodes' folder
"""

import numpy as np
import cv2
from PyQt6 import QtGui

# Import the base class from parent directory
import sys
import os
# --- This is the new, correct block ---
import __main__
BaseNode = __main__.BaseNode
PA_INSTANCE = getattr(__main__, "PA_INSTANCE", None)
# ------------------------------------

try:
    import pyaudio
except ImportError:
    pyaudio = None

class MediaSourceNode(BaseNode):
    """Source node for video (Webcam) or audio (Microphone) input."""
    NODE_CATEGORY = "Source"
    NODE_COLOR = QtGui.QColor(40, 120, 80)
    
    def __init__(self, source_type='Webcam', device_id=0, width=160, height=120, sample_rate=44100):
        super().__init__()
        self.device_id = int(device_id) 
        self.source_type = source_type
        self.node_title = f"Source ({source_type})"
        self.w, self.h = width, height
        self.sample_rate = sample_rate
        
        self.outputs = {'signal': 'signal', 'image': 'image'}

        self.frame = np.zeros((self.h, self.w, 3), dtype=np.uint8)
        self.signal_output = 0.0 
        
        self.pa = PA_INSTANCE
        self.cap = None 
        self.stream = None
        
        # self.setup_source()
        
    def setup_source(self):
        """Initializes or re-initializes resources based on selected type."""
        # Cleanup existing resources
        if self.cap and self.cap.isOpened():
            self.cap.release()
        if self.stream:
            try: self.stream.stop_stream(); self.stream.close()
            except Exception: pass
        
        self.cap = None
        self.stream = None

        try:
            if self.source_type == 'Webcam':
                self.cap = cv2.VideoCapture(self.device_id)
                self.cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
                self.cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)
                if not self.cap.isOpened():
                    print(f"Warning: Cannot open webcam {self.device_id}")
            
            elif self.source_type == 'Microphone':
                if not self.pa:
                    print("Error: PyAudio not available for Microphone input.")
                    return
                
                channels = 1
                
                self.stream = self.pa.open(
                    format=pyaudio.paInt16,
                    channels=channels, 
                    rate=int(self.sample_rate),
                    input=True,
                    input_device_index=self.device_id,
                    frames_per_buffer=1024
                )
        except Exception as e:
            print(f"Error setting up source {self.source_type}: {e}")
            self.node_title = f"Source ({self.source_type} ERROR)"
            return
            
        self.node_title = f"Source ({self.source_type})"

    def step(self):
        self.frame *= 0  # clear frame to black
        
        if self.source_type == 'Webcam' and self.cap and self.cap.isOpened():
            ret, frame = self.cap.read()
            if ret:
                self.frame = cv2.resize(frame, (self.w, self.h))
                gray = cv2.cvtColor(self.frame, cv2.COLOR_BGR2GRAY)
                self.signal_output = np.mean(gray) / 255.0  # Luminance signal
                
        elif self.source_type == 'Microphone' and self.stream and self.stream.is_active():
            try:
                data = self.stream.read(256, exception_on_overflow=False)
                audio_data = np.frombuffer(data, dtype=np.int16).astype(np.float32) / 32768.0
                
                if audio_data.size > 0:
                    self.signal_output = np.sqrt(np.mean(audio_data**2)) * 5.0 
                
                # Visual Feedback
                if audio_data.size > 0:
                    padded_audio = np.pad(audio_data, (0, 1024 - len(audio_data)))
                    spec = np.abs(np.fft.fft(padded_audio))
                    spec = spec[:self.w].copy() 
                    
                    spec = np.log1p(spec)
                    spec = (spec - spec.min()) / (spec.max() - spec.min() + 1e-9)
                    
                    audio_img = np.zeros((self.h, self.w), dtype=np.uint8)
                    for i in range(self.w):
                        h = int(spec[i] * self.h)
                        audio_img[self.h - h:, i] = 255
                    
                    self.frame = cv2.cvtColor(audio_img, cv2.COLOR_GRAY2BGR)
                    
            except Exception:
                self.signal_output = 0.0
        
    def get_output(self, port_name):
        if port_name == 'image':
            if self.frame.ndim == 3:
                gray = cv2.cvtColor(self.frame, cv2.COLOR_BGR2GRAY).astype(np.float32) / 255.0
            else:
                gray = self.frame.astype(np.float32) / 255.0
            return gray
        elif port_name == 'signal':
            return self.signal_output
        return None
        
    def get_display_image(self):
        rgb = cv2.cvtColor(self.frame, cv2.COLOR_BGR2RGB)
        h, w = rgb.shape[:2]
        return QtGui.QImage(rgb.data, w, h, 3*w, QtGui.QImage.Format.Format_RGB888)
        
    def close(self):
        if self.cap and self.cap.isOpened():
            self.cap.release()
        if self.stream:
            try: self.stream.stop_stream(); self.stream.close()
            except Exception: pass
        super().close()
        
    def get_config_options(self):
        webcam_devices = [("Default Webcam (0)", 0), ("Secondary Webcam (1)", 1)]
        mic_devices = []
        if self.pa:
            for i in range(self.pa.get_device_count()):
                info = self.pa.get_device_info_by_index(i)
                if info.get('maxInputChannels', 0) > 0:
                    mic_devices.append((f"{info['name']} ({i})", i))
        
        device_options = mic_devices if self.source_type == 'Microphone' else webcam_devices
        
        if not any(v == self.device_id for _, v in device_options):
             device_options.append((f"Selected Device ({self.device_id})", self.device_id))
        
        return [
            ("Source Type", "source_type", self.source_type, [("Webcam", "Webcam"), ("Microphone", "Microphone")]),
            ("Device ID", "device_id", self.device_id, device_options),
        ]

=== FILE: metadynamiccoupler.py ===

"""
Meta-Dynamic Coupler Node - A simplified model of the Meta-Dynamic Ephaptic
Intelligence System. The agent learns to adjust its own internal coupling
parameter (alpha) based on prediction success.

Outputs the current learned physics parameter (alpha).
"""

import numpy as np
from PyQt6 import QtGui
import cv2

import __main__
BaseNode = __main__.BaseNode
PA_INSTANCE = getattr(__main__, "PA_INSTANCE", None)
QtGui = __main__.QtGui

class MetaDynamicCouplerNode(BaseNode):
    NODE_CATEGORY = "Analysis"
    NODE_COLOR = QtGui.QColor(255, 100, 255) # Meta-Dynamic Magenta
    
    def __init__(self, initial_coupling=0.5, learning_rate=0.01):
        super().__init__()
        self.node_title = "Meta-Dynamic Coupler"
        
        self.inputs = {
            'input_a': 'signal',          # Primary input field
            'success_target': 'signal',   # Target value (The goal state)
        }
        self.outputs = {
            'agent_output': 'signal',
            'current_coupling': 'signal', # The learned physics parameter
        }
        
        # --- Meta-Dynamic State Variables ---
        self.current_coupling = float(initial_coupling) # Alpha (the "rule")
        self.learning_rate = float(learning_rate)
        self.stabilizer = 0.5 # Keeps coupling adjustment smooth
        
        # Internal processing state
        self.internal_state = 0.0
        self.agent_output = 0.0

    def step(self):
        # 1. Get Inputs
        input_A = self.get_blended_input('input_a', 'sum') or 0.0
        target = self.get_blended_input('success_target', 'sum') or 0.0
        
        # 2. Agent's Forward Pass (The Decision/Output)
        # Decision = Internal State * Coupling + Input
        self.internal_state = self.internal_state * self.stabilizer + input_A
        self.agent_output = np.tanh(self.internal_state * self.current_coupling)
        
        # 3. Calculate Error (Success/Failure)
        # Goal: Make the output match the target using minimal change.
        error = target - self.agent_output
        
        # 4. Meta-Dynamic Learning (Rewriting the Rule/Physics)
        # The coupling (alpha) is adjusted based on the error and the input state.
        # This is a simplified form of gradient descent on the coupling equation itself.
        
        # Derivative of output w.r.t. coupling: d(tanh(I*a))/da = I * sechÂ²(I*a)
        # We approximate the gradient as: Error * Input * (1 - Output^2)
        
        approx_grad_coupling = input_A * (1.0 - self.agent_output**2)
        
        # Update Coupling: Adjust the rule to reduce the error.
        coupling_change = self.learning_rate * error * approx_grad_coupling
        
        self.current_coupling += coupling_change
        
        # Clamp coupling to a sensible range
        self.current_coupling = np.clip(self.current_coupling, 0.01, 5.0)

    def get_output(self, port_name):
        if port_name == 'agent_output':
            return self.agent_output
        elif port_name == 'current_coupling':
            return self.current_coupling
        return None
        
    def get_display_image(self):
        w, h = 96, 96
        img = np.zeros((h, w, 3), dtype=np.uint8)
        
        # Visualize Learning Progress (Color represents Coupling Value)
        norm_coupling = (self.current_coupling - 0.01) / 4.99 # Normalize 0.01 to 5.0
        
        # Map coupling to green/red (success/over-coupling)
        r = int(np.clip(norm_coupling * 255, 0, 255))
        g = int(np.clip((1 - norm_coupling) * 255, 0, 255))
        
        cv2.rectangle(img, (0, 0), (w, h), (g, 0, r), -1)
        
        # Draw current output value
        output_norm = (self.agent_output + 1) / 2.0 # Map [-1, 1] to [0, 1]
        out_bar_h = int(output_norm * h)
        cv2.rectangle(img, (w//4, h - out_bar_h), (w//2, h), (255, 255, 255), -1)

        # Draw Target value
        target = self.get_blended_input('success_target', 'sum') or 0.0
        target_norm = (target + 1) / 2.0 
        target_y = h - int(target_norm * h)
        cv2.line(img, (w//2 + 5, target_y), (w - 5, target_y), (255, 255, 0), 2)
        
        cv2.putText(img, f"a={self.current_coupling:.2f}", (5, 15), 
                    cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255, 255, 255), 1, cv2.LINE_AA)

        img = np.ascontiguousarray(img)
        return QtGui.QImage(img.data, w, h, 3*w, QtGui.QImage.Format.Format_BGR888)

    def get_config_options(self):
        return [
            ("Initial Coupling (Î±)", "current_coupling", self.current_coupling, None),
            ("Learning Rate", "learning_rate", self.learning_rate, None),
        ]

=== FILE: morecoordinatenodes.py ===

"""
More Coordinate-Driven Nodes - ULTRA SAFE VERSION

Wave interference, Voronoi fields, Lissajous curves, Flow field
All with bulletproof bounds checking
"""

import numpy as np
import cv2

import __main__
BaseNode = __main__.BaseNode
QtGui = __main__.QtGui

class WaveInterferenceNode(BaseNode):
    """Wave interference - SAFE"""
    NODE_CATEGORY = "Generator"
    NODE_COLOR = QtGui.QColor(100, 180, 220)
    
    def __init__(self, size=256, num_sources=3):
        super().__init__()
        self.node_title = "Wave Interference"
        
        self.inputs = {
            'source1_x': 'signal',
            'source1_y': 'signal',
            'source2_x': 'signal',
            'source2_y': 'signal',
            'frequency': 'signal',
            'phase_speed': 'signal'
        }
        self.outputs = {
            'image': 'image',
            'intensity': 'signal'
        }
        
        self.size = int(size)
        self.num_sources = int(num_sources)
        self.sources = np.random.rand(self.num_sources, 2) * self.size
        self.phase = 0.0
        
        y, x = np.mgrid[0:self.size, 0:self.size]
        self.coords = np.stack([x, y], axis=-1)
        
        self.field = np.zeros((self.size, self.size), dtype=np.float32)
        self.intensity = 0.0
        
    def step(self):
        s1x = self.get_blended_input('source1_x', 'sum') or 0.0
        s1y = self.get_blended_input('source1_y', 'sum') or 0.0
        s2x = self.get_blended_input('source2_x', 'sum') or 0.0
        s2y = self.get_blended_input('source2_y', 'sum') or 0.0
        freq = self.get_blended_input('frequency', 'sum') or 0.0
        phase_speed = self.get_blended_input('phase_speed', 'sum') or 1.0
        
        self.sources[0] = [(s1x + 1) * 0.5 * self.size, (s1y + 1) * 0.5 * self.size]
        if len(self.sources) > 1:
            self.sources[1] = [(s2x + 1) * 0.5 * self.size, (s2y + 1) * 0.5 * self.size]
        
        for i in range(2, len(self.sources)):
            angle = (i / len(self.sources)) * 2 * np.pi + self.phase * 0.1
            self.sources[i] = [
                self.size * 0.5 + np.cos(angle) * self.size * 0.3,
                self.size * 0.5 + np.sin(angle) * self.size * 0.3
            ]
        
        wave_frequency = 0.05 + freq * 0.05
        self.phase += 0.1 * phase_speed
        
        field = np.zeros((self.size, self.size), dtype=np.float32)
        
        for source in self.sources:
            dx = self.coords[:, :, 0] - source[0]
            dy = self.coords[:, :, 1] - source[1]
            dist = np.sqrt(dx**2 + dy**2)
            wave = np.sin(dist * wave_frequency - self.phase)
            amplitude = 1.0 / (1.0 + dist / 100.0)
            field += wave * amplitude
        
        self.field = (field - field.min()) / (field.max() - field.min() + 1e-9)
        center = self.size // 2
        self.intensity = float(self.field[center, center])
        
    def get_output(self, port_name):
        if port_name == 'image':
            colored = cv2.applyColorMap((self.field * 255).astype(np.uint8), cv2.COLORMAP_TWILIGHT)
            return colored.astype(np.float32) / 255.0
        elif port_name == 'intensity':
            return self.intensity
        return None


class VoronoiFieldNode(BaseNode):
    """Voronoi field - SAFE"""
    NODE_CATEGORY = "Generator"
    NODE_COLOR = QtGui.QColor(220, 150, 100)
    
    def __init__(self, size=256, num_seeds=8):
        super().__init__()
        self.node_title = "Voronoi Field"
        
        self.inputs = {
            'seed1_x': 'signal',
            'seed1_y': 'signal',
            'seed2_x': 'signal',
            'seed2_y': 'signal',
            'rotation': 'signal',
            'scale': 'signal'
        }
        self.outputs = {
            'image': 'image',
            'edge_density': 'signal'
        }
        
        self.size = int(size)
        self.num_seeds = int(num_seeds)
        self.seeds = np.random.rand(self.num_seeds, 2) * self.size
        self.colors = np.random.rand(self.num_seeds, 3)
        self.image = np.zeros((self.size, self.size, 3), dtype=np.float32)
        self.edge_density = 0.0
        
    def step(self):
        s1x = self.get_blended_input('seed1_x', 'sum') or 0.0
        s1y = self.get_blended_input('seed1_y', 'sum') or 0.0
        s2x = self.get_blended_input('seed2_x', 'sum') or 0.0
        s2y = self.get_blended_input('seed2_y', 'sum') or 0.0
        rotation = self.get_blended_input('rotation', 'sum') or 0.0
        scale = self.get_blended_input('scale', 'sum') or 0.0
        
        self.seeds[0] = [(s1x + 1) * 0.5 * self.size, (s1y + 1) * 0.5 * self.size]
        if self.num_seeds > 1:
            self.seeds[1] = [(s2x + 1) * 0.5 * self.size, (s2y + 1) * 0.5 * self.size]
        
        angle_offset = rotation * np.pi
        scale_factor = 0.3 + scale * 0.2
        
        for i in range(2, self.num_seeds):
            angle = (i / self.num_seeds) * 2 * np.pi + angle_offset
            self.seeds[i] = [
                self.size * 0.5 + np.cos(angle) * self.size * scale_factor,
                self.size * 0.5 + np.sin(angle) * self.size * scale_factor
            ]
        
        image = np.zeros((self.size, self.size, 3), dtype=np.float32)
        y, x = np.mgrid[0:self.size, 0:self.size]
        
        min_dist = np.full((self.size, self.size), np.inf)
        closest_seed = np.zeros((self.size, self.size), dtype=int)
        
        for i, seed in enumerate(self.seeds):
            dx = x - seed[0]
            dy = y - seed[1]
            dist = np.sqrt(dx**2 + dy**2)
            mask = dist < min_dist
            min_dist[mask] = dist[mask]
            closest_seed[mask] = i
        
        for i in range(self.num_seeds):
            mask = closest_seed == i
            image[mask] = self.colors[i]
        
        edges = np.zeros((self.size, self.size), dtype=np.float32)
        for i in range(1, self.size - 1):
            for j in range(1, self.size - 1):
                if closest_seed[i, j] != closest_seed[i-1, j] or \
                   closest_seed[i, j] != closest_seed[i, j-1]:
                    edges[i, j] = 1.0
        
        edges_colored = np.stack([edges, edges, edges], axis=-1)
        image = image * (1 - edges_colored * 0.5) + edges_colored * 0.5
        
        self.image = image
        self.edge_density = float(np.mean(edges))
        
    def get_output(self, port_name):
        if port_name == 'image':
            return self.image
        elif port_name == 'edge_density':
            return self.edge_density
        return None


class LissajousNode(BaseNode):
    """Lissajous curves - ULTRA SAFE VERSION"""
    NODE_CATEGORY = "Generator"
    NODE_COLOR = QtGui.QColor(180, 120, 220)
    
    def __init__(self, size=256, trail_length=100):
        super().__init__()
        self.node_title = "Lissajous Curves"
        
        self.inputs = {
            'freq_x': 'signal',
            'freq_y': 'signal',
            'phase': 'signal',
            'speed': 'signal'
        }
        self.outputs = {
            'image': 'image',
            'symmetry': 'signal'
        }
        
        self.size = int(size)
        self.trail_length = max(10, int(trail_length))
        
        # Trail buffer - use list for safety
        self.trail = [[self.size // 2, self.size // 2] for _ in range(self.trail_length)]
        self.trail_idx = 0
        
        self.t = 0.0
        self.symmetry = 0.0
        
    def step(self):
        fx = self.get_blended_input('freq_x', 'sum') or 0.0
        fy = self.get_blended_input('freq_y', 'sum') or 0.0
        phase = self.get_blended_input('phase', 'sum') or 0.0
        speed = self.get_blended_input('speed', 'sum') or 1.0
        
        freq_x = 1.0 + fx * 2.0
        freq_y = 1.0 + fy * 2.0
        phase_shift = phase * np.pi
        
        x = np.sin(freq_x * self.t + phase_shift)
        y = np.sin(freq_y * self.t)
        
        px = int(np.clip((x + 1) * 0.5 * self.size, 0, self.size - 1))
        py = int(np.clip((y + 1) * 0.5 * self.size, 0, self.size - 1))
        
        # SAFE: Update current trail position
        self.trail[self.trail_idx] = [px, py]
        
        # SAFE: Increment with wrap
        self.trail_idx = (self.trail_idx + 1) % self.trail_length
        
        self.t += 0.05 * speed
        
        # Symmetry calculation
        if self.trail_length > 20:
            recent = np.array(self.trail[-20:])
            variance = np.var(recent, axis=0)
            self.symmetry = 1.0 / (1.0 + np.mean(variance) / 100.0)
        else:
            self.symmetry = 0.0
        
    def get_output(self, port_name):
        if port_name == 'image':
            image = np.zeros((self.size, self.size, 3), dtype=np.uint8)
            
            # Convert trail to numpy array and clamp
            points = np.array(self.trail, dtype=np.int32)
            points = np.clip(points, 0, self.size - 1)
            
            # Draw lines
            for i in range(len(points) - 1):
                p1 = tuple(points[i])
                p2 = tuple(points[i + 1])
                color_intensity = int((i / len(points)) * 255)
                color = (color_intensity, 100, 255 - color_intensity)
                cv2.line(image, p1, p2, color, 2, cv2.LINE_AA)
            
            # Draw current point
            current_idx = (self.trail_idx - 1 + self.trail_length) % self.trail_length
            current = tuple(points[current_idx])
            cv2.circle(image, current, 5, (255, 255, 255), -1)
            
            return image.astype(np.float32) / 255.0
        elif port_name == 'symmetry':
            return self.symmetry
        return None


class FlowFieldNode(BaseNode):
    """Flow field - ULTRA SAFE VERSION"""
    NODE_CATEGORY = "Generator"
    NODE_COLOR = QtGui.QColor(120, 200, 150)
    
    def __init__(self, size=256, particle_count=200):
        super().__init__()
        self.node_title = "Flow Field"
        
        self.inputs = {
            'offset_x': 'signal',
            'offset_y': 'signal',
            'scale': 'signal',
            'strength': 'signal'
        }
        self.outputs = {
            'image': 'image',
            'turbulence': 'signal'
        }
        
        self.size = int(size)
        self.particle_count = int(particle_count)
        
        # Initialize particles in safe zone
        self.particles = np.random.rand(self.particle_count, 2) * (self.size - 2) + 1
        self.trail_buffer = np.zeros((self.size, self.size, 3), dtype=np.float32)
        self.turbulence = 0.0
        
    def step(self):
        ox = self.get_blended_input('offset_x', 'sum') or 0.0
        oy = self.get_blended_input('offset_y', 'sum') or 0.0
        scale = self.get_blended_input('scale', 'sum') or 0.0
        strength = self.get_blended_input('strength', 'sum') or 1.0
        
        noise_scale = 0.02 + scale * 0.03
        offset = np.array([ox * 100, oy * 100])
        
        for i in range(len(self.particles)):
            pos = self.particles[i]
            noise_pos = (pos + offset) * noise_scale
            
            angle = np.sin(noise_pos[0]) * np.cos(noise_pos[1]) * 2 * np.pi
            vx = np.cos(angle) * strength
            vy = np.sin(angle) * strength
            
            # Limit velocity
            vx = np.clip(vx, -5, 5)
            vy = np.clip(vy, -5, 5)
            
            self.particles[i] += [vx, vy]
            
            # HARD clamp
            self.particles[i] = np.clip(self.particles[i], 0, self.size - 1)
            
            # Draw - SAFE
            x = int(self.particles[i][0])
            y = int(self.particles[i][1])
            
            if 0 <= x < self.size and 0 <= y < self.size:
                color = np.clip(np.array([vx, vy, 0.5]) * 0.5 + 0.5, 0, 1)
                self.trail_buffer[y, x] = color
        
        self.trail_buffer *= 0.95
        
        # Turbulence
        velocities = []
        for pos in self.particles:
            noise_pos = (pos + offset) * noise_scale
            angle = np.sin(noise_pos[0]) * np.cos(noise_pos[1]) * 2 * np.pi
            velocities.append([np.cos(angle), np.sin(angle)])
        
        self.turbulence = float(np.var(velocities))
        
    def get_output(self, port_name):
        if port_name == 'image':
            return self.trail_buffer
        elif port_name == 'turbulence':
            return self.turbulence
        return None

=== FILE: neuralstringattractornode.py ===

"""
Neural String Attractor Node - Converts phase space coordinates into a strange attractor
Inspired by the Neural String Attractor HTML system.

[FIXED-v3]
- Uses new logic (phase_x, energy inputs).
- Uses float32 (0.0-1.0) buffers to prevent OverflowError.
- Applies colormaps to all 'image' outputs to prevent MoirÃ©/Broadcast error.
"""

import numpy as np
from PyQt6 import QtGui
import cv2

import __main__
BaseNode = __main__.BaseNode
PA_INSTANCE = getattr(__main__, "PA_INSTANCE", None)

class NeuralString:
    """A single vibrating neural string with frequency resonance"""
    def __init__(self, string_id, length=64):
        self.id = string_id
        self.length = length
        self.values = np.random.randn(length).astype(np.float32) * 0.1
        self.previous_values = self.values.copy()
        
        # String properties
        self.frequency = 100 + np.random.rand() * 900  # 100-1000 Hz
        self.phase = np.random.rand() * 2 * np.pi
        self.energy = 0.0
        self.coherence = 0.0
        self.is_active = False
        
    def apply_frequency(self, input_freq, amplitude=0.1):
        """Apply frequency modulation with resonance"""
        # Calculate resonance (peaks when input_freq matches string frequency)
        resonance = np.exp(-np.abs(self.frequency - input_freq) / 200.0)
        
        # Update phase
        self.phase += self.frequency * 0.01 * resonance
        self.phase %= (2 * np.pi)
        
        # Apply wave to string
        for i in range(self.length):
            spatial_phase = (i / self.length) * 2 * np.pi
            wave = np.sin(self.phase + spatial_phase) * amplitude * resonance
            self.values[i] += wave
            
        return resonance
    
    def update(self):
        """Update string physics (diffusion and damping)"""
        self.previous_values = self.values.copy()
        
        # Diffusion (neighbor averaging)
        for i in range(1, self.length - 1):
            diffusion = (self.values[i-1] + self.values[i+1] - 2 * self.values[i]) * 0.1
            self.values[i] += diffusion
            
        # Damping
        self.values *= 0.99
        
        # Calculate metrics
        self.energy = np.sqrt(np.mean(self.values ** 2))
        
        # Coherence (lower variance = higher coherence)
        mean_val = np.mean(self.values)
        variance = np.mean((self.values - mean_val) ** 2)
        self.coherence = np.exp(-variance)
        
        self.is_active = self.energy > 0.01
        
    def get_output(self):
        """Get scalar output representing string state"""
        # Clamp energy to 0-1 range for safe multiplication
        safe_energy = np.clip(self.energy, 0, 1.0)
        return safe_energy * self.coherence * np.sin(self.phase)


class NeuralStringAttractorNode(BaseNode):
    NODE_CATEGORY = "Transform"
    NODE_COLOR = QtGui.QColor(180, 60, 180)  # Neural purple
    
    def __init__(self, num_strings=8, string_length=64):
        super().__init__()
        self.node_title = "Neural String Attractor"
        
        self.inputs = {
            'phase_x': 'signal',     # From WebcamPhaseNode
            'phase_y': 'signal',
            'phase_z': 'signal',
            'energy': 'signal',      # Used to modulate frequency
            'frequency': 'signal'    # Direct frequency control
        }
        
        self.outputs = {
            'attractor_x': 'signal',  # 3D attractor coordinates
            'attractor_y': 'signal',
            'attractor_z': 'signal',
            'coherence': 'signal',    # Average string coherence
            'attractor_image': 'image',  # Visual trajectory
            'string_viz': 'image'       # Neural strings visualization
        }
        
        self.num_strings = int(num_strings)
        self.string_length = int(string_length)
        
        # Create neural strings
        self.strings = [NeuralString(i, self.string_length) for i in range(self.num_strings)]
        
        # Attractor trajectory history
        self.trajectory = np.zeros((500, 3), dtype=np.float32)
        self.trajectory_idx = 0
        
        # Current attractor position
        self.attractor_pos = np.array([0.0, 0.0, 0.0], dtype=np.float32)
        
        # --- FIX: Use float32 buffers (0.0 - 1.0) to prevent overflow ---
        self.attractor_img = np.zeros((128, 128), dtype=np.float32)
        self.strings_img = np.zeros((64, 128), dtype=np.float32)
        
        # Base frequency (modulated by inputs)
        self.base_frequency = 1000.0
        
    def step(self):
        # Get inputs
        phase_x = self.get_blended_input('phase_x', 'sum') or 0.0
        phase_y = self.get_blended_input('phase_y', 'sum') or 0.0
        phase_z = self.get_blended_input('phase_z', 'sum') or 0.0
        energy = self.get_blended_input('energy', 'sum') or 0.0
        freq_control = self.get_blended_input('frequency', 'sum')
        
        # Calculate input frequency (base + modulation from energy)
        if freq_control is not None:
            # Direct frequency control (map [-1,1] to [500, 2000] Hz)
            input_frequency = 500 + (freq_control + 1.0) * 750.0
        else:
            # Frequency from energy (500-2000 Hz range)
            input_frequency = 500 + energy * 1500.0
            
        self.base_frequency = input_frequency
        
        # Update each neural string
        active_count = 0
        total_coherence = 0.0
        
        for string in self.strings:
            resonance = string.apply_frequency(input_frequency, energy)
            string.update()
            
            if string.is_active:
                active_count += 1
            total_coherence += string.coherence
            
        avg_coherence = total_coherence / self.num_strings
        
        # Generate attractor point from neural string outputs
        outputs = np.array([s.get_output() for s in self.strings])
        
        # Combine string outputs into 3D attractor coordinates
        # Mix phase space inputs with neural string dynamics
        if self.num_strings >= 8:
            self.attractor_pos[0] = (outputs[0] + outputs[1] * 0.5 + outputs[2] * 0.25) + phase_x * 0.3
            self.attractor_pos[1] = (outputs[3] + outputs[4] * 0.5 + outputs[5] * 0.25) + phase_y * 0.3
            self.attractor_pos[2] = (outputs[6] + outputs[7] * 0.5 + avg_coherence) + phase_z * 0.3
        else:
            # Fallback for fewer strings
            self.attractor_pos[0] = np.sum(outputs) + phase_x * 0.3
            self.attractor_pos[1] = avg_coherence + phase_y * 0.3
            self.attractor_pos[2] = np.mean(outputs) + phase_z * 0.3
        
        # Clamp to reasonable range
        self.attractor_pos = np.clip(self.attractor_pos, -2.0, 2.0)
        
        # Store in trajectory
        self.trajectory[self.trajectory_idx] = self.attractor_pos
        self.trajectory_idx = (self.trajectory_idx + 1) % len(self.trajectory)
        
        # Update visualizations
        self._update_attractor_viz()
        self._update_strings_viz()
        
    def _update_attractor_viz(self):
        """Render 2D projection of 3D attractor trajectory"""
        self.attractor_img *= 0.9  # Fade buffer instead of clearing
        
        # Project 3D to 2D (X-Y plane with Z affecting brightness)
        # Draw only the most recent points for performance
        num_points_to_draw = 100
        for i in range(num_points_to_draw):
            idx = (self.trajectory_idx - 1 - i + len(self.trajectory)) % len(self.trajectory)
            x, y, z = self.trajectory[idx]
            
            # Map to image coordinates
            px = int((x + 2.0) / 4.0 * 127)
            py = int((y + 2.0) / 4.0 * 127)
            
            px = np.clip(px, 0, 127)
            py = np.clip(py, 0, 127)
            
            # Brightness from Z and age
            age_factor = (num_points_to_draw - i) / num_points_to_draw # 1.0 (newest) to 0.0 (oldest)
            z_factor = (z + 2.0) / 4.0 # 0.0 to 1.0
            brightness = age_factor * z_factor
            
            # Draw point (using float32)
            self.attractor_img[py, px] = max(self.attractor_img[py, px], brightness)
            
        # Blur for smooth trails
        self.attractor_img = cv2.GaussianBlur(self.attractor_img, (3, 3), 0)
        
    def _update_strings_viz(self):
        """Render neural strings as waveforms"""
        self.strings_img *= 0.8  # Fade buffer instead of clearing
        
        h, w = self.strings_img.shape
        
        for i, string in enumerate(self.strings):
            if not string.is_active:
                continue
                
            # Y position for this string
            y_base = int((i + 0.5) / self.num_strings * h)
            
            # Draw waveform
            for j in range(self.string_length):
                x = int(j / self.string_length * (w - 1))
                
                # Wave amplitude
                amp = string.values[j]
                y_offset = int(amp * 10) # 10 pixel max amplitude
                y = np.clip(y_base + y_offset, 0, h - 1)
                
                # Brightness from energy (clamped 0-1)
                brightness = np.clip(string.energy, 0.0, 1.0)
                
                self.strings_img[y, x] = max(self.strings_img[y, x], brightness)
                
    def get_output(self, port_name):
        if port_name == 'attractor_x':
            return float(self.attractor_pos[0])
        elif port_name == 'attractor_y':
            return float(self.attractor_pos[1])
        elif port_name == 'attractor_z':
            return float(self.attractor_pos[2])
        elif port_name == 'coherence':
            return float(np.mean([s.coherence for s in self.strings]))
            
        # --- FIX: Apply colormap to return a 3-channel RGB image ---
        elif port_name == 'attractor_image':
            img_u8 = (np.clip(self.attractor_img, 0, 1) * 255).astype(np.uint8)
            img_color = cv2.applyColorMap(img_u8, cv2.COLORMAP_MAGMA)
            return img_color.astype(np.float32) / 255.0
            
        elif port_name == 'string_viz':
            img_u8 = (np.clip(self.strings_img, 0, 1) * 255).astype(np.uint8)
            img_color = cv2.applyColorMap(img_u8, cv2.COLORMAP_VIRIDIS)
            return img_color.astype(np.float32) / 255.0
            
        return None
        
    def get_display_image(self):
        # --- FIX: Use float buffer, convert to uint8 for colormap ---
        img_u8 = (np.clip(self.attractor_img, 0, 1) * 255).astype(np.uint8)
        
        # Apply colormap for better visibility
        img_color = cv2.applyColorMap(img_u8, cv2.COLORMAP_MAGMA)
        
        # Draw current position marker
        x = int((self.attractor_pos[0] + 2.0) / 4.0 * 127)
        y = int((self.attractor_pos[1] + 2.0) / 4.0 * 127)
        x = np.clip(x, 0, 127)
        y = np.clip(y, 0, 127)
        cv2.circle(img_color, (x, y), 3, (255, 255, 255), -1)
        
        # --- FIX: Return a QImage safely ---
        img_color = np.ascontiguousarray(img_color)
        h, w = img_color.shape[:2]
        # Use BGR888 because OpenCV's colormap output is BGR
        return QtGui.QImage(img_color.data, w, h, 3*w, QtGui.QImage.Format.Format_BGR888)
    
    def get_config_options(self):
        return [
            ("Number of Strings", "num_strings", self.num_strings, None),
            ("String Length", "string_length", self.string_length, None),
        ]

=== FILE: noise_generator.py ===

"""
Noise Generator Node - Generates various noise types
Place this file in the 'nodes' folder
"""

import numpy as np
from PyQt6 import QtGui

import sys
import os
# --- This is the new, correct block ---
import __main__
BaseNode = __main__.BaseNode
PA_INSTANCE = getattr(__main__, "PA_INSTANCE", None)
# ------------------------------------

class NoiseGeneratorNode(BaseNode):
    NODE_CATEGORY = "Source"
    NODE_COLOR = QtGui.QColor(40, 120, 80) # Source Green
    
    def __init__(self, width=160, height=120, noise_type='white', speed=0.1):
        super().__init__()
        self.node_title = "Noise Gen"
        self.outputs = {'image': 'image', 'signal': 'signal'} 
        self.w, self.h = int(width), int(height)
        self.noise_type = noise_type 
        self.speed = float(speed)
        
        self._init_arrays()
        
    def _init_arrays(self):
        """Initialize or reinitialize arrays based on current w, h"""
        self.img = np.random.rand(self.h, self.w).astype(np.float32)
        self.signal_value = 0.0 
        self.brown_state = np.zeros((self.h, self.w), dtype=np.float32)
        self.perlin_phase = np.random.rand(2) * 100

    def _generate_noise_step(self, shape):
        """Generates a noise array based on the selected type."""
        if self.noise_type == 'white':
            return np.random.rand(*shape)
        
        elif self.noise_type == 'brown':
            # Ensure brown_state matches current shape
            if self.brown_state.shape != shape:
                self.brown_state = np.zeros(shape, dtype=np.float32)
            
            rand_step = np.random.randn(*shape) * 0.05 * self.speed
            self.brown_state = self.brown_state + rand_step
            self.brown_state = np.clip(self.brown_state, -1.0, 1.0)
            return (self.brown_state + 1.0) / 2.0
        
        elif self.noise_type == 'perlin':
            X, Y = np.meshgrid(np.arange(shape[1]), np.arange(shape[0]))
            self.perlin_phase += self.speed * 0.1 
            
            noise_val = (
                np.sin(X * 0.1 + self.perlin_phase[0]) + 
                np.sin(Y * 0.05 + self.perlin_phase[1] * 0.5)
            )
            noise_val = (noise_val - noise_val.min()) / (noise_val.max() - noise_val.min() + 1e-9)
            noise_val += np.random.rand(*shape) * 0.01 
            return np.clip(noise_val, 0, 1)
            
        elif self.noise_type == 'quantum':
            noise = np.random.rand(*shape)
            if np.random.rand() < 0.02 * self.speed * 10: 
                 noise += np.random.rand(*shape) * 0.5 * self.speed
            return np.clip(noise, 0, 1)
            
        return np.random.rand(*shape)

    def step(self):
        # Check if dimensions changed (from config update)
        if self.img.shape != (self.h, self.w):
            self._init_arrays()
        
        new_noise = self._generate_noise_step((self.h, self.w))
        
        self.img = self.img * (1.0 - self.speed) + new_noise * self.speed
        
        center_y, center_x = self.h // 2, self.w // 2
        window_size = 10
        y_start = max(0, center_y - window_size//2)
        y_end = min(self.h, center_y + window_size//2)
        x_start = max(0, center_x - window_size//2)
        x_end = min(self.w, center_x + window_size//2)
        
        center_patch = self.img[y_start:y_end, x_start:x_end]
        
        if center_patch.size > 0:
            self.signal_value = np.mean(center_patch) * 2.0 - 1.0
        else:
            self.signal_value = 0.0
        
    def get_output(self, port_name):
        if port_name == 'image':
            return self.img
        elif port_name == 'signal':
            return self.signal_value
        return None
        
    def get_display_image(self):
        img_u8 = (np.clip(self.img, 0, 1) * 255).astype(np.uint8)
        img_u8 = np.ascontiguousarray(img_u8)
        return QtGui.QImage(img_u8.data, self.w, self.h, self.w, QtGui.QImage.Format.Format_Grayscale8)

    def get_config_options(self):
        return [
            ("Noise Type", "noise_type", self.noise_type, [
                ("White (Uniform)", "white"), 
                ("Brown (Coherent)", "brown"),
                ("Perlin (Pattern)", "perlin"), 
                ("Quantum (Spikes)", "quantum")
            ]),
            ("Speed (Blend Factor)", "speed", self.speed, None),
        ]

=== FILE: opticalflownode.py ===

"""
Optical Flow Motion Tracker Node

This node ACTUALLY extracts coordinate data from webcam movement.
Uses Lucas-Kanade optical flow to track motion vectors.

Real use cases:
- Gesture control interfaces
- Motion-reactive installations
- Game input via webcam
- Accessibility tools (head tracking for mouse control)
"""

import numpy as np
import cv2

import __main__
BaseNode = __main__.BaseNode
QtGui = __main__.QtGui

class OpticalFlowNode(BaseNode):
    """Tracks motion in video and outputs motion vectors as coordinates"""
    NODE_CATEGORY = "Analysis"
    NODE_COLOR = QtGui.QColor(50, 150, 200)
    
    def __init__(self, points_to_track=20, quality_level=0.3, min_distance=7):
        super().__init__()
        self.node_title = "Optical Flow Tracker"
        
        self.inputs = {'image': 'image'}
        self.outputs = {
            'motion_x': 'signal',      # Average horizontal motion
            'motion_y': 'signal',      # Average vertical motion
            'motion_magnitude': 'signal',  # Speed of motion
            'motion_angle': 'signal',  # Direction (-1 to 1, maps to -180 to 180 degrees)
            'flow_vis': 'image',       # Visualization of motion vectors
            'has_motion': 'signal'     # 1.0 if significant motion detected
        }
        
        # Parameters
        self.points_to_track = int(points_to_track)
        self.quality_level = float(quality_level)
        self.min_distance = int(min_distance)
        
        # State
        self.prev_gray = None
        self.prev_points = None
        
        # Outputs
        self.motion_x = 0.0
        self.motion_y = 0.0
        self.motion_magnitude = 0.0
        self.motion_angle = 0.0
        self.has_motion = 0.0
        self.flow_vis = None
        
        # Lucas-Kanade parameters
        self.lk_params = dict(
            winSize=(15, 15),
            maxLevel=2,
            criteria=(cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03)
        )
        
        # Feature detection parameters
        self.feature_params = dict(
            maxCorners=self.points_to_track,
            qualityLevel=self.quality_level,
            minDistance=self.min_distance,
            blockSize=7
        )
        
    def step(self):
        image = self.get_blended_input('image', 'mean')
        
        if image is None:
            return
            
        # Convert to grayscale uint8
        if image.dtype != np.uint8:
            if image.max() <= 1.0:
                gray = (image * 255).astype(np.uint8)
            else:
                gray = np.clip(image, 0, 255).astype(np.uint8)
        else:
            gray = image
            
        if len(gray.shape) == 3:
            gray = cv2.cvtColor(gray, cv2.COLOR_RGB2GRAY)
            
        # Initialize on first frame
        if self.prev_gray is None:
            self.prev_gray = gray
            self.prev_points = cv2.goodFeaturesToTrack(
                gray, 
                mask=None, 
                **self.feature_params
            )
            self.flow_vis = np.zeros((*gray.shape, 3), dtype=np.uint8)
            return
            
        # Calculate optical flow
        if self.prev_points is not None and len(self.prev_points) > 0:
            next_points, status, error = cv2.calcOpticalFlowPyrLK(
                self.prev_gray,
                gray,
                self.prev_points,
                None,
                **self.lk_params
            )
            
            # Select good points
            if next_points is not None:
                good_new = next_points[status == 1]
                good_old = self.prev_points[status == 1]
                
                if len(good_new) > 0:
                    # Calculate motion vectors
                    motion_vectors = good_new - good_old
                    
                    # Average motion
                    avg_motion = np.mean(motion_vectors, axis=0)
                    self.motion_x = float(avg_motion[0]) / gray.shape[1]  # Normalize by width
                    self.motion_y = float(avg_motion[1]) / gray.shape[0]  # Normalize by height
                    
                    # Motion magnitude (speed)
                    magnitudes = np.linalg.norm(motion_vectors, axis=1)
                    self.motion_magnitude = float(np.mean(magnitudes)) / gray.shape[1]
                    
                    # Motion angle
                    if self.motion_magnitude > 0.001:
                        angle_rad = np.arctan2(self.motion_y, self.motion_x)
                        self.motion_angle = float(angle_rad / np.pi)  # Normalize to -1 to 1
                        self.has_motion = 1.0
                    else:
                        self.motion_angle = 0.0
                        self.has_motion = 0.0
                    
                    # Create visualization
                    self.flow_vis = np.zeros((*gray.shape, 3), dtype=np.uint8)
                    
                    # Draw tracks
                    for i, (new, old) in enumerate(zip(good_new, good_old)):
                        a, b = new.ravel().astype(int)
                        c, d = old.ravel().astype(int)
                        
                        # Draw line
                        cv2.line(self.flow_vis, (a, b), (c, d), (0, 255, 0), 2)
                        # Draw point
                        cv2.circle(self.flow_vis, (a, b), 3, (0, 0, 255), -1)
                    
                    # Draw average motion vector
                    h, w = gray.shape
                    center = (w // 2, h // 2)
                    end = (
                        int(center[0] + self.motion_x * w * 10),
                        int(center[1] + self.motion_y * h * 10)
                    )
                    cv2.arrowedLine(self.flow_vis, center, end, (255, 0, 0), 3, tipLength=0.3)
                    
                    # Update points for next frame
                    self.prev_points = good_new.reshape(-1, 1, 2)
                else:
                    # No good points, reset
                    self.prev_points = None
                    self.has_motion = 0.0
            else:
                self.prev_points = None
                self.has_motion = 0.0
        
        # Redetect features if we lost tracking
        if self.prev_points is None or len(self.prev_points) < self.points_to_track // 2:
            self.prev_points = cv2.goodFeaturesToTrack(
                gray,
                mask=None,
                **self.feature_params
            )
        
        # Update previous frame
        self.prev_gray = gray
        
    def get_output(self, port_name):
        if port_name == 'motion_x':
            return self.motion_x
        elif port_name == 'motion_y':
            return self.motion_y
        elif port_name == 'motion_magnitude':
            return self.motion_magnitude
        elif port_name == 'motion_angle':
            return self.motion_angle
        elif port_name == 'has_motion':
            return self.has_motion
        elif port_name == 'flow_vis':
            if self.flow_vis is not None:
                return self.flow_vis.astype(np.float32) / 255.0
        return None


class MotionToCoordinatesNode(BaseNode):
    """Converts motion signals to accumulated position coordinates"""
    NODE_CATEGORY = "Transform"
    NODE_COLOR = QtGui.QColor(100, 150, 50)
    
    def __init__(self, sensitivity=0.5, decay=0.95, bounds=1.0):
        super().__init__()
        self.node_title = "Motion â Coordinates"
        
        self.inputs = {
            'motion_x': 'signal',
            'motion_y': 'signal',
            'reset': 'signal'
        }
        self.outputs = {
            'x_coord': 'signal',  # Accumulated X position (-1 to 1)
            'y_coord': 'signal',  # Accumulated Y position (-1 to 1)
            'distance_from_center': 'signal',  # 0 to 1
            'normalized_angle': 'signal'  # 0 to 1 (for circular mapping)
        }
        
        self.sensitivity = float(sensitivity)
        self.decay = float(decay)
        self.bounds = float(bounds)
        
        # State
        self.x = 0.0
        self.y = 0.0
        self.last_reset = 0.0
        
    def step(self):
        motion_x = self.get_blended_input('motion_x', 'sum') or 0.0
        motion_y = self.get_blended_input('motion_y', 'sum') or 0.0
        reset = self.get_blended_input('reset', 'sum') or 0.0
        
        # Reset on trigger
        if reset > 0.5 and self.last_reset <= 0.5:
            self.x = 0.0
            self.y = 0.0
        self.last_reset = reset
        
        # Accumulate motion with decay
        self.x = self.x * self.decay + motion_x * self.sensitivity
        self.y = self.y * self.decay + motion_y * self.sensitivity
        
        # Clamp to bounds
        self.x = np.clip(self.x, -self.bounds, self.bounds)
        self.y = np.clip(self.y, -self.bounds, self.bounds)
        
    def get_output(self, port_name):
        if port_name == 'x_coord':
            return self.x
        elif port_name == 'y_coord':
            return self.y
        elif port_name == 'distance_from_center':
            return np.sqrt(self.x**2 + self.y**2) / self.bounds
        elif port_name == 'normalized_angle':
            angle = np.arctan2(self.y, self.x)
            return (angle + np.pi) / (2 * np.pi)  # 0 to 1
        return None


"""
COMMERCIAL APPLICATIONS:

1. GESTURE CONTROL:
   Webcam â OpticalFlow â MotionToCoordinates â Control any parameter
   Use case: Hands-free control for music production, VJ software, accessibility

2. HEAD TRACKING MOUSE:
   Webcam â OpticalFlow â Scale motion_x/y â Mouse control
   Use case: Accessibility tool for people with limited hand mobility
   Market: Assistive technology (high willingness to pay)

3. MOTION-REACTIVE ART:
   Webcam â OpticalFlow â Drive fractal params, colors, effects
   Use case: Interactive installations, museums, retail displays
   Market: B2B (museums, experiential marketing)

4. WEBCAM GAME CONTROLLER:
   OpticalFlow â Map to game inputs
   Use case: Alternative controller for rhythm games, casual games
   Market: Gaming accessories

TO USE:
1. Save as OpticalFlowNode.py in your nodes folder
2. Restart Perception Lab
3. Connect webcam â OpticalFlowNode
4. Use motion_x/y to control ANYTHING
5. MotionToCoordinates accumulates motion into position for cursor-like control
"""

=== FILE: pcanode.py ===


"""
Spectral PCA Node - Learns principal components of FFT spectra
Discovers which frequency patterns co-occur in your visual environment
"""

import numpy as np
from PyQt6 import QtGui
import cv2

import __main__
BaseNode = __main__.BaseNode
QtGui = __main__.QtGui

class SpectralPCANode(BaseNode):
    """
    Learns PCA basis from complex FFT spectra.
    Compresses spectrum to latent code, reconstructs back.
    """
    NODE_CATEGORY = "AI / Physics"
    NODE_COLOR = QtGui.QColor(120, 180, 220)
    
    def __init__(self, latent_dim=16, buffer_size=100):
        super().__init__()
        self.node_title = "Spectral PCA"
        
        self.inputs = {
            'complex_spectrum': 'complex_spectrum',
            'learn': 'signal',  # 0-1: when to collect samples
            'pc_weights': 'spectrum'  # Optional: manually set latent code
        }
        self.outputs = {
            'latent_code': 'spectrum',  # The compressed representation
            'reconstructed_spectrum': 'complex_spectrum',
            'reconstruction_error': 'signal'
        }
        
        self.latent_dim = int(latent_dim)
        self.buffer_size = int(buffer_size)
        
        # Learning buffers
        self.spectrum_buffer = []
        self.is_learned = False
        
        # PCA parameters (the learned W-matrix!)
        self.mean_spectrum = None
        self.pca_components = None  # The principal components
        self.explained_variance = None
        
        # Current state
        self.latent_code = None
        self.reconstructed_spectrum = None
        self.error = 0.0
        
    def step(self):
        # Get input spectrum
        spec_in = self.get_blended_input('complex_spectrum', 'first')
        learn_signal = self.get_blended_input('learn', 'sum') or 0.0
        
        if spec_in is None:
            return
            
        # Flatten spectrum to vector
        spec_flat = spec_in.flatten()
        
        # LEARNING MODE: Collect samples
        if learn_signal > 0.5 and len(self.spectrum_buffer) < self.buffer_size:
            self.spectrum_buffer.append(spec_flat.copy())
            
            # When buffer full, compute PCA
            if len(self.spectrum_buffer) == self.buffer_size:
                self._compute_pca()
                
        # INFERENCE MODE: Encode/decode
        if self.is_learned:
            # Check if external latent code provided
            external_code = self.get_blended_input('pc_weights', 'first')
            
            if external_code is not None and len(external_code) == self.latent_dim:
                # Use provided latent code
                self.latent_code = external_code
            else:
                # Encode: project onto learned basis
                self.latent_code = self._encode(spec_flat)
            
            # Decode: reconstruct from latent
            self.reconstructed_spectrum = self._decode(self.latent_code)
            
            # Reshape back to 2D
            self.reconstructed_spectrum = self.reconstructed_spectrum.reshape(spec_in.shape)
            
            # Calculate reconstruction error
            self.error = np.mean(np.abs(spec_in - self.reconstructed_spectrum))
    
    def _compute_pca(self):
        """Compute PCA from collected spectra"""
        X = np.array(self.spectrum_buffer, dtype=np.complex64)
        
        # Separate real and imaginary parts
        X_real = X.real
        X_imag = X.imag
        
        # Compute mean
        self.mean_spectrum = X.mean(axis=0)
        
        # Center data
        X_real_centered = X_real - X_real.mean(axis=0)
        X_imag_centered = X_imag - X_imag.mean(axis=0)
        
        # SVD on real part (you could also do on magnitude)
        U, S, Vt = np.linalg.svd(X_real_centered, full_matrices=False)
        
        # Keep top components
        self.pca_components = Vt[:self.latent_dim]
        self.explained_variance = S[:self.latent_dim] ** 2 / len(X)
        
        self.is_learned = True
        print(f"PCA learned! Variance explained: {self.explained_variance.sum() / S.sum():.2%}")
        
    def _encode(self, spectrum):
        """Project spectrum onto learned PCA basis"""
        if not self.is_learned:
            return np.zeros(self.latent_dim)
            
        # Center
        centered = spectrum - self.mean_spectrum
        
        # Project (works for complex, projects real part)
        latent = centered.real @ self.pca_components.T
        
        return latent
    
    def _decode(self, latent_code):
        """Reconstruct spectrum from latent code"""
        if not self.is_learned:
            return np.zeros_like(self.mean_spectrum)
            
        # Reconstruct real part
        reconstructed_real = self.mean_spectrum.real + latent_code @ self.pca_components
        
        # Keep imaginary part from mean (or zero)
        reconstructed = reconstructed_real + 1j * self.mean_spectrum.imag
        
        return reconstructed
        
    def get_output(self, port_name):
        if port_name == 'latent_code':
            return self.latent_code
        elif port_name == 'reconstructed_spectrum':
            return self.reconstructed_spectrum
        elif port_name == 'reconstruction_error':
            return self.error
        return None
        
    def get_display_image(self):
        """Visualize latent code as bar graph"""
        img = np.zeros((128, 256, 3), dtype=np.uint8)
        
        if self.latent_code is None:
            return QtGui.QImage(img.data, 256, 128, 256*3, QtGui.QImage.Format.Format_RGB888)
            
        # Normalize latent code for display
        code = self.latent_code.copy()
        code_min, code_max = code.min(), code.max()
        if code_max - code_min > 1e-6:
            code_norm = (code - code_min) / (code_max - code_min)
        else:
            code_norm = np.zeros_like(code)
            
        # Draw bars
        bar_width = 256 // self.latent_dim
        for i, val in enumerate(code_norm):
            x = i * bar_width
            h = int(val * 128)
            
            # Color based on explained variance if available
            if self.explained_variance is not None:
                var_ratio = self.explained_variance[i] / self.explained_variance.max()
                color = (int(255 * var_ratio), 100, 255 - int(255 * var_ratio))
            else:
                color = (255, 255, 255)
                
            cv2.rectangle(img, (x, 128-h), (x+bar_width-1, 128), color, -1)
            
        return QtGui.QImage(img.data, 256, 128, 256*3, QtGui.QImage.Format.Format_RGB888)
        
    def get_config_options(self):
        return [
            ("Latent Dim", "latent_dim", self.latent_dim, None),
            ("Buffer Size", "buffer_size", self.buffer_size, None)
        ]


=== FILE: phaseexplorernode.py ===

"""
Phase Explorer Node - An automated probe to find the "Goldilocks Zone"
by mapping the phase space of a toy universe's fundamental constants.

Ported from goldilocks_explorer.py
Requires: pip install numpy scipy
"""

import numpy as np
from PyQt6 import QtGui, QtCore
import cv2
import sys
import os
import threading
import time

import __main__
BaseNode = __main__.BaseNode
PA_INSTANCE = getattr(__main__, "PA_INSTANCE", None)
QtGui = __main__.QtGui

try:
    from scipy.signal import convolve2d
    SCIPY_AVAILABLE = True
except ImportError:
    SCIPY_AVAILABLE = False
    print("Warning: PhaseExplorerNode requires 'scipy'.")


# --- Core Physics Engine (from explorer.py) ---
class UniverseSimulator:
    def __init__(self, grid_size=64, params=None):
        self.grid_size = grid_size
        self.params = params
        self.phi = np.zeros((grid_size, grid_size), dtype=np.float32)
        self.phi_old = np.zeros_like(self.phi)
        self.lambda_coupling = self.params['lambda_coupling']
        self.vev_sq = self.params['vev']**2
        self.spin_force = self.params['spin_force']
        self.laplacian_kernel = np.array([[0, 1, 0], [1, -4, 1]], dtype=np.float32)
        self.singularity_threshold = 50.0
        self.heat_death_threshold = 0.1
        self._initialize_field()

    def _initialize_field(self):
        y, x = np.ogrid[:self.grid_size, :self.grid_size]
        cx1, cx2 = self.grid_size // 2 - 8, self.grid_size // 2 + 8
        cy = self.grid_size // 2
        radius = self.grid_size / 8.0
        self.phi = np.full_like(self.phi, self.params['vev'])
        self.phi += 2.0 * np.exp(-((x - cx1)**2 + (y - cy)**2) / (2 * radius**2))
        self.phi += -2.0 * np.exp(-((x - cx2)**2 + (y - cy)**2) / (2 * radius**2))
        self.phi_old = np.copy(self.phi)

    def _apply_spin_forces(self):
        if self.spin_force == 0: return 0
        grad_y, grad_x = np.gradient(self.phi)
        return (grad_y - grad_x) * self.spin_force

    def run(self, max_steps=400):
        for step in range(max_steps):
            potential_accel = self.lambda_coupling * self.phi * (self.phi**2 - self.vev_sq)
            lap_phi = convolve2d(self.phi, self.laplacian_kernel, 'same', 'wrap')
            spin_accel = self._apply_spin_forces()
            total_accel = -potential_accel + lap_phi + spin_accel
            
            velocity = self.phi - self.phi_old
            dt = 0.05
            phi_new = self.phi + (1.0 - 0.01*dt)*velocity + (dt**2)*total_accel
            self.phi_old, self.phi = self.phi, phi_new
            
            if np.max(np.abs(self.phi)) > self.singularity_threshold:
                return "SINGULARITY"
        
        if np.max(np.abs(self.phi - self.params['vev'])) < self.heat_death_threshold:
             return "HEAT DEATH"
        return "STABLE & COMPLEX"

# --- The Main Node Class ---

class PhaseExplorerNode(BaseNode):
    NODE_CATEGORY = "Analysis"
    NODE_COLOR = QtGui.QColor(220, 180, 40) # Golden
    
    def __init__(self, num_trials=500, grid_size=64):
        super().__init__()
        self.node_title = "Phase Explorer"
        
        self.inputs = {'trigger': 'signal'}
        self.outputs = {
            'phase_diagram': 'image',
            'status': 'signal'
        }
        
        if not SCIPY_AVAILABLE:
            self.node_title = "Explorer (No SciPy!)"
            return
            
        self.num_trials = int(num_trials)
        self.grid_size = int(grid_size)
        self.results = []
        
        self.param_space = {
            'lambda_coupling': (0.1, 2.0),
            'spin_force': (0.0, 0.8),
            'vev': (1.0, 1.0)
        }
        
        self.last_trigger = 0.0
        self.is_running = False
        self.progress = 0.0 # 0.0 to 1.0
        self.output_image = np.zeros((self.grid_size, self.grid_size, 3), dtype=np.uint8)
        self.thread = None

    def _exploration_thread(self):
        """Runs the heavy simulation in a separate thread."""
        self.is_running = True
        self.progress = 0.0
        self.results = []
        
        for i in range(self.num_trials):
            if not self.is_running: # Allow early exit
                break
                
            # 1. Randomly select laws
            trial_params = {
                'lambda_coupling': np.random.uniform(*self.param_space['lambda_coupling']),
                'spin_force': np.random.uniform(*self.param_space['spin_force']),
                'vev': np.random.uniform(*self.param_space['vev']),
            }
            
            # 2. Create and run universe
            simulator = UniverseSimulator(grid_size=self.grid_size, params=trial_params)
            outcome = simulator.run()

            # 3. Log the laws and outcome
            self.results.append({
                'params': trial_params,
                'outcome': outcome
            })
            
            # 4. Update progress
            self.progress = (i + 1) / self.num_trials
            
        # 5. When done, generate the plot
        if self.is_running: # Check if finished, not cancelled
            self.output_image = self._plot_phase_diagram()
            self.is_running = False

    def _plot_phase_diagram(self):
        """Draws the phase diagram onto a numpy array (OpenCV)."""
        w, h = 256, 256
        img = np.zeros((h, w, 3), dtype=np.uint8)
        
        if not self.results:
            return img
            
        lambda_vals = [r['params']['lambda_coupling'] for r in self.results]
        spin_vals = [r['params']['spin_force'] for r in self.results]
        
        color_map = {
            "STABLE & COMPLEX": (0, 215, 255), # Gold/Yellow (BGR)
            "SINGULARITY": (0, 0, 255),      # Red
            "HEAT DEATH": (10, 10, 10)       # Dark Gray
        }
        
        # Normalize coordinates to image size
        spin_min, spin_max = self.param_space['spin_force']
        lambda_min, lambda_max = self.param_space['lambda_coupling']
        
        for i, outcome in enumerate([r['outcome'] for r in self.results]):
            x = int( (spin_vals[i] - spin_min) / (spin_max - spin_min) * (w - 1) )
            y = int( (1.0 - (lambda_vals[i] - lambda_min) / (lambda_max - lambda_min)) * (h - 1) )
            
            color = color_map.get(outcome, (255, 255, 255))
            cv2.circle(img, (x, y), 2, color, -1)
            
        # Draw Goldilocks Zone box (approximate)
        x1 = int( (0.2 - spin_min) / (spin_max - spin_min) * (w - 1) )
        x2 = int( (0.5 - spin_min) / (spin_max - spin_min) * (w - 1) )
        y1 = int( (1.0 - (0.7 - lambda_min) / (lambda_max - lambda_min)) * (h - 1) )
        y2 = int( (1.0 - (0.2 - lambda_min) / (lambda_max - lambda_min)) * (h - 1) )
        cv2.rectangle(img, (x1, y1), (x2, y2), (0, 215, 255), 1)
        
        return img

    def step(self):
        if not SCIPY_AVAILABLE:
            return
            
        trigger_val = self.get_blended_input('trigger', 'sum') or 0.0
        
        # On rising edge, start the simulation thread
        if trigger_val > 0.5 and self.last_trigger <= 0.5:
            if not self.is_running:
                print("Starting Phase Exploration...")
                self.thread = threading.Thread(target=self._exploration_thread, daemon=True)
                self.thread.start()
            
        self.last_trigger = trigger_val

    def get_output(self, port_name):
        if port_name == 'phase_diagram':
            return self.output_image.astype(np.float32) / 255.0
        elif port_name == 'status':
            return self.progress
        return None
        
    def get_display_image(self):
        if self.is_running:
            # Show a progress bar
            w, h = 96, 96
            img = np.zeros((h, w, 3), dtype=np.uint8)
            progress_w = int(self.progress * w)
            cv2.rectangle(img, (0, h//2 - 10), (progress_w, h//2 + 10), (0, 255, 0), -1)
            cv2.putText(img, f"{(self.progress * 100):.0f}%", (w//2 - 15, h//2 + 5),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.4, (0, 0, 0), 1, cv2.LINE_AA)
            return QtGui.QImage(img.data, w, h, 3*w, QtGui.QImage.Format.Format_BGR888)
        else:
            # Show the final plot
            img_rgb = np.ascontiguousarray(self.output_image)
            h, w = img_rgb.shape[:2]
            if w == 0 or h == 0:
                 img_rgb = np.zeros((96, 96, 3), dtype=np.uint8)
                 h, w = 96, 96
                 cv2.putText(img_rgb, "Ready", (20, 45), 
                             cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)

            return QtGui.QImage(img_rgb.data, w, h, 3*w, QtGui.QImage.Format.Format_BGR888)

    def get_config_options(self):
        return [
            ("Num Trials", "num_trials", self.num_trials, None),
            ("Grid Size (NxN)", "grid_size", self.grid_size, None),
        ]
        
    def close(self):
        self.is_running = False # Signal thread to stop
        if self.thread is not None:
            self.thread.join(timeout=0.5)
        super().close()

=== FILE: phasefusionnode.py ===

"""
Phase Fusion Field Node - Merges two signals through quantum field dynamics
Creates coherent phase-locked oscillations from independent inputs via instanton-mediated coupling.
Place this file in the 'nodes' folder as 'phasefusionnode.py'
"""

import numpy as np
from PyQt6 import QtGui
import cv2

import __main__
BaseNode = __main__.BaseNode
PA_INSTANCE = getattr(__main__, "PA_INSTANCE", None)
QtGui = __main__.QtGui

try:
    from scipy.fft import fft, ifft, fftfreq
    SCIPY_AVAILABLE = True
except ImportError:
    SCIPY_AVAILABLE = False
    print("Warning: PhaseFusionNode requires scipy")

class PhaseFusionNode(BaseNode):
    NODE_CATEGORY = "Transform"
    NODE_COLOR = QtGui.QColor(120, 80, 200)  # Purple for quantum coupling
    
    def __init__(self, field_size=256, coupling_strength=0.01):
        super().__init__()
        self.node_title = "Phase Fusion Field"
        
        self.inputs = {
            'signal_a': 'signal',      # First signal to fuse
            'signal_b': 'signal',      # Second signal to fuse
            'coupling': 'signal',      # Control fusion strength
            'damping': 'signal'        # Control field dissipation
        }
        
        self.outputs = {
            'fused_output': 'signal',     # Phase-locked merged signal
            'coherence': 'signal',        # Phase coherence measure
            'field_image': 'image',       # Field amplitude visualization
            'phase_diff': 'signal'        # Phase difference between inputs
        }
        
        if not SCIPY_AVAILABLE:
            self.node_title = "Phase Fusion (No SciPy!)"
            return
        
        self.field_size = int(field_size)
        self.alpha = float(coupling_strength)  # Non-linear coupling (instanton strength)
        
        # Complex field state (instantons live here)
        self.field = np.zeros(self.field_size, dtype=np.complex128)
        self.field_prev = np.zeros_like(self.field)
        
        # Time evolution parameters
        self.dt = 0.01
        self.damping = 0.98
        
        # Frequency space (for fast Laplacian)
        k = fftfreq(self.field_size, 1.0) * 2 * np.pi
        self.k2 = k**2
        
        # Injection points for the two signals
        self.inject_a_pos = self.field_size // 4
        self.inject_b_pos = 3 * self.field_size // 4
        
        # Instanton tracking (peaks in the field)
        self.instantons = []
        
    def inject_signals(self, signal_a, signal_b, coupling_strength):
        """
        Inject two signals at different positions in the field.
        They will create localized excitations (instantons) that interact.
        """
        # Scale signals for field injection
        amp_a = signal_a * 0.5
        amp_b = signal_b * 0.5
        
        # Create complex injection (amplitude + phase)
        # The imaginary part allows phase information to propagate
        inject_a = amp_a * (1 + 1j)
        inject_b = amp_b * (1 + 1j)
        
        # Apply coupling strength
        inject_a *= coupling_strength
        inject_b *= coupling_strength
        
        # Inject at specified positions with Gaussian spread
        spread = 10
        x = np.arange(self.field_size)
        
        gaussian_a = np.exp(-((x - self.inject_a_pos)**2) / (2 * spread**2))
        gaussian_b = np.exp(-((x - self.inject_b_pos)**2) / (2 * spread**2))
        
        self.field += inject_a * gaussian_a
        self.field += inject_b * gaussian_b
    
    def evolve_field(self):
        """
        Evolve the field using a non-linear wave equation.
        The instanton dynamics come from the non-linear term that depends on field intensity.
        """
        # Transform to frequency space for fast Laplacian
        F = fft(self.field)
        laplacian = ifft(-self.k2 * F)
        
        # Non-linear term (instanton coupling)
        # This creates localized, stable structures (instantons)
        intensity = np.abs(self.field)**2
        nonlinear_factor = 1.0 / (1.0 + self.alpha * intensity)
        
        # Wave equation: dÂ²Ï/dtÂ² = âÂ²Ï / (1 + Î±|Ï|Â²)
        acceleration = laplacian * nonlinear_factor
        
        # Verlet integration
        new_field = 2 * self.field - self.field_prev + self.dt**2 * acceleration
        
        # Apply damping
        new_field *= self.damping
        
        # Update state
        self.field_prev[:] = self.field
        self.field[:] = new_field
    
    def detect_instantons(self):
        """
        Find peaks in the field amplitude (instantons are localized excitations)
        """
        amplitude = np.abs(self.field)
        
        # Simple peak detection
        peaks = []
        for i in range(1, len(amplitude) - 1):
            if amplitude[i] > amplitude[i-1] and amplitude[i] > amplitude[i+1]:
                if amplitude[i] > 0.1:  # Threshold
                    peaks.append(i)
        
        self.instantons = peaks
        return peaks
    
    def measure_coherence(self):
        """
        Measure phase coherence between the two injection regions.
        High coherence means the signals have phase-locked.
        """
        # Get phases at injection points
        phase_a = np.angle(self.field[self.inject_a_pos])
        phase_b = np.angle(self.field[self.inject_b_pos])
        
        # Phase difference
        phase_diff = np.abs(phase_a - phase_b)
        phase_diff = min(phase_diff, 2*np.pi - phase_diff)  # Wrap to [0, Ï]
        
        # Coherence: 1 when in-phase, 0 when out-of-phase
        coherence = 1.0 - (phase_diff / np.pi)
        
        return coherence, phase_diff
    
    def get_fused_signal(self):
        """
        Extract the merged signal from the middle of the field.
        This is where the two signals have propagated and interfered.
        """
        middle = self.field_size // 2
        
        # Average over a small region
        region = slice(middle - 5, middle + 5)
        fused_amplitude = np.mean(np.abs(self.field[region]))
        fused_phase = np.angle(np.mean(self.field[region]))
        
        # Convert to real signal
        fused = fused_amplitude * np.cos(fused_phase)
        
        return fused
    
    def step(self):
        if not SCIPY_AVAILABLE:
            return
        
        # Get inputs
        signal_a = self.get_blended_input('signal_a', 'sum') or 0.0
        signal_b = self.get_blended_input('signal_b', 'sum') or 0.0
        coupling_in = self.get_blended_input('coupling', 'sum')
        damping_in = self.get_blended_input('damping', 'sum')
        
        # Update parameters
        coupling_strength = coupling_in if coupling_in is not None else 1.0
        if coupling_in is not None:
            coupling_strength = 0.5 + coupling_in * 0.5  # Map to [0, 1]
        
        if damping_in is not None:
            self.damping = 0.95 + damping_in * 0.04  # Map to [0.95, 0.99]
        
        # Inject the two signals
        self.inject_signals(signal_a, signal_b, coupling_strength)
        
        # Evolve the field (instanton dynamics)
        self.evolve_field()
        
        # Detect instantons
        self.detect_instantons()
    
    def get_output(self, port_name):
        if port_name == 'fused_output':
            return self.get_fused_signal()
        
        elif port_name == 'coherence':
            coherence, _ = self.measure_coherence()
            return coherence
        
        elif port_name == 'phase_diff':
            _, phase_diff = self.measure_coherence()
            return phase_diff / np.pi  # Normalize to [0, 1]
        
        elif port_name == 'field_image':
            return self.generate_field_image()
        
        return None
    
    def generate_field_image(self):
        """Generate visualization of the field"""
        h = 64
        w = self.field_size
        
        # Create 2D image (amplitude and phase)
        amplitude = np.abs(self.field)
        phase = np.angle(self.field)
        
        # Normalize amplitude
        amp_norm = amplitude / (np.max(amplitude) + 1e-9)
        
        # Create image
        img = np.zeros((h, w), dtype=np.float32)
        
        # Draw amplitude as height
        for i in range(w):
            height = int(amp_norm[i] * (h - 1))
            img[h - height:, i] = amp_norm[i]
        
        return img
    
    def get_display_image(self):
        if not SCIPY_AVAILABLE:
            return None
        
        field_img = self.generate_field_image()
        img_u8 = (np.clip(field_img, 0, 1) * 255).astype(np.uint8)
        
        # Apply colormap
        img_color = cv2.applyColorMap(img_u8, cv2.COLORMAP_VIRIDIS)
        
        h, w = img_color.shape[:2]
        
        # Mark injection points
        inject_a_x = self.inject_a_pos * w // self.field_size
        inject_b_x = self.inject_b_pos * w // self.field_size
        
        cv2.circle(img_color, (inject_a_x, h - 5), 3, (255, 0, 0), -1)  # Red
        cv2.circle(img_color, (inject_b_x, h - 5), 3, (0, 255, 0), -1)  # Green
        
        # Mark instantons (field peaks)
        for inst_pos in self.instantons:
            inst_x = inst_pos * w // self.field_size
            cv2.circle(img_color, (inst_x, 10), 2, (255, 255, 255), -1)  # White
        
        # Mark fusion point (center)
        center_x = w // 2
        cv2.line(img_color, (center_x, 0), (center_x, h), (255, 255, 0), 1)  # Yellow
        
        # Resize for display
        img_resized = cv2.resize(img_color, (128, 64), interpolation=cv2.INTER_LINEAR)
        img_resized = np.ascontiguousarray(img_resized)
        
        h, w = img_resized.shape[:2]
        return QtGui.QImage(img_resized.data, w, h, 3*w, QtGui.QImage.Format.Format_BGR888)
    
    def get_config_options(self):
        return [
            ("Field Size", "field_size", self.field_size, None),
            ("Coupling Strength (Î±)", "alpha", self.alpha, None),
            ("Time Step (dt)", "dt", self.dt, None),
        ]

=== FILE: planck_engine.py ===

"""
Revolving Bit Simulator (Planck Engine) Node
Implements the core mathematics of the Revolving Bit Theory from bit-theory.py
- Fundamental Bits (Spinors `S`)
- Lagging Manifest Fields (Complex Scalar `Î¦`)
- Emergent motion and forces
Place this file in the 'nodes' folder
"""

import numpy as np
from PyQt6 import QtGui
import cv2

import sys
import os
# --- This is the new, correct block ---
import __main__
BaseNode = __main__.BaseNode
PA_INSTANCE = getattr(__main__, "PA_INSTANCE", None)
# ------------------------------------

# --- Pauli Matrices (NumPy version) ---
SIGMA_1 = np.array([[0, 1], [1, 0]], dtype=np.complex64) # sigma_x
SIGMA_2 = np.array([[0, -1j], [1j, 0]], dtype=np.complex64) # sigma_y
SIGMA_3 = np.array([[1, 0], [0, -1]], dtype=np.complex64) # sigma_z

class RevolvingBit:
    """ Represents a single fundamental Bit (Spinor S) """
    def __init__(self, initial_pos, omega_0, k1, k2, tau_dt, grid_size):
        self.pos = np.array(initial_pos, dtype=np.float32)
        self.velocity = np.zeros(2, dtype=np.float32)
        self.S = np.array([1.0 + 0j, 0.0 + 0j], dtype=np.complex64)
        self.tau = 0.0
        self.grid_size = grid_size
        
        # Store constants
        self.omega_0 = omega_0
        self.k1 = k1
        self.k2 = k2
        self.tau_dt = tau_dt

    def normalize_S(self):
        norm_S_sq = np.sum(np.abs(self.S)**2)
        if norm_S_sq > 1e-9:
            self.S /= np.sqrt(norm_S_sq)

    def revolve_step(self, external_phi_field_at_pos):
        """ Intrinsic revolution + interaction with external Phi field """
        V_spinor = (self.k1 * external_phi_field_at_pos.real * SIGMA_1 +
                    self.k2 * external_phi_field_at_pos.imag * SIGMA_2)
        
        H_spinor = self.omega_0 * SIGMA_3 + V_spinor
        
        dS = -1j * np.dot(H_spinor, self.S) * self.tau_dt
        self.S += dS
        self.normalize_S()
        self.tau += self.tau_dt

    def move_step(self, force_gradient, attraction_gamma, dt):
        """ Move based on gradients in the total Phi field """
        acceleration = attraction_gamma * force_gradient
        self.velocity += acceleration * dt
        self.velocity *= 0.98 # Damping
        self.pos += self.velocity * dt
        self.pos %= self.grid_size # Wrap around grid

class RevolvingBitNode(BaseNode):
    NODE_CATEGORY = "Source"
    NODE_COLOR = QtGui.QColor(100, 150, 200) # A "quantum" blue
    
    def __init__(self, grid_size=64, num_bits=3):
        super().__init__()
        self.node_title = "Planck Engine"
        
        self.inputs = {'coupling': 'signal', 'attraction': 'signal'}
        self.outputs = {
            'field_amp': 'image', 
            'field_phase': 'image', 
            'avg_amp': 'signal'
        }
        
        self.N = int(grid_size)
        
        # --- Physics Parameters from bit-theory.py ---
        self.DT = 0.01
        self.TAU_DT = 0.05
        self.C_SUBSTRATE = 1.0
        self.FIELD_MASS = 0.1
        self.OMEGA_0 = 1.0
        
        # Controllable params
        self.bit_field_coupling_g = 0.5
        self.spinor_potential_k1 = 0.1
        self.spinor_potential_k2 = 0.1
        self.attraction_gamma = 0.2
        
        # --- Internal State ---
        self.phi = np.zeros((self.N, self.N), dtype=np.complex64)
        self.phi_prev = self.phi.copy()
        
        self.bits = []
        for i in range(int(num_bits)):
            pos = np.random.uniform(self.N * 0.2, self.N * 0.8, 2)
            self.bits.append(RevolvingBit(
                pos, self.OMEGA_0, self.spinor_potential_k1, 
                self.spinor_potential_k2, self.TAU_DT, self.N
            ))
            
        # Precompute grid for field generation
        x_coords = np.arange(self.N, dtype=np.float32)
        self.X_grid, self.Y_grid = np.meshgrid(x_coords, x_coords, indexing='ij')

    def _laplacian_2d(self, grid):
        return (np.roll(grid, 1, axis=0) + np.roll(grid, -1, axis=0) +
                np.roll(grid, 1, axis=1) + np.roll(grid, -1, axis=1) - 4 * grid)

    def _get_field_at_pos(self, bit, field_to_sample):
        """ Interpolate field value at a Bit's continuous position """
        x_idx = int(round(bit.pos[0])) % self.N
        y_idx = int(round(bit.pos[1])) % self.N
        return field_to_sample[x_idx, y_idx]

    def _get_gradient_at_pos(self, bit, field_mag):
        """ Estimate gradient of field magnitude at Bit's position """
        x = int(round(bit.pos[0]))
        y = int(round(bit.pos[1]))

        grad_x = (field_mag[(x + 1) % self.N, y % self.N] - 
                    field_mag[(x - 1) % self.N, y % self.N]) / 2.0
        grad_y = (field_mag[x % self.N, (y + 1) % self.N] - 
                    field_mag[x % self.N, (y - 1) % self.N]) / 2.0
        return np.array([grad_x, grad_y], dtype=np.float32)

    def step(self):
        # Update params from inputs
        self.bit_field_coupling_g = (self.get_blended_input('coupling', 'sum') or 0.0) * 0.5 + 0.5 # [0, 1]
        self.attraction_gamma = (self.get_blended_input('attraction', 'sum') or 0.0) * 0.2 + 0.2 # [0, 0.4]
        
        # --- 1. Evolve each Bit's internal spinor state S ---
        for bit in self.bits:
            phi_ext = self._get_field_at_pos(bit, self.phi)
            bit.revolve_step(phi_ext)

        # --- 2. Update the Manifest Field Phi based on ALL Bits ---
        source_term = np.zeros_like(self.phi)
        
        for bit in self.bits:
            dist_sq = (self.X_grid - bit.pos[0])**2 + (self.Y_grid - bit.pos[1])**2
            source_spread_sigma_sq = 4.0 # 2.0**2
            bit_source_profile = np.exp(-dist_sq / (2 * source_spread_sigma_sq))
            
            # Source is the complex spinor component S[0]
            source_term += self.bit_field_coupling_g * bit.S[0] * bit_source_profile

        # Evolve Phi field (Klein-Gordon)
        lap_phi = self._laplacian_2d(self.phi)
        
        phi_new = (2 * self.phi - self.phi_prev +
                   self.C_SUBSTRATE**2 * self.DT**2 * (lap_phi - self.FIELD_MASS**2 * self.phi + source_term))
        
        self.phi_prev = self.phi.copy()
        self.phi = phi_new
        
        # --- 3. Move each Bit based on the TOTAL Phi field ---
        phi_magnitude_field = np.abs(self.phi)
        for bit in self.bits:
            grad_phi_mag_at_pos = self._get_gradient_at_pos(bit, phi_magnitude_field)
            bit.move_step(grad_phi_mag_at_pos, self.attraction_gamma, self.DT)

    def get_output(self, port_name):
        mag = np.abs(self.phi)
        vmax = mag.max() + 1e-9
        
        if port_name == 'field_amp':
            return mag / vmax
        elif port_name == 'field_phase':
            return (np.angle(self.phi) + np.pi) / (2 * np.pi) # [0, 1]
        elif port_name == 'avg_amp':
            return np.mean(mag)
        return None
        
    def get_display_image(self):
        mag = np.abs(self.phi)
        vmax = mag.max() + 1e-9
        
        # Normalize amplitude and apply MAGMA colormap
        img_norm = (mag / vmax * 255).astype(np.uint8)
        img_color = cv2.applyColorMap(img_norm, cv2.COLORMAP_MAGMA)
        
        # Draw bits
        for bit in self.bits:
            # (y, x) for cv2 drawing
            x_pos = int(round(bit.pos[1])) % self.N 
            y_pos = int(round(bit.pos[0])) % self.N
            cv2.circle(img_color, (x_pos, y_pos), 3, (0, 255, 255), -1) # Cyan bits
            
        img_color = np.ascontiguousarray(img_color)
        h, w = img_color.shape[:2]
        return QtGui.QImage(img_color.data, w, h, 3*w, QtGui.QImage.Format.Format_BGR888)

    def get_config_options(self):
        return [
            ("Grid Size", "N", self.N, None),
            ("Num Bits", "num_bits", len(self.bits), None),
        ]

=== FILE: quantumwavenode.py ===

"""
Quantum Wave Node - A PyTorch-based simulator for a 2D quantum wave function.
Implements the time-dependent SchrÃ¶dinger equation (free particle).
Place this file in the 'nodes' folder
Requires: pip install torch numpy
"""

import numpy as np
from PyQt6 import QtGui
import cv2
import torch

import __main__
BaseNode = __main__.BaseNode
PA_INSTANCE = getattr(__main__, "PA_INSTANCE", None)
QtGui = __main__.QtGui

# Use GPU if available
try:
    DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
except Exception:
    DEVICE = torch.device("cpu")

# Simulation parameters (natural units: â = 1, mass = 1)
LX, LY = 10.0, 10.0
DT = 1e-3  # Time step

class QuantumWaveNode(BaseNode):
    NODE_CATEGORY = "Source"
    NODE_COLOR = QtGui.QColor(100, 150, 255) # Complex Blue
    
    def __init__(self, resolution=128, k_momentum=5.0, steps_per_frame=10):
        super().__init__()
        self.node_title = "Quantum Wave"
        
        # Inputs allow external control over the simulation speed or initial state
        self.inputs = {
            'momentum_x': 'signal', # Control k0x
            'reset': 'signal'
        }
        self.outputs = {
            'image': 'image',        # Probability density |Ï|Â²
            'total_prob': 'signal'   # Should always be 1.0 (Normalization check)
        }
        
        self.Nx = self.Ny = int(resolution)
        self.k0x = float(k_momentum)
        self.k0y = 0.0
        self.steps_per_frame = int(steps_per_frame)
        
        self.dx = LX / self.Nx
        self.dy = LY / self.Ny
        
        # Internal state
        self.psi = None
        self.initialize_wavefunction()
        
    def normalize(self, psi):
        """Normalize the wavefunction (PyTorch version)"""
        norm = torch.sqrt(torch.sum(torch.abs(psi)**2) * self.dx * self.dy)
        if norm.item() > 1e-9:
            return psi / norm
        return psi # Return original if norm is zero/near-zero

    def laplacian(self, psi):
        """Precompute the Laplacian operator with periodic boundaries"""
        dx, dy = self.dx, self.dy
        
        psi_roll_x_forward = torch.roll(psi, shifts=-1, dims=0)
        psi_roll_x_backward = torch.roll(psi, shifts=1, dims=0)
        psi_roll_y_forward = torch.roll(psi, shifts=-1, dims=1)
        psi_roll_y_backward = torch.roll(psi, shifts=1, dims=1)
        
        lap = (psi_roll_x_forward + psi_roll_x_backward - 2*psi) / (dx**2) \
              + (psi_roll_y_forward + psi_roll_y_backward - 2*psi) / (dy**2)
        return lap

    def evolve(self, psi, dt):
        """Time evolution using the Euler method: âÏ/ât = -i/2 âÂ²Ï"""
        dpsi_dt = -1j * 0.5 * self.laplacian(psi)
        psi_new = psi + dpsi_dt * dt
        psi_new = self.normalize(psi_new)
        return psi_new

    def initialize_wavefunction(self):
        """Define the initial state (Gaussian wave packet with momentum)"""
        x = torch.linspace(-LX/2, LX/2, self.Nx, device=DEVICE)
        y = torch.linspace(-LY/2, LY/2, self.Ny, device=DEVICE)
        X, Y = torch.meshgrid(x, y, indexing='ij')

        x0, y0 = 0.0, 0.0         # Center of the packet
        sigma = 1.0               # Width of the packet
        
        # Create a real-valued Gaussian envelope
        envelope = torch.exp(-((X - x0)**2 + (Y - y0)**2) / (2 * sigma**2))
        
        # Add a complex phase for momentum
        phase = torch.exp(1j * (self.k0x * X + self.k0y * Y))
        psi0 = envelope * phase

        self.psi = self.normalize(psi0).type(torch.complex64)
        
    def randomize(self):
        """Called by 'R' button - restart simulation"""
        self.initialize_wavefunction()

    def step(self):
        # Update parameters from inputs
        mom_in = self.get_blended_input('momentum_x', 'sum')
        if mom_in is not None:
            # FIX: Handle both scalar and array inputs
            if hasattr(mom_in, '__len__'):  # Is array-like
                new_k0x = float(np.mean(mom_in)) * 10.0
            else:  # Is scalar
                new_k0x = float(mom_in) * 10.0
            
            if abs(new_k0x - self.k0x) > 1.0:
                self.k0x = new_k0x
                self.initialize_wavefunction()
            
        # Check for reset signal
        reset_sig = self.get_blended_input('reset', 'sum')
        if reset_sig is not None and reset_sig > 0.5:
            self.initialize_wavefunction()

        # Perform time steps
        for _ in range(self.steps_per_frame):
            self.psi = self.evolve(self.psi, DT)
            
        # Calculate output metric
        self.total_probability = torch.sum(torch.abs(self.psi)**2 * self.dx * self.dy).item()

    def get_output(self, port_name):
        if port_name == 'image':
            # Output probability density: |Ï|Â²
            prob_density_np = torch.abs(self.psi).pow(2).cpu().numpy()
            
            # Normalize to [0, 1]
            max_val = np.max(prob_density_np)
            if max_val > 1e-9:
                return prob_density_np / max_val
            return prob_density_np
            
        elif port_name == 'total_prob':
            # Should be ~1.0
            return self.total_probability
        return None
        
    def get_display_image(self):
        # Get the density image
        prob_density = self.get_output('image')
        if prob_density is None:
            return None
            
        # Resize for display thumbnail (64x64) and convert to RGB (viridis-like)
        img_u8 = (prob_density * 255).astype(np.uint8)
        
        # Apply colormap (viridis)
        img_color = cv2.applyColorMap(img_u8, cv2.COLORMAP_VIRIDIS)
        
        # Resize to thumbnail size
        img_resized = cv2.resize(img_color, (96, 96), interpolation=cv2.INTER_LINEAR)
        img_resized = np.ascontiguousarray(img_resized)
        
        h, w = img_resized.shape[:2]
        return QtGui.QImage(img_resized.data, w, h, 3*w, QtGui.QImage.Format.Format_BGR888)

    def get_config_options(self):
        return [
            ("Resolution (NxN)", "resolution", self.Nx, None),
            ("Initial Momentum (k0x)", "k0x", self.k0x, None),
            ("Steps per Frame", "steps_per_frame", self.steps_per_frame, None),
        ]


=== FILE: reaction_diffusion_node.py ===

"""
Reaction-Diffusion Node - Simulates Gray-Scott pattern formation
Place this file in the 'nodes' folder
"""

import numpy as np
from PyQt6 import QtGui
import cv2

import sys
import os
# --- This is the new, correct block ---
import __main__
BaseNode = __main__.BaseNode
PA_INSTANCE = getattr(__main__, "PA_INSTANCE", None)
# ------------------------------------

class ReactionDiffusionNode(BaseNode):
    NODE_CATEGORY = "Transform"
    NODE_COLOR = QtGui.QColor(200, 80, 200) # A wild, purple-ish color
    
    def __init__(self, width=128, height=96):
        super().__init__()
        self.node_title = "Reaction-Diffusion"
        self.inputs = {
            'seed_image': 'image',
            'feed_rate': 'signal',
            'kill_rate': 'signal'
        }
        self.outputs = {'image': 'image', 'signal': 'signal'}
        
        self.w, self.h = width, height
        
        # Gray-Scott parameters
        self.f = 0.055  # Feed rate
        self.k = 0.062  # Kill rate
        self.dA = 1.0   # Diffusion rate A
        self.dB = 0.5   # Diffusion rate B
        
        # Chemical concentrations
        # A (U) is the "substrate", B (V) is the "reactant"
        self.A = np.ones((self.h, self.w), dtype=np.float32)
        self.B = np.zeros((self.h, self.w), dtype=np.float32)
        
        # Seed the reaction
        self.seed_chemicals(self.w//2, self.h//2, 10)
        
        # Laplacian kernel for diffusion
        self.laplacian_kernel = np.array([[0.05, 0.2, 0.05],
                                          [0.2, -1.0, 0.2],
                                          [0.05, 0.2, 0.05]], dtype=np.float32)

    def seed_chemicals(self, x, y, size):
        self.B[y-size:y+size, x-size:x+size] = 1.0

    def step(self):
        # Get parameters from inputs, or use defaults
        # Map signal range [0, 1] to a good parameter range
        f_in = self.get_blended_input('feed_rate', 'sum')
        k_in = self.get_blended_input('kill_rate', 'sum')
        
        if f_in is not None:
            self.f = np.clip(0.01 + f_in * 0.09, 0.01, 0.1) # map [0,1] to [0.01, 0.1]
        if k_in is not None:
            self.k = np.clip(0.045 + k_in * 0.025, 0.045, 0.07) # map [0,1] to [0.045, 0.07]

        # Use an input image to "paint" chemical B
        img_in = self.get_blended_input('seed_image', 'mean')
        if img_in is not None:
            img_resized = cv2.resize(img_in, (self.w, self.h))
            self.B[img_resized > 0.5] = 1.0
            
        # Run 5 simulation steps per frame for speed
        for _ in range(5):
            # Calculate diffusion using convolution
            laplace_A = cv2.filter2D(self.A, -1, self.laplacian_kernel)
            laplace_B = cv2.filter2D(self.B, -1, self.laplacian_kernel)
            
            # The reaction part
            reaction = self.A * self.B**2
            
            # Gray-Scott equations
            delta_A = (self.dA * laplace_A) - reaction + (self.f * (1 - self.A))
            delta_B = (self.dB * laplace_B) + reaction - ((self.k + self.f) * self.B)
            
            # Update chemicals
            self.A += delta_A
            self.B += delta_B
            
            # Clamp values
            self.A = np.clip(self.A, 0.0, 1.0)
            self.B = np.clip(self.B, 0.0, 1.0)

    def get_output(self, port_name):
        if port_name == 'image':
            # We visualize chemical B, which forms the patterns
            return self.B
        elif port_name == 'signal':
            # Output the mean concentration of B
            return np.mean(self.B)
        return None
        
    def get_display_image(self):
        # Display chemical B
        img_u8 = (np.clip(self.B, 0, 1) * 255).astype(np.uint8)
        img_u8 = np.ascontiguousarray(img_u8)
        return QtGui.QImage(img_u8.data, self.w, self.h, self.w, QtGui.QImage.Format.Format_Grayscale8)

    def get_config_options(self):
        return [
            ("Feed Rate (f)", "f", self.f, None),
            ("Kill Rate (k)", "k", self.k, None),
        ]

=== FILE: realvaenode.py ===

"""
Real VAE Node - (v3 - Fixed External Latent Decoding)
Trains incrementally on webcam, allows latent space exploration

Requires: pip install torch torchvision
Place this file in the 'nodes' folder as realvaenode.py

FIX v3:
- The step() function will now correctly check for 'latent_in'
  even if 'image_in' is not connected.
- This allows a "decoder-only" VAE node to work,
  just as you intended in your graph.
"""

import numpy as np
import cv2

# --- CRITICAL IMPORT BLOCK ---
import __main__
BaseNode = __main__.BaseNode
QtGui = __main__.QtGui
# -----------------------------

try:
    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False
    print("Warning: RealVAENode requires PyTorch")
    print("Install with: pip install torch torchvision")


class ConvVAE(nn.Module):
    """Convolutional Variational Autoencoder"""
    def __init__(self, latent_dim=16, img_size=64):
        super().__init__()
        self.latent_dim = latent_dim
        self.img_size = img_size
        
        # Encoder: 64x64 -> 16D latent
        self.encoder = nn.Sequential(
            nn.Conv2d(1, 32, 4, 2, 1),   # 64 -> 32
            nn.ReLU(),
            nn.Conv2d(32, 64, 4, 2, 1),  # 32 -> 16
            nn.ReLU(),
            nn.Conv2d(64, 128, 4, 2, 1), # 16 -> 8
            nn.ReLU(),
            nn.Conv2d(128, 256, 4, 2, 1), # 8 -> 4
            nn.ReLU(),
            nn.Flatten(),
        )
        
        # Latent space
        hidden_dim = 256 * 4 * 4
        self.fc_mu = nn.Linear(hidden_dim, latent_dim)
        self.fc_logvar = nn.Linear(hidden_dim, latent_dim)
        
        # Decoder: 16D latent -> 64x64
        self.fc_decode = nn.Linear(latent_dim, hidden_dim)
        
        self.decoder = nn.Sequential(
            nn.ConvTranspose2d(256, 128, 4, 2, 1), # 4 -> 8
            nn.ReLU(),
            nn.ConvTranspose2d(128, 64, 4, 2, 1),  # 8 -> 16
            nn.ReLU(),
            nn.ConvTranspose2d(64, 32, 4, 2, 1),   # 16 -> 32
            nn.ReLU(),
            nn.ConvTranspose2d(32, 1, 4, 2, 1),    # 32 -> 64
            nn.Sigmoid()
        )
        
    def encode(self, x):
        h = self.encoder(x)
        mu = self.fc_mu(h)
        logvar = self.fc_logvar(h)
        return mu, logvar
    
    def reparameterize(self, mu, logvar):
        std = torch.exp(0.5 * logvar)
        eps = torch.randn_like(std)
        return mu + eps * std
    
    def decode(self, z):
        h = self.fc_decode(z)
        h = h.view(-1, 256, 4, 4)
        return self.decoder(h)
    
    def forward(self, x):
        mu, logvar = self.encode(x)
        z = self.reparameterize(mu, logvar)
        recon = self.decode(z)
        return recon, mu, logvar


class RealVAENode(BaseNode):
    """
    Real Variational Autoencoder - learns visual compression
    """
    NODE_CATEGORY = "AI / Physics"
    NODE_COLOR = QtGui.QColor(180, 100, 220)
    
    def __init__(self, latent_dim=16, img_size=64):
        super().__init__()
        self.node_title = "Real VAE"
        
        self.inputs = {
            'image_in': 'image',
            'latent_in': 'spectrum',
            'train': 'signal',
            'reset': 'signal'
        }
        self.outputs = {
            'image_out': 'image',
            'latent_out': 'spectrum',
            'loss': 'signal'
        }
        
        if not TORCH_AVAILABLE:
            self.node_title = "Real VAE (NO TORCH!)"
            return
        
        self.latent_dim = int(latent_dim)
        self.img_size = int(img_size)
        
        # Setup device
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"RealVAENode: Using device: {self.device}")
        
        # Create model
        self.model = ConvVAE(self.latent_dim, self.img_size).to(self.device)
        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=1e-3)
        
        # State
        self.current_latent = np.zeros(self.latent_dim, dtype=np.float32)
        self.reconstructed = np.zeros((self.img_size, self.img_size), dtype=np.float32)
        self.current_loss = 0.0
        self.training_steps = 0
        
    def vae_loss(self, recon, x, mu, logvar):
        """VAE loss: reconstruction + KL divergence"""
        recon_loss = F.mse_loss(recon, x, reduction='sum')
        kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())
        return recon_loss + 0.1 * kl_loss
    
    def step(self):
        if not TORCH_AVAILABLE:
            return
        
        # Get all inputs
        img_in = self.get_blended_input('image_in', 'mean')
        train_signal = self.get_blended_input('train', 'sum') or 0.0
        reset_signal = self.get_blended_input('reset', 'sum') or 0.0
        external_latent = self.get_blended_input('latent_in', 'first')
        
        # --- FIX: Check for any activity at all ---
        has_image = img_in is not None
        has_external_latent = external_latent is not None
        
        if not has_image and not has_external_latent:
            self.reconstructed *= 0.95 # Fade out
            return
        # --- END FIX ---
        
        # Reset training
        if reset_signal > 0.5:
            print("RealVAENode: Resetting training...")
            self.model = ConvVAE(self.latent_dim, self.img_size).to(self.device)
            self.optimizer = torch.optim.Adam(self.model.parameters(), lr=1e-3)
            self.training_steps = 0
        
        
        # --- Handle Training & Latent Output (Requires Image) ---
        if has_image:
            # Prepare image
            img = cv2.resize(img_in, (self.img_size, self.img_size))
            if img.ndim == 3:
                img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
            img = img.astype(np.float32)
            if img.max() > 1.0:
                img = img / 255.0
            
            # Convert to torch tensor
            x = torch.from_numpy(img).unsqueeze(0).unsqueeze(0).to(self.device)
        
            if train_signal > 0.5:
                self.model.train()
                self.optimizer.zero_grad()
                
                recon, mu, logvar = self.model(x)
                loss = self.vae_loss(recon, x, mu, logvar)
                
                loss.backward()
                self.optimizer.step()
                
                self.current_loss = loss.item()
                self.training_steps += 1
                
                if self.training_steps % 50 == 0:
                    print(f"VAE Step {self.training_steps}, Loss: {self.current_loss:.2f}")

            # ALWAYS encode to update the latent_out port
            self.model.eval()
            with torch.no_grad():
                mu, logvar = self.model.encode(x)
                self.current_latent = mu.cpu().numpy().flatten().astype(np.float32)
        
        
        # --- Handle Decoding (Image Output) ---
        self.model.eval()
        z_to_decode = None
        
        if has_external_latent:
            # Priority: Use the external latent vector if plugged in
            if len(external_latent) == self.latent_dim:
                z_to_decode = torch.from_numpy(external_latent).float().unsqueeze(0).to(self.device)
        
        elif has_image:
            # Fallback: Use the live latent vector from the image
            z_to_decode = torch.from_numpy(self.current_latent).float().unsqueeze(0).to(self.device)
            
        # Run decoder
        if z_to_decode is not None:
            with torch.no_grad():
                recon = self.model.decode(z_to_decode)
                self.reconstructed = recon.squeeze().cpu().numpy()
        else:
            self.reconstructed *= 0.95 # Fade out
    
    def get_output(self, port_name):
        if port_name == 'latent_out':
            return self.current_latent
        elif port_name == 'image_out':
            return self.reconstructed
        elif port_name == 'loss':
            # Scale loss to a more reasonable 0-1 signal range
            return np.clip(self.current_loss / 10000.0, 0.0, 1.0)
        return None
    
    def get_display_image(self):
        if not TORCH_AVAILABLE:
            img = np.zeros((128, 128, 3), dtype=np.uint8)
            cv2.putText(img, "PyTorch not installed", (10, 64),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255, 0, 0), 1)
            return QtGui.QImage(img.data, 128, 128, 128*3, QtGui.QImage.Format.Format_RGB888)
        
        # Display the reconstructed image
        img = (np.clip(self.reconstructed, 0, 1) * 255).astype(np.uint8)
        img = cv2.resize(img, (256, 256))
        
        status = f"Steps: {self.training_steps}"
        loss_text = f"Loss: {self.current_loss:.1f}"
        
        cv2.putText(img, status, (5, 15), cv2.FONT_HERSHEY_SIMPLEX,
                   0.4, (255, 255, 255), 1)
        cv2.putText(img, loss_text, (5, 35), cv2.FONT_HERSHEY_SIMPLEX,
                   0.4, (255, 255, 255), 1)
        
        device_text = "GPU" if self.device.type == 'cuda' else "CPU"
        cv2.putText(img, device_text, (5, 250), cv2.FONT_HERSHEY_SIMPLEX,
                   0.3, (0, 255, 0) if self.device.type == 'cuda' else (255, 255, 0), 1)
        
        return QtGui.QImage(img.data, 256, 256, 256, QtGui.QImage.Format.Format_Grayscale8)
    
    def get_config_options(self):
        return [
            ("Latent Dim", "latent_dim", self.latent_dim, None),
            ("Image Size", "img_size", self.img_size, None)
        ]

    def close(self):
        # Clean up torch model
        if hasattr(self, 'model') and self.model is not None:
            del self.model
            if TORCH_AVAILABLE and torch.cuda.is_available():
                torch.cuda.empty_cache()
        super().close()

=== FILE: resonant_instanton_node.py ===

"""
ResonantInstantonNode - Simulates self-resonant instanton fields for atomic structures.
Based on instantonassim x.py, modeling atoms as field lumps with intrinsic resonances.
Place this file in the 'nodes' folder as 'resonant_instanton_node.py'
"""

import numpy as np
from PyQt6 import QtGui
import cv2
from scipy.ndimage import gaussian_filter

import __main__
BaseNode = __main__.BaseNode
PA_INSTANCE = getattr(__main__, "PA_INSTANCE", None)
QtGui = __main__.QtGui

class ResonantInstantonNode(BaseNode):
    NODE_CATEGORY = "Simulation"
    NODE_COLOR = QtGui.QColor(100, 50, 200)  # Quantum purple

    def __init__(self, grid_size=96, dt=0.05, c=1.0, a=0.1, b=0.1, gamma=0.02, substrate_noise=0.0005):
        super().__init__()
        self.node_title = "Resonant Instanton"

        self.inputs = {
            'atomic_number': 'signal',  # Input to set atomic number (scaled, e.g., 1-100)
            'stable_isotope': 'signal',  # >0.5 for stable, else unstable
            'perturbation': 'signal',  # External noise or nudge to field
            'reset': 'signal'  # >0.5 to reinitialize
        }

        self.outputs = {
            'field_image': 'image',  # 96x96 float32 of phi field
            'stability': 'signal',  # Stability metric (0-1)
            'instanton_count': 'signal',  # Cumulative instanton events
            'decay_event': 'signal'  # 1 if decay occurred this step, else 0
        }

        self.grid_size = grid_size
        self.dt = float(dt)
        self.c = float(c)
        self.a = float(a)
        self.b = float(b)
        self.gamma = float(gamma)
        self.substrate_noise = float(substrate_noise)

        # Field state
        self.phi = np.zeros((grid_size, grid_size))
        self.phi_prev = np.zeros((grid_size, grid_size))

        # Tracking
        self.mode_energies = []
        self.resonance_peaks = []
        self.instanton_density = np.zeros((grid_size, grid_size))
        self.instanton_count = 0
        self.instanton_events = []
        self.stability_metric = 1.0

        # Time
        self.time = 0.0
        self.frame_count = 0

        # Default atom
        self.current_atomic_number = 2  # Helium
        self.current_stable_isotope = True
        self.initialize_atom(self.current_atomic_number, stable_isotope=self.current_stable_isotope)

    def initialize_atom(self, atomic_number, position=None, stable_isotope=True):
        if position is None:
            position = (self.grid_size // 2, self.grid_size // 2)

        # Clear state
        self.phi.fill(0)
        self.phi_prev.fill(0)
        self.instanton_density.fill(0)
        self.instanton_count = 0
        self.instanton_events = []
        self.stability_metric = 1.0

        # Core radius
        core_radius = 4 + np.log(1 + atomic_number)

        # Core amplitude
        core_amplitude = 1.0 + 0.2 * atomic_number

        # Meshgrid
        x, y = np.meshgrid(np.arange(self.grid_size), np.arange(self.grid_size))
        r = np.sqrt((x - position[0])**2 + (y - position[1])**2)

        # Nuclear core
        self.phi = core_amplitude * np.exp(-r**2 / (2 * core_radius**2))

        # Shell config
        shell_config = self._calculate_shell_configuration(atomic_number)

        # Add shells
        for shell, electrons in enumerate(shell_config):
            if electrons > 0:
                shell_radius = self._shell_radius(shell + 1)
                shell_amplitude = 0.3 * (electrons / (2 * (2 * shell + 1)**2))
                shell_wave = shell_amplitude * np.cos(np.pi * r / shell_radius)**2 * (r < 2 * shell_radius)
                self.phi += shell_wave

        # Isotope variation
        if not stable_isotope:
            asymmetry = 0.1 * np.sin(3 * np.arctan2(y - position[1], x - position[0]))
            self.phi += asymmetry * np.exp(-r**2 / (2 * core_radius**2))
            self.stability_metric = 0.7 + 0.3 * np.random.random()

        self.phi_prev = self.phi.copy()
        self.time = 0.0
        self.frame_count = 0
        self.mode_energies = []
        self._analyze_resonant_modes()

    def _calculate_shell_configuration(self, atomic_number):
        shell_capacity = [2, 8, 18, 32, 50]
        shells = []
        electrons_left = atomic_number
        for capacity in shell_capacity:
            if electrons_left >= capacity:
                shells.append(capacity)
                electrons_left -= capacity
            else:
                shells.append(electrons_left)
                electrons_left = 0
                break
        while electrons_left > 0:
            next_capacity = 2 * (len(shells) + 1)**2
            if electrons_left >= next_capacity:
                shells.append(next_capacity)
                electrons_left -= next_capacity
            else:
                shells.append(electrons_left)
                break
        return shells

    def _shell_radius(self, n):
        base_radius = 8
        return base_radius * n**2

    def _laplacian(self, field):
        laplacian = np.zeros_like(field)
        field_padded = np.pad(field, 1, mode='wrap')
        laplacian = (field_padded[:-2, 1:-1] + field_padded[2:, 1:-1] +
                     field_padded[1:-1, :-2] + field_padded[1:-1, 2:] -
                     4 * field_padded[1:-1, 1:-1])
        return laplacian

    def _biharmonic(self, field):
        return self._laplacian(self._laplacian(field))

    def _analyze_resonant_modes(self):
        center = self.grid_size // 2
        x = np.arange(self.grid_size) - center
        y = np.arange(self.grid_size) - center
        X, Y = np.meshgrid(x, y)
        R = np.sqrt(X**2 + Y**2)
        r_values = np.arange(0, self.grid_size // 2)
        radial_avg = np.zeros_like(r_values, dtype=float)
        for i, r in enumerate(r_values):
            mask = (R >= r - 0.5) & (R < r + 0.5)
            if np.sum(mask) > 0:
                radial_avg[i] = np.mean(self.phi[mask])
        peaks = []
        for i in range(1, len(radial_avg) - 1):
            if radial_avg[i] > radial_avg[i-1] and radial_avg[i] > radial_avg[i+1] and radial_avg[i] > 0.05:
                peaks.append((i, radial_avg[i]))
        self.resonance_peaks = peaks

    def _detect_instanton_event(self, phi_old, phi_new):
        delta_phi = phi_new - phi_old
        delta_phi_smoothed = gaussian_filter(delta_phi, sigma=1.0)
        threshold = 0.1 * np.max(np.abs(self.phi))
        significant_changes = np.abs(delta_phi_smoothed) > threshold
        if np.any(significant_changes):
            y_indices, x_indices = np.where(significant_changes)
            if len(x_indices) > 0:
                center_x = np.mean(x_indices)
                center_y = np.mean(y_indices)
                magnitude = np.max(np.abs(delta_phi_smoothed))
                self.instanton_count += 1
                self.instanton_events.append({
                    'time': self.time,
                    'position': (center_x, center_y),
                    'magnitude': magnitude
                })
                x, y = np.meshgrid(np.arange(self.grid_size), np.arange(self.grid_size))
                r = np.sqrt((x - center_x)**2 + (y - center_y)**2)
                self.instanton_density += 0.2 * np.exp(-r**2 / 50)
                return True
        return False

    def _update_stability(self):
        if len(self.instanton_events) > 0:
            recent_count = sum(1 for event in self.instanton_events if event['time'] > self.time - 100 * self.dt)
            if recent_count > 5:
                self.stability_metric -= 0.01
            else:
                self.stability_metric = min(1.0, self.stability_metric + 0.001)
        self.stability_metric = max(0.0, min(1.0, self.stability_metric))

    def step(self):
        # Get inputs
        atomic_number_in = self.get_blended_input('atomic_number', 'sum')
        stable_isotope_in = self.get_blended_input('stable_isotope', 'sum')
        perturbation_in = self.get_blended_input('perturbation', 'sum') or 0.0
        reset_in = self.get_blended_input('reset', 'sum') or 0.0

        # Handle reset or param changes
        if reset_in > 0.5 or atomic_number_in is not None or stable_isotope_in is not None:
            if atomic_number_in is not None:
                self.current_atomic_number = max(1, int(1 + atomic_number_in * 100))  # Scale to 1-101
            if stable_isotope_in is not None:
                self.current_stable_isotope = stable_isotope_in > 0.5
            self.initialize_atom(self.current_atomic_number, stable_isotope=self.current_stable_isotope)

        # Save old phi
        phi_old = self.phi.copy()

        # Compute terms
        laplacian_phi = self._laplacian(self.phi)
        biharmonic_phi = self._biharmonic(self.phi) if self.gamma != 0 else 0
        noise = self.substrate_noise * np.random.normal(size=self.phi.shape) + perturbation_in * 0.1  # Add input perturbation

        accel = (self.c**2 * laplacian_phi +
                 self.a * self.phi -
                 self.b * self.phi**3 -
                 self.gamma * biharmonic_phi +
                 noise)

        # Verlet update
        phi_new = 2 * self.phi - self.phi_prev + self.dt**2 * accel
        self.phi_prev = self.phi
        self.phi = phi_new

        # Detect instanton
        self._detect_instanton_event(phi_old, self.phi)

        # Update stability
        self._update_stability()

        # Analyze modes every 50 frames
        if self.frame_count % 50 == 0:
            self._analyze_resonant_modes()
            energy = np.sum(self.phi**2)
            self.mode_energies.append((self.time, energy))

        # Decay check
        self.decay_event = 0
        decay_probability = (1.0 - self.stability_metric)**2 * 0.001
        if np.random.random() < decay_probability:
            self.decay_event = 1

        # Update time
        self.time += self.dt
        self.frame_count += 1

    def get_output(self, port_name):
        if port_name == 'field_image':
            # Normalize phi to [0,1] for image
            phi_norm = (self.phi - np.min(self.phi)) / (np.max(self.phi) - np.min(self.phi) + 1e-9)
            return phi_norm.astype(np.float32)
        elif port_name == 'stability':
            return self.stability_metric
        elif port_name == 'instanton_count':
            return self.instanton_count / 100.0  # Scaled for signal
        elif port_name == 'decay_event':
            return self.decay_event
        return None

    def get_display_image(self):
        # Render colored field with overlays
        phi_norm = (self.phi - np.min(self.phi)) / (np.max(self.phi) - np.min(self.phi) + 1e-9)
        img_u8 = (phi_norm * 255).astype(np.uint8)
        img_color = cv2.applyColorMap(img_u8, cv2.COLORMAP_COOL)

        # Overlay instanton density
        if np.max(self.instanton_density) > 0:
            inst_norm = (self.instanton_density / np.max(self.instanton_density) * 255).astype(np.uint8)
            inst_color = cv2.applyColorMap(inst_norm, cv2.COLORMAP_HOT)
            img_color = cv2.addWeighted(img_color, 0.7, inst_color, 0.3, 0)

        # Add text overlays (simulated, since no plt here)
        cv2.putText(img_color, f"Stab: {self.stability_metric:.2f}", (5, 15), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255, 255, 255), 1)
        cv2.putText(img_color, f"Inst: {self.instanton_count}", (5, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255, 255, 255), 1)

        # Shell circles
        center = self.grid_size // 2
        for r, _ in self.resonance_peaks:
            cv2.circle(img_color, (center, center), int(r), (255, 255, 255), 1, lineType=cv2.LINE_AA)

        img_color = np.ascontiguousarray(img_color)
        h, w = img_color.shape[:2]
        return QtGui.QImage(img_color.data, w, h, 3 * w, QtGui.QImage.Format.Format_BGR888)

    def get_config_options(self):
        return [
            ("Grid Size", "grid_size", self.grid_size, None),
            ("Time Step (dt)", "dt", self.dt, None),
            ("Wave Speed (c)", "c", self.c, None),
            ("Linear Term (a)", "a", self.a, None),
            ("Nonlinear Term (b)", "b", self.b, None),
            ("Biharmonic (gamma)", "gamma", self.gamma, None),
            ("Substrate Noise", "substrate_noise", self.substrate_noise, None),
        ]

=== FILE: signal_numerical_output.py ===

"""
Signal Display Node - Displays a live numerical value
Place this file in the 'nodes' folder
"""

import numpy as np
from PyQt6 import QtGui
from PIL import Image, ImageDraw, ImageFont
import os

# --- !! CRITICAL IMPORT BLOCK !! ---
# This is the *only* correct way to import BaseNode and shared resources.
import __main__
BaseNode = __main__.BaseNode
QtGui = __main__.QtGui
PA_INSTANCE = getattr(__main__, "PA_INSTANCE", None)
# -----------------------------------

class SignalDisplayNode(BaseNode):
    NODE_CATEGORY = "Output"
    NODE_COLOR = QtGui.QColor(120, 40, 120) # Output Purple
    
    def __init__(self):
        super().__init__()
        self.node_title = "Signal Display"
        
        # Define ports
        self.inputs = {'signal': 'signal'}  # port_name: port_type
        self.outputs = {} # No outputs for this node
        
        # Internal state
        self.current_value = 0.0
        
        # Try to load a font
        try:
            self.font = ImageFont.load_default(size=14)
        except IOError:
            print("Warning: Default PIL font not found. Display text may be small.")
            self.font = None

    def step(self):
        """Called every frame - main processing logic"""
        # Get input data using 'sum' to handle multiple inputs
        input_val = self.get_blended_input('signal', 'sum')
        
        if input_val is not None:
            self.current_value = input_val
        else:
            # Gently decay to 0 if no signal is present
            self.current_value *= 0.95
        
    def get_output(self, port_name):
        """This node has no outputs"""
        return None
        
    def get_display_image(self):
        """Return a QImage for node preview"""
        w, h = 64, 32  # A smaller, wider display for text
        
        # Create a black background image
        img = np.zeros((h, w, 3), dtype=np.uint8)
        img_pil = Image.fromarray(img)
        draw = ImageDraw.Draw(img_pil)
        
        # Format the text
        text = f"{self.current_value:.3f}"
        
        # Determine text color based on value
        if self.current_value > 0.01:
            text_color = (100, 255, 100) # Green
        elif self.current_value < -0.01:
            text_color = (255, 100, 100) # Red
        else:
            text_color = (200, 200, 200) # Gray
            
        # Calculate text position to center it
        bbox = draw.textbbox((0, 0), text, font=self.font)
        text_w = bbox[2] - bbox[0]
        text_h = bbox[3] - bbox[1]
        x = (w - text_w) / 2
        y = (h - text_h) / 2
        
        # Draw the text
        draw.text((x, y), text, fill=text_color, font=self.font)
        
        # Convert back to QImage
        img_final = np.array(img_pil)
        img_final = np.ascontiguousarray(img_final)
        return QtGui.QImage(img_final.data, w, h, 3*w, QtGui.QImage.Format.Format_RGB888)

    def get_config_options(self):
        # No configuration options for this simple node
        return []

=== FILE: signal_processor.py ===

"""
Signal Processor Node - Applies various filters to a signal
Place this file in the 'nodes' folder
"""

import numpy as np
from PyQt6 import QtGui

import sys
import os
# --- This is the new, correct block ---
import __main__
BaseNode = __main__.BaseNode
PA_INSTANCE = getattr(__main__, "PA_INSTANCE", None)
# ------------------------------------

class SignalProcessorNode(BaseNode):
    NODE_CATEGORY = "Transform"
    NODE_COLOR = QtGui.QColor(220, 120, 40) # Transform Orange
    
    def __init__(self, processing_mode='smoothing', factor=0.1):
        super().__init__()
        self.node_title = "Signal Processor"
        self.inputs = {'input_signal': 'signal'}
        self.outputs = {'output_signal': 'signal'}
        
        self.processing_mode = processing_mode
        self.factor = float(factor)
        self.last_input = 0.0
        self.integrated_state = 0.0
        self.processed_output = 0.0
        
    def step(self):
        u = self.get_blended_input('input_signal', 'sum') or 0.0
        
        output = u
        
        if self.processing_mode == 'smoothing':
            alpha = np.clip(self.factor, 0.0, 1.0) # Smoothing factor
            self.processed_output = self.processed_output * (1.0 - alpha) + u * alpha
            output = self.processed_output
            
        elif self.processing_mode == 'differentiation':
            # Factor acts as sensitivity (1/dt)
            output = (u - self.last_input) * (1.0 / max(self.factor, 1e-6)) 
            self.processed_output = output
            
        elif self.processing_mode == 'integration':
            # Factor acts as decay speed
            decay = np.clip(1.0 - self.factor * 0.1, 0.9, 1.0) 
            self.integrated_state = self.integrated_state * decay + u * 0.05
            output = self.integrated_state
            self.processed_output = output
            
        elif self.processing_mode == 'high_pass':
            # 1st order IIR high-pass. Factor is (1-alpha)
            alpha = np.clip(1.0 - self.factor, 0.01, 0.99)
            self.processed_output = alpha * (self.processed_output + u - self.last_input)
            output = self.processed_output

        elif self.processing_mode == 'full_wave_rectify':
            # Factor is unused
            output = np.abs(u)
            self.processed_output = output

        elif self.processing_mode == 'tanh_distortion':
            # Factor acts as gain/drive
            gain = max(self.factor, 1e-6)
            output = np.tanh(u * gain)
            self.processed_output = output

        self.last_input = u
        
    def get_output(self, port_name):
        if port_name == 'output_signal':
            return self.processed_output
        return None
        
    def get_display_image(self):
        w, h = 64, 64
        img = np.zeros((h, w), dtype=np.uint8)
        
        # Simple bar display of the processed output
        v = np.clip(self.processed_output, -1.0, 1.0)
        bar_height = int((v + 1.0) / 2.0 * h)
        
        img[h - bar_height:, w//2 - 2 : w//2 + 2] = 255
        img[h//2 - 1 : h//2 + 1, :] = 80 # Center line

        img = np.ascontiguousarray(img)
        return QtGui.QImage(img.data, w, h, w, QtGui.QImage.Format.Format_Grayscale8)

    def get_config_options(self):
        return [
            ("Mode", "processing_mode", self.processing_mode, [
                ("Smoothing (EMA)", "smoothing"), 
                ("Differentiation", "differentiation"),
                ("Integration (Decay)", "integration"),
                ("High-Pass Filter", "high_pass"),
                ("Full Wave Rectify", "full_wave_rectify"),
                ("Tanh Distortion", "tanh_distortion")
            ]),
            ("Factor", "factor", self.factor, None)
        ]

=== FILE: singlepulsenode.py ===

"""
Single Pulse Node - Outputs a signal of 1.0 for exactly one frame
when the user presses the R-button on the node.
"""

import numpy as np
from PyQt6 import QtGui
from PIL import Image, ImageDraw, ImageFont
import sys
import os

import __main__
BaseNode = __main__.BaseNode
PA_INSTANCE = getattr(__main__, "PA_INSTANCE", None)

class SinglePulseNode(BaseNode):
    NODE_CATEGORY = "Source"  # Changed to Source because it now generates the input
    NODE_COLOR = QtGui.QColor(255, 120, 0) # Pulse Orange
    
    def __init__(self):
        super().__init__()
        self.node_title = "Pulse Trigger (R-Button)"
        
        # --- MODIFIED: No input port needed ---
        self.inputs = {}
        self.outputs = {'pulse_out': 'signal'}
        
        self.output_pulse = 0.0
        
        # Flag controlled by the manual R-button press
        self.manual_pulse_flag = False 
        self.frames_since_pulse = 0
        
        try:
            self.font = ImageFont.load_default()
        except IOError:
            self.font = None 

    def randomize(self):
        """
        This method is called when the user presses the 'R' button on the node.
        It sets the flag to trigger a pulse on the next step().
        """
        self.manual_pulse_flag = True
        
    def step(self):
        # 1. Check if the manual button was pressed (flag is True)
        if self.manual_pulse_flag:
            self.output_pulse = 1.0 # Send pulse for this frame
            self.frames_since_pulse = 0
            self.manual_pulse_flag = False # Reset the flag immediately
        
        # 2. If a pulse was sent last frame, ensure it returns to 0.0 now
        elif self.output_pulse > 0.0:
            self.output_pulse = 0.0
            self.frames_since_pulse += 1
        
        else:
            self.frames_since_pulse += 1

    def get_output(self, port_name):
        if port_name == 'pulse_out':
            return self.output_pulse
        return None
        
    def get_display_image(self):
        w, h = 96, 32 # Increased size for better text display
        img = np.zeros((h, w), dtype=np.uint8)
        
        # Show pulse state
        if self.output_pulse == 1.0:
            img.fill(255)
            text = "PULSE!"
            fill_color = 0
        else:
            text = "Click R to Pulse"
            fill_color = 255
            
        img_pil = Image.fromarray(img)
        draw = ImageDraw.Draw(img_pil)
        
        font_to_use = self.font if self.font else ImageFont.load_default()
            
        draw.text((w//8, h//4), text, fill=fill_color, font=font_to_use)
        
        img = np.array(img_pil)
        img = np.ascontiguousarray(img)
        return QtGui.QImage(img.data, w, h, w, QtGui.QImage.Format.Format_Grayscale8)

    def get_config_options(self):
        return []


=== FILE: space_screensaver.py ===

"""
Space Screensaver Node - A 3D tensor universe simulation
Ported from the SpaceScreensaver.py script.
Requires: pip install torch scipy
Place this file in the 'nodes' folder
"""

import numpy as np
import cv2
import sys
import os

# --- This is the new, correct block ---
import __main__
BaseNode = __main__.BaseNode
PA_INSTANCE = getattr(__main__, "PA_INSTANCE", None)
QtGui = __main__.QtGui # <--- THIS IS THE FIX
# ------------------------------------

# --- Dependency Checks ---
try:
    import torch
    from scipy.ndimage import label
    LIBS_AVAILABLE = True
except ImportError:
    LIBS_AVAILABLE = False
    print("Warning: SpaceScreensaverNode requires 'torch' and 'scipy'.")
    print("Please run: pip install torch scipy")

# --- Color Map Dictionary ---
# Maps string names to OpenCV colormap constants
CMAP_DICT = {
    "gray": None, # Special case for no colormap
    "viridis": cv2.COLORMAP_VIRIDIS,
    "plasma": cv2.COLORMAP_PLASMA,
    "inferno": cv2.COLORMAP_INFERNO,
    "magma": cv2.COLORMAP_MAGMA,
    "cividis": cv2.COLORMAP_CIVIDIS,
    "hot": cv2.COLORMAP_HOT,
    "jet": cv2.COLORMAP_JET
}

# --- Core Simulation Classes (from SpaceScreensaver.py) ---
# These are helper classes, placed inside the node file for portability

class PhysicalTensorSingularity:
    def __init__(self, dimension=128, position=None, mass=1.0, device='cpu'):
        self.dimension = dimension
        self.device = device
        # Physical properties
        if position is not None:
            if isinstance(position, np.ndarray):
                self.position = torch.from_numpy(position).float().to(self.device)
            else:
                self.position = position.clone().detach().float().to(self.device)
        else:
            self.position = torch.tensor(np.random.rand(3), dtype=torch.float32, device=self.device)
        self.velocity = torch.randn(3, device=self.device) * 0.1
        self.mass = mass
        # Tensor properties
        self.core = torch.randn(dimension, device=self.device)
        self.field = self.generate_gravitational_field()

    def generate_gravitational_field(self):
        field = self.core.clone()
        r = torch.linspace(0, 2 * np.pi, self.dimension, device=self.device)
        field *= torch.exp(-r / self.mass)
        return field

    def update_position(self, dt, force):
        acceleration = force / self.mass
        self.velocity += acceleration * dt
        self.position += self.velocity * dt

class PhysicalTensorUniverse:
    def __init__(self, size=50, num_singularities=100, dimension=128, device='cpu'):
        self.G = 6.67430e-11  # Gravitational constant
        self.size = size
        self.dimension = dimension
        self.device = device
        self.space = torch.zeros((size, size, size), device=self.device)
        self.singularities = []
        self.initialize_singularities(num_singularities)

    def initialize_singularities(self, num):
        """Initialize singularities with random positions and masses"""
        self.singularities = []  # Reset list
        for _ in range(num):
            position = torch.tensor(np.random.rand(3) * self.size, dtype=torch.float32, device=self.device)
            mass = torch.distributions.Exponential(1.0).sample().item()
            self.singularities.append(
                PhysicalTensorSingularity(
                    dimension=self.dimension,
                    position=position,
                    mass=mass,
                    device=self.device
                )
            )

    def update_tensor_interactions(self):
        """Update tensor field interactions using vectorized operations"""
        if not self.singularities:
            return
            
        positions = torch.stack([s.position for s in self.singularities])
        masses = torch.tensor([s.mass for s in self.singularities], device=self.device)

        delta = positions.unsqueeze(1) - positions.unsqueeze(0)
        distance = torch.norm(delta, dim=2) + 1e-10
        force_magnitude = self.G * masses.unsqueeze(1) * masses.unsqueeze(0) / (distance ** 2)
        force_direction = delta / (distance.unsqueeze(2) + 1e-10)
        
        # Zero out self-interaction
        force_magnitude.fill_diagonal_(0)
        
        force = torch.sum(force_magnitude.unsqueeze(2) * force_direction, dim=1)

        fields = torch.stack([s.field for s in self.singularities])
        field_interaction = torch.tanh(torch.matmul(fields, fields.T))
        force *= (1 + torch.mean(field_interaction, dim=1)).unsqueeze(1)

        for i, singularity in enumerate(self.singularities):
            singularity.update_position(dt=0.1, force=force[i])

    def update_space(self):
        """Update 3D space based on singularity positions and fields"""
        self.space.fill_(0)
        x = torch.linspace(0, self.size-1, self.size, device=self.device)
        y = torch.linspace(0, self.size-1, self.size, device=self.device)
        z = torch.linspace(0, self.size-1, self.size, device=self.device)
        X, Y, Z = torch.meshgrid(x, y, z, indexing='ij')

        for s in self.singularities:
            R = torch.sqrt((X - s.position[0]) ** 2 +
                          (Y - s.position[1]) ** 2 +
                          (Z - s.position[2]) ** 2)
            self.space += s.mass / (R + 1) * torch.mean(s.field)

# --- The Main Node Class ---

class SpaceScreensaverNode(BaseNode):
    NODE_CATEGORY = "Source"
    NODE_COLOR = QtGui.QColor(50, 80, 160) # Deep space blue
    
    def __init__(self, universe_size=48, num_singularities=100, color_scheme='plasma'):
        super().__init__()
        self.node_title = "Space Screensaver"
        
        self.inputs = {'reset': 'signal'}
        self.outputs = {'image': 'image', 'total_mass': 'signal'}
        
        if not LIBS_AVAILABLE:
            self.node_title = "Space (Libs Missing!)"
            return
            
        self.universe_size = int(universe_size)
        self.num_singularities = int(num_singularities)
        self.color_scheme = str(color_scheme)
        
        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'
        
        # Initialize simulation
        self.simulation = PhysicalTensorUniverse(
            size=self.universe_size,
            num_singularities=self.num_singularities,
            device=self.device
        )
        
        self.output_image_data = np.zeros((self.universe_size, self.universe_size), dtype=np.float32)
        self.total_mass = 0.0

    def randomize(self):
        """Called by 'R' button - re-initializes the simulation"""
        if LIBS_AVAILABLE:
            self.simulation.initialize_singularities(self.num_singularities)
            
    def _get_density_slice(self):
        """Internal helper to get a 2D slice from the 3D sim"""
        if not LIBS_AVAILABLE:
            return
            
        # Get the middle slice on the Z axis
        slice_index = self.universe_size // 2
        density_slice = self.simulation.space[:, :, slice_index].cpu().numpy()

        # Normalize the density slice for visualization
        min_v, max_v = density_slice.min(), density_slice.max()
        range_v = max_v - min_v
        if range_v > 1e-9:
            self.output_image_data = (density_slice - min_v) / range_v
        else:
            self.output_image_data.fill(0.0)

    def step(self):
        if not LIBS_AVAILABLE:
            return
            
        # Check for reset signal
        reset_sig = self.get_blended_input('reset', 'sum')
        if reset_sig is not None and reset_sig > 0.5:
            self.randomize()
            
        # Run simulation steps
        self.simulation.update_tensor_interactions()
        self.simulation.update_space()
        
        # Get 2D image data
        self._get_density_slice()
        
        # Get metrics
        self.total_mass = float(torch.sum(self.simulation.space).item())

    def get_output(self, port_name):
        if port_name == 'image':
            return self.output_image_data
        elif port_name == 'total_mass':
            return self.total_mass
        return None
        
    def get_display_image(self):
        if not LIBS_AVAILABLE:
            return None
            
        img_u8 = (np.clip(self.output_image_data, 0, 1) * 255).astype(np.uint8)
        
        # Apply the selected colormap
        cmap_cv2 = CMAP_DICT.get(self.color_scheme)
        
        if cmap_cv2 is not None:
            # Apply CV2 colormap and resize
            img_color = cv2.applyColorMap(img_u8, cmap_cv2)
            img_resized = cv2.resize(img_color, (96, 96), interpolation=cv2.INTER_LINEAR)
            img_resized = np.ascontiguousarray(img_resized)
            h, w = img_resized.shape[:2]
            return QtGui.QImage(img_resized.data, w, h, 3*w, QtGui.QImage.Format.Format_BGR888)
        else:
            # Just resize (for 'gray')
            img_resized = cv2.resize(img_u8, (96, 96), interpolation=cv2.INTER_LINEAR)
            img_resized = np.ascontiguousarray(img_resized)
            h, w = img_resized.shape
            return QtGui.QImage(img_resized.data, w, h, w, QtGui.QImage.Format.Format_Grayscale8)


    def get_config_options(self):
        if not LIBS_AVAILABLE:
            return [("Error", "error", "PyTorch or SciPy not found!", [])]
            
        # Create color scheme options for the dropdown
        color_options = [(name.title(), name) for name in CMAP_DICT.keys()]
        
        return [
            ("Universe Size (3D)", "universe_size", self.universe_size, None),
            ("Num Singularities", "num_singularities", self.num_singularities, None),
            ("Color Scheme", "color_scheme", self.color_scheme, color_options),
        ]

=== FILE: space_simulator.py ===

"""
Space Simulator Node - Simulates a 2D particle universe
Place this file in the 'nodes' folder
"""

import numpy as np
from PyQt6 import QtGui
import cv2

import sys
import os
# --- This is the new, correct block ---
import __main__
BaseNode = __main__.BaseNode
PA_INSTANCE = getattr(__main__, "PA_INSTANCE", None)
# ------------------------------------

class SpaceSimulatorNode(BaseNode):
    NODE_CATEGORY = "Source"
    NODE_COLOR = QtGui.QColor(50, 80, 160) # Deep space blue
    
    def __init__(self, particle_count=200, width=160, height=120):
        super().__init__()
        self.node_title = "Space Simulator"
        self.outputs = {'image': 'image', 'signal': 'signal'}
        
        self.w, self.h = width, height
        self.particle_count = int(particle_count)
        
        # Particle state
        self.positions = np.random.rand(self.particle_count, 2).astype(np.float32) * [self.w, self.h]
        self.velocities = (np.random.rand(self.particle_count, 2).astype(np.float32) - 0.5) * 2.0
        
        # The "density" image
        self.space = np.zeros((self.h, self.w), dtype=np.float32)
        
        self.time = 0.0

    def step(self):
        self.time += 0.01
        
        # Central attractor
        attractor_pos = np.array([
            self.w / 2 + np.sin(self.time * 0.5) * self.w * 0.3,
            self.h / 2 + np.cos(self.time * 0.3) * self.h * 0.3
        ])
        
        # Calculate forces (simple gravity)
        to_attractor = attractor_pos - self.positions
        dist_sq = np.sum(to_attractor**2, axis=1, keepdims=True) + 1e-3
        force = to_attractor / dist_sq * 5.0 # Gravity strength
        
        # Update velocities
        self.velocities += force * 0.1 # dt
        self.velocities *= 0.98 # Damping
        
        # Update positions
        self.positions += self.velocities
        
        # Bounce off walls
        mask_x_low = self.positions[:, 0] < 0
        mask_x_high = self.positions[:, 0] >= self.w
        mask_y_low = self.positions[:, 1] < 0
        mask_y_high = self.positions[:, 1] >= self.h
        
        self.positions[mask_x_low, 0] = 0
        self.positions[mask_x_high, 0] = self.w - 1
        self.positions[mask_y_low, 1] = 0
        self.positions[mask_y_high, 1] = self.h - 1
        
        self.velocities[mask_x_low | mask_x_high, 0] *= -0.5
        self.velocities[mask_y_low | mask_y_high, 1] *= -0.5

        # Update the density image
        self.space *= 0.9 # Fade old trails
        
        # Get integer positions
        int_pos = self.positions.astype(int)
        
        # Valid coordinates
        valid = (int_pos[:, 0] >= 0) & (int_pos[:, 0] < self.w) & \
                (int_pos[:, 1] >= 0) & (int_pos[:, 1] < self.h)
        
        valid_pos = int_pos[valid]
        
        # "Splat" particles onto the image
        if valid_pos.shape[0] > 0:
            self.space[valid_pos[:, 1], valid_pos[:, 0]] = 1.0 # Bright points
        
        # Blur to make it look like a density field
        display_img = cv2.GaussianBlur(self.space, (5, 5), 0)
        self.display_img = display_img

    def get_output(self, port_name):
        if port_name == 'image':
            return self.display_img
        elif port_name == 'signal':
            # Output mean velocity as a signal
            return np.mean(np.linalg.norm(self.velocities, axis=1))
        return None
        
    def get_display_image(self):
        img_u8 = (np.clip(self.display_img, 0, 1) * 255).astype(np.uint8)
        img_u8 = np.ascontiguousarray(img_u8)
        return QtGui.QImage(img_u8.data, self.w, self.h, self.w, QtGui.QImage.Format.Format_Grayscale8)

    def get_config_options(self):
        return [
            ("Particle Count", "particle_count", self.particle_count, None)
        ]

=== FILE: speaker_output.py ===

"""
Speaker Output Node - Outputs audio to speakers/headphones
** REBUILT **
This version uses a non-blocking callback and synthesizes a
sine wave, using the input signals for frequency and amplitude.
Place this file in the 'nodes' folder
"""

import numpy as np
from PyQt6 import QtGui
import pyaudio
import sys
import os

# --- This is the new, correct block ---
import __main__
BaseNode = __main__.BaseNode
PA_INSTANCE = getattr(__main__, "PA_INSTANCE", None)
# ------------------------------------


class SpeakerOutputNode(BaseNode):
    NODE_CATEGORY = "Output"
    NODE_COLOR = QtGui.QColor(120, 40, 120) 
    
    def __init__(self, sample_rate=44100, device_index=None):
        super().__init__()
        self.node_title = "Speaker (Synth)"
        # FIX: Inputs are now 'frequency' and 'amplitude'
        self.inputs = {'frequency': 'signal', 'amplitude': 'signal'}
        
        self.pa = PA_INSTANCE
        self.sample_rate = int(sample_rate)
        self.device_index = device_index
        self.stream = None
        
        # Synthesis parameters
        self.current_freq = 440.0 # A4
        self.current_amp = 0.0
        self.phase = 0.0
        
        # Store last values for interpolation
        self._last_amp = 0.0
        self._last_freq = 440.0
        
        if not self.pa:
            self.node_title = "Speaker (NO PA)"
            return
        
        if self.device_index is None:
            try:
                self.device_index = self.pa.get_default_output_device_info()['index']
            except Exception:
                self.device_index = -1 
        
        self.open_stream()
        
    def _audio_callback(self, in_data, frame_count, time_info, status):
        """This is called by a separate audio thread"""
        
        # Get smooth ramps for parameters
        target_freq = self.current_freq
        target_amp = self.current_amp
        
        # Simple linear interpolation for smoothing
        amp_ramp = np.linspace(self._last_amp, target_amp, frame_count, dtype=np.float32)
        freq_ramp = np.linspace(self._last_freq, target_freq, frame_count, dtype=np.float32)
        
        # Calculate phase increments
        phase_inc = (2 * np.pi * freq_ramp) / self.sample_rate
        
        # Generate audio buffer
        phase_buffer = np.cumsum(phase_inc) + self.phase
        audio_buffer = (np.sin(phase_buffer) * amp_ramp).astype(np.float32)
        
        # Store last state for next buffer
        self.phase = phase_buffer[-1] % (2 * np.pi)
        self._last_amp = target_amp
        self._last_freq = target_freq
        
        # Convert to 16-bit int
        audio_int = np.clip(audio_buffer * 32767.0, -32768, 32767).astype(np.int16)
        
        return (audio_int.tobytes(), pyaudio.paContinue)
        
    def open_stream(self):
        """Opens or re-opens the PyAudio stream."""
        if self.stream: 
            try: self.stream.stop_stream(); self.stream.close()
            except Exception: pass
            
        if not self.pa or self.device_index < 0:
            return
            
        # Store last values for interpolation
        self._last_amp = self.current_amp
        self._last_freq = self.current_freq
            
        try:
            self.stream = self.pa.open(
                format=pyaudio.paInt16,
                channels=1,
                rate=self.sample_rate,
                output=True,
                output_device_index=self.device_index,
                frames_per_buffer=256,
                stream_callback=self._audio_callback
            )
            self.stream.start_stream()
            try:
                device_name = self.pa.get_device_info_by_index(self.device_index)['name']
                self.node_title = f"Speaker ({device_name[:15]}...)"
            except:
                self.node_title = "Speaker (Active)"
            
        except Exception as e:
            print(f"Error opening audio stream: {e}")
            self.stream = None
            self.node_title = "Speaker (ERROR)"
            
    def step(self):
        # This runs at the SIMULATION frame rate
        
        # --- FIX: Receive Freq/Amp and use them directly ---
        freq_in = self.get_blended_input('frequency', 'sum')
        amp_in = self.get_blended_input('amplitude', 'sum')
        
        # The input signal is assumed to be the correct, calculated Hertz value
        self.current_freq = freq_in if freq_in is not None else 0.0
        
        if amp_in is None:
            self.current_amp = 0.0 # Default to silence if amp is disconnected
        else:
            # Map amplitude signal [0, 1] to a safe volume range [0, 0.5]
            self.current_amp = np.clip(amp_in * 0.5, 0.0, 0.5)
        
        # Ensure minimum frequency for stability
        if self.current_freq < 10.0 and self.current_freq > 0.0:
            self.current_freq = 10.0
        # --- END FIX ---

    def get_display_image(self):
        w, h = 64, 64
        img = np.zeros((h, w), dtype=np.uint8)
        
        # Draw amplitude bar
        amp_h = int(np.clip(self.current_amp * 2.0, 0, 1) * h)
        img[h - amp_h:, :w//2] = 255
        
        # Draw frequency bar
        # Normalize the frequency display based on the expected range (100 to 1000 Hz)
        freq_h = int(np.clip((self.current_freq - 100) / 900, 0, 1) * h)
        img[h - freq_h:, w//2:] = 180 

        img = np.ascontiguousarray(img)
        return QtGui.QImage(img.data, w, h, w, QtGui.QImage.Format.Format_Grayscale8)
        
    def get_config_options(self):
        if not self.pa:
            return [("PyAudio Not Found", "error", "Install PyAudio", [])]
            
        devices = []
        for i in range(self.pa.get_device_count()):
            try:
                info = self.pa.get_device_info_by_index(i)
                if info['max_output_channels'] > 0:
                    devices.append((f"Selected Device ({self.device_index})", self.device_index))
            except Exception:
                continue 
            
        return [
            ("Output Device", "device_index", self.device_index, devices),
            ("Sample Rate", "sample_rate", self.sample_rate, None)
        ]
        
    def close(self):
        if self.stream:
            try: self.stream.stop_stream(); self.stream.close()
            except Exception: pass
        super().close()

=== FILE: spectrum_analyzer_node.py ===

"""
Spectrum Analyzer Node - Splits an FFT spectrum into discrete bands
Place this file in the 'nodes/ folder
"""

import numpy as np
from PyQt6 import QtGui
import cv2

import __main__
BaseNode = __main__.BaseNode
PA_INSTANCE = getattr(__main__, "PA_INSTANCE", None)

class SpectrumAnalyzerNode(BaseNode):
    NODE_CATEGORY = "Transform"
    NODE_COLOR = QtGui.QColor(220, 120, 40) # Transform Orange
    
    def __init__(self, low_split=0.1, high_split=0.5):
        super().__init__()
        self.node_title = "Spectrum Analyzer"
        
        self.inputs = {'spectrum_in': 'spectrum'}
        self.outputs = {
            'bass': 'signal',
            'mids': 'signal',
            'high': 'signal'
        }
        
        self.low_split = float(low_split)  # 10% mark
        self.high_split = float(high_split) # 50% mark
        
        self.bass = 0.0
        self.mids = 0.0
        self.high = 0.0
        
        self.vis_img = np.zeros((64, 64, 3), dtype=np.uint8)

    def step(self):
        # get_blended_input will use 'mean' for array types like 'spectrum'
        spectrum = self.get_blended_input('spectrum_in', 'mean') 
        
        if spectrum is None or len(spectrum) == 0:
            self.bass *= 0.9
            self.mids *= 0.9
            self.high *= 0.9
            return
            
        spec_len = len(spectrum)
        low_idx = int(spec_len * self.low_split)
        high_idx = int(spec_len * self.high_split)
        
        # Calculate mean power in each band
        self.bass = np.mean(spectrum[0 : low_idx])
        self.mids = np.mean(spectrum[low_idx : high_idx])
        self.high = np.mean(spectrum[high_idx :])
        
        # Normalize (signals are often very small)
        total = self.bass + self.mids + self.high + 1e-9
        self.bass /= total
        self.mids /= total
        self.high /= total
        
        # Update visualization
        self.vis_img.fill(0)
        cv2.rectangle(self.vis_img, (0, 63 - int(self.bass * 63)), (20, 63), (0, 0, 255), -1)
        cv2.rectangle(self.vis_img, (22, 63 - int(self.mids * 63)), (42, 63), (0, 255, 0), -1)
        cv2.rectangle(self.vis_img, (44, 63 - int(self.high * 63)), (63, 63), (255, 0, 0), -1)

    def get_output(self, port_name):
        if port_name == 'bass':
            return self.bass
        elif port_name == 'mids':
            return self.mids
        elif port_name == 'high':
            return self.high
        return None

    # --- THIS IS THE FIX ---
    def get_display_image(self):
    # --- END FIX ---
        img = np.ascontiguousarray(self.vis_img)
        return QtGui.QImage(img.data, 64, 64, 64*3, QtGui.QImage.Format.Format_BGR888)

    def get_config_options(self):
        return [
            ("Bass/Mid Split (0-1)", "low_split", self.low_split, None),
            ("Mid/High Split (0-1)", "high_split", self.high_split, None),
        ]


=== FILE: strange_attractor.py ===

"""
Strange Attractor Node - Generates chaotic 2D patterns
Place this file in the 'nodes' folder
"""

import numpy as np
from PyQt6 import QtGui
import cv2

import sys
import os
# --- This is the new, correct block ---
import __main__
BaseNode = __main__.BaseNode
PA_INSTANCE = getattr(__main__, "PA_INSTANCE", None)
# ------------------------------------

class StrangeAttractorNode(BaseNode):
    NODE_CATEGORY = "Source"
    NODE_COLOR = QtGui.QColor(40, 140, 100) # A generative green
    
    def __init__(self, width=160, height=120):
        super().__init__()
        self.node_title = "Strange Attractor"
        self.inputs = {
            'param_a': 'signal',
            'param_b': 'signal',
            'param_c': 'signal',
            'param_d': 'signal'
        }
        self.outputs = {'image': 'image', 'x_signal': 'signal', 'y_signal': 'signal'}
        
        self.w, self.h = width, height
        self.img = np.zeros((self.h, self.w), dtype=np.float32)
        
        # Attractor state
        self.x, self.y = 0.1, 0.1
        
        # Default parameters for a "Clifford" attractor
        self.a = -1.4
        self.b = 1.6
        self.c = 1.0
        self.d = 0.7
        
        # For visualization
        self.points = np.zeros((self.h, self.w), dtype=np.float32)

    def step(self):
        # Update parameters from inputs, or use internal values
        self.a = self.get_blended_input('param_a', 'sum') or self.a
        self.b = self.get_blended_input('param_b', 'sum') or self.b
        self.c = self.get_blended_input('param_c', 'sum') or self.c
        self.d = self.get_blended_input('param_d', 'sum') or self.d
        
        # Iterate the attractor equations 500 times per frame for a dense plot
        for _ in range(500):
            # Clifford Attractor equations
            x_new = np.sin(self.a * self.y) + self.c * np.cos(self.a * self.x)
            y_new = np.sin(self.b * self.x) + self.d * np.cos(self.b * self.y)
            
            self.x, self.y = x_new, y_new
            
            # Scale from [-2, 2] range to image coordinates [0, w] and [0, h]
            px = int((self.x + 2.0) / 4.0 * self.w)
            py = int((self.y + 2.0) / 4.0 * self.h)
            
            # Plot the point
            if 0 <= px < self.w and 0 <= py < self.h:
                self.points[py, px] += 0.1 # Add energy to this pixel
        
        # Apply decay to the image so it fades
        self.points *= 0.98
        self.points = np.clip(self.points, 0, 1.0)
        
        # Blur the image slightly for a "glowing" effect
        self.img = cv2.GaussianBlur(self.points, (3, 3), 0)
        
    def get_output(self, port_name):
        if port_name == 'image':
            return self.img
        elif port_name == 'x_signal':
            return self.x / 2.0 # Normalize to [-1, 1]
        elif port_name == 'y_signal':
            return self.y / 2.0 # Normalize to [-1, 1]
        return None
        
    def get_display_image(self):
        img_u8 = (np.clip(self.img, 0, 1) * 255).astype(np.uint8)
        img_u8 = np.ascontiguousarray(img_u8)
        return QtGui.QImage(img_u8.data, self.w, self.h, self.w, QtGui.QImage.Format.Format_Grayscale8)

    def get_config_options(self):
        return [
            ("Param A", "a", self.a, None),
            ("Param B", "b", self.b, None),
            ("Param C", "c", self.c, None),
            ("Param D", "d", self.d, None),
        ]

    def randomize(self):
        # Add a randomize button
        self.a = np.random.uniform(-2.0, 2.0)
        self.b = np.random.uniform(-2.0, 2.0)
        self.c = np

=== FILE: su2.py ===

"""
SU2FieldNode (Weak Force Metaphor)

Simulates an SU(2) gauge force with 3 components.
Treats the input image's RGB channels as a 3D "flavor space"
and rotates this space, simulating flavor change.

[FIXED] Initialized self.field_out in __init__ to prevent AttributeError.
"""

import numpy as np
import cv2

# --- Magic import block ---
import __main__
BaseNode = __main__.BaseNode
QtGui = __main__.QtGui
# --------------------------

class SU2FieldNode(BaseNode):
    """
    Rotates the RGB "flavor" space of an image.
    """
    NODE_CATEGORY = "Filter"
    NODE_COLOR = QtGui.QColor(220, 100, 100) # Red

    def __init__(self, size=128):
        super().__init__()
        self.node_title = "SU(2) Field (Weak)"
        
        self.inputs = {
            'field_in': 'image',   # Color image (flavor field)
            'rot_X': 'signal',     # 'W+' (R <-> G)
            'rot_Y': 'signal',     # 'W-' (G <-> B)
            'rot_Z': 'signal'      # 'Z0' (B <-> R)
        }
        self.outputs = {'field_out': 'image'}
        
        self.size = int(size)
        self.t = 0.0 # Internal time
        
        # --- START FIX ---
        # Initialize the output variable to prevent race condition
        self.field_out = np.zeros((self.size, self.size, 3), dtype=np.float32)
        # --- END FIX ---
        
    def _prepare_image(self, img):
        if img is None:
            return np.zeros((self.size, self.size, 3), dtype=np.float32)
        
        if img.dtype != np.float32: img = img.astype(np.float32)
        if img.max() > 1.0: img /= 255.0
            
        img_resized = cv2.resize(img, (self.size, self.size), 
                                 interpolation=cv2.INTER_LINEAR)
        
        if img_resized.ndim == 2:
            return cv2.cvtColor(img_resized, cv2.COLOR_GRAY2RGB)
        return img_resized

    def step(self):
        # --- 1. Get Inputs ---
        field = self._prepare_image(self.get_blended_input('field_in', 'first'))
        
        # Get rotation angles
        angle_x = (self.get_blended_input('rot_X', 'sum') or 0.0) * 0.1
        angle_y = (self.get_blended_input('rot_Y', 'sum') or 0.0) * 0.1
        angle_z = (self.get_blended_input('rot_Z', 'sum') or 0.0) * 0.1
        
        # --- 2. Build Rotation Matrices ---
        cx, sx = np.cos(angle_x), np.sin(angle_x)
        cy, sy = np.cos(angle_y), np.sin(angle_y)
        cz, sz = np.cos(angle_z), np.sin(angle_z)
        
        # Note: OpenCV uses BGR, so we'll treat B=X, G=Y, R=Z
        
        # Z-axis rotation (R <-> G)
        R_z = np.float32([
            [cz, -sz, 0],
            [sz,  cz, 0],
            [ 0,   0, 1]
        ])
        
        # X-axis rotation (G <-> B)
        R_x = np.float32([
            [1,  0,   0],
            [0, cx, -sx],
            [0, sx,  cx]
        ])
        
        # Y-axis rotation (B <-> R)
        R_y = np.float32([
            [ cy, 0, sy],
            [  0, 1,  0],
            [-sy, 0,  cy]
        ])
        
        # Combine all rotations
        R_total = R_z @ R_y @ R_x
        
        # --- 3. Apply SU(2) Flavor Rotation ---
        # Reshape image for matrix multiplication
        h, w, c = field.shape
        field_flat = field.reshape((-1, 3))
        
        # Apply transformation
        # (field_flat @ R_total.T) is the same as (R_total @ field_flat.T).T
        rotated_field_flat = field_flat @ R_total.T
        
        # Reshape back to image
        self.field_out = rotated_field_flat.reshape((h, w, 3))
        
        # Clip to maintain valid color range
        self.field_out = np.clip(self.field_out, 0.0, 1.0)

    def get_output(self, port_name):
        if port_name == 'field_out':
            return self.field_out
        return None

    def get_display_image(self):
        # self.field_out is guaranteed to exist now
        return self.field_out

=== FILE: su3.py ===

"""
SU3FieldNode (Strong Force Metaphor)

Simulates an SU(3) "color" force with confinement.
Pure colors (R, G, B) are "far" from neutral gray and are
"pulled" back strongly, creating a vibrating/jiggling effect.

[FIXED] Initialized self.field_out in __init__ to prevent AttributeError.
"""

import numpy as np
import cv2

# --- Magic import block ---
import __main__
BaseNode = __main__.BaseNode
QtGui = __main__.QtGui
# --------------------------

class SU3FieldNode(BaseNode):
    """
    Simulates "color confinement" by pulling colors to neutral.
    """
    NODE_CATEGORY = "Filter"
    NODE_COLOR = QtGui.QColor(100, 220, 100) # Green

    def __init__(self, size=128):
        super().__init__()
        self.node_title = "SU(3) Field (Strong)"
        
        self.inputs = {
            'field_in': 'image',           # Color charge field
            'confinement': 'signal'      # 0-1, strength of confinement
        }
        self.outputs = {'field_out': 'image'}
        
        self.size = int(size)
        
        # Internal buffer for jiggling
        self.dx = np.zeros((self.size, self.size), dtype=np.float32)
        self.dy = np.zeros((self.size, self.size), dtype=np.float32)
        
        y, x = np.mgrid[0:self.size, 0:self.size]
        self.grid_x = x.astype(np.float32)
        self.grid_y = y.astype(np.float32)
        
        # --- START FIX ---
        # Initialize the output variable to prevent race condition
        self.field_out = np.zeros((self.size, self.size, 3), dtype=np.float32)
        # --- END FIX ---
        
    def _prepare_image(self, img):
        if img is None:
            return np.full((self.size, self.size, 3), 0.5, dtype=np.float32)
        
        if img.dtype != np.float32: img = img.astype(np.float32)
        if img.max() > 1.0: img /= 255.0
            
        img_resized = cv2.resize(img, (self.size, self.size), 
                                 interpolation=cv2.INTER_LINEAR)
        
        if img_resized.ndim == 2:
            return cv2.cvtColor(img_resized, cv2.COLOR_GRAY2RGB)
        return img_resized

    def step(self):
        # --- 1. Get Inputs ---
        field = self._prepare_image(self.get_blended_input('field_in', 'first'))
        confinement = (self.get_blended_input('confinement', 'sum') or 0.2) * 20.0
        
        # --- 2. Calculate "Color Purity" ---
        # Find the mean color (neutral gray point)
        mean_color = np.mean(field, axis=(0, 1))
        
        # Find distance from neutral for each pixel
        # This is our "confinement force" map
        color_diff = field - mean_color
        force_map = np.linalg.norm(color_diff, axis=2) # (H, W)
        
        # --- 3. Simulate "Gluon Jiggle" ---
        # Apply force to a simple oscillator (our displacement map)
        self.dx = self.dx * 0.9 + (np.random.randn(self.size, self.size) * force_map * confinement)
        self.dy = self.dy * 0.9 + (np.random.randn(self.size, self.size) * force_map * confinement)
        
        # Clamp displacement
        self.dx = np.clip(self.dx, -10.0, 10.0)
        self.dy = np.clip(self.dy, -10.0, 10.0)
        
        # --- 4. Apply Confinement Warp ---
        map_x = (self.grid_x + self.dx).astype(np.float32)
        map_y = (self.grid_y + self.dy).astype(np.float32)
        
        self.field_out = cv2.remap(
            field, map_x, map_y, 
            cv2.INTER_LINEAR, 
            borderMode=cv2.BORDER_REFLECT_101
        )
        
        # --- 5. Apply "Color Rotation" (Gluon Exchange) ---
        # We also slowly pull the colors toward the mean
        self.field_out = self.field_out * 0.99 + mean_color * 0.01
        self.field_out = np.clip(self.field_out, 0, 1) # Add clip for safety

    def get_output(self, port_name):
        if port_name == 'field_out':
            return self.field_out
        return None

    def get_display_image(self):
        # self.field_out is guaranteed to exist now
        return self.field_out

=== FILE: thetagammascanner.py ===

"""
Theta-Gamma Sweep Scanner Node
-----------------------------------
This node simulates a dynamic cortical map based on concepts from
four key papers:

1.  Fractal Cortex (Wang et al., 2024): The node uses a 2D map
    representing the cortex, which is described as a fractal structure.
    
2.  Theta Sweeps (Vollan et al., 2025): The map is scanned by a
    theta-paced (8Hz) "look around" mechanism that alternates
    left and right, modeling the hippocampal-entorhinal system.
    
3.  Gamma Gating (Drebitz et al., 2025): Information is processed
    (gated) based on its phase-relationship to an internal gamma
    oscillation (40Hz), modeling "communication through coherence".
    [cite: 6244, 6606]
4.  Time-Domain Brain (Baker & Cariani, 2025): The node is
    "signal-centric"  and models the interaction between
    oscillation bands (Theta and Gamma) as a core processing
    mechanism. [cite: 4599]

Place this file in the 'nodes' folder
"""

import numpy as np
from PyQt6 import QtGui
import cv2

import __main__
BaseNode = __main__.BaseNode
PA_INSTANCE = getattr(__main__, "PA_INSTANCE", None)
QtGui = __main__.QtGui

try:
    from scipy.ndimage import gaussian_filter
    SCIPY_AVAILABLE = True
except ImportError:
    SCIPY_AVAILABLE = False
    print("Warning: ThetaGammaScannerNode requires scipy")

class ThetaGammaScannerNode(BaseNode):
    NODE_CATEGORY = "Cognitive"
    NODE_COLOR = QtGui.QColor(0, 150, 200)  # Deep Teal
    
    def __init__(self, map_size=64, learning_rate=0.05, decay_rate=0.99, sweep_angle_deg=30.0, theta_freq_hz=8.0, gamma_freq_hz=40.0):
        super().__init__()
        self.node_title = "Theta-Gamma Scanner"
        
        self.inputs = {
            'phase_field': 'image',       # The sensory input to process
            'internal_direction': 'signal', # Bias for the sweep (e.g., head direction)
            'ext_theta': 'signal',        # Optional external theta to sync with
            'ext_gamma': 'signal'         # The "phase" of the input signal
        }
        
        self.outputs = {
            'gated_output': 'image',      # The input signal, gated by coherence
            'memory_map': 'image',        # The internal holographic/fractal map
            'theta_phase': 'signal',      # Our internal theta clock output
            'gamma_phase': 'signal',      # Our internal gamma clock output
            'coherence_gate': 'signal'    # The resulting gamma gate (0-1)
        }
        
        if not SCIPY_AVAILABLE:
            self.node_title = "Theta-Gamma (No SciPy!)"
            return
            
        # --- Configurable Parameters ---
        self.map_size = int(map_size)
        self.learning_rate = float(learning_rate)
        self.decay_rate = float(decay_rate)
        self.sweep_angle_deg = float(sweep_angle_deg)
        self.theta_freq_hz = float(theta_freq_hz)
        self.gamma_freq_hz = float(gamma_freq_hz)

        # --- Internal State ---
        # 1. The Holographic Map (from Paper 1 & 3)
        self.memory_map = np.random.rand(self.map_size, self.map_size).astype(np.float32) * 0.1
        
        # 2. Oscillators (from Paper 2, 3, 4)
        self.theta_phase_rad = 0.0
        self.gamma_phase_rad = 0.0
        self.last_theta_cos = 1.0
        # Assuming 30 FPS for simulation, pre-calculate increments
        self.theta_increment = (2 * np.pi * self.theta_freq_hz) / 30.0
        self.gamma_increment = (2 * np.pi * self.gamma_freq_hz) / 30.0

        # 3. Theta Sweep State (from Paper 2)
        self.sweep_direction = 1.0  # 1.0 for Right, -1.0 for Left
        
        # --- Output Buffers ---
        self.gated_output_img = np.zeros((self.map_size, self.map_size, 3), dtype=np.float32)
        self.coherence_gate_out = 0.0
        
        # Gaze mask for visualization
        self.gaze_mask = np.zeros((self.map_size, self.map_size), dtype=np.float32)
        self.sweep_x = self.map_size // 2
        self.sweep_y = self.map_size // 2


    def _create_gaze_mask(self, center_x, center_y, size, max_val=1.0):
        """Creates a soft circular mask at the sweep gaze location."""
        y, x = np.indices((size, size))
        dist_sq = (x - center_x)**2 + (y - center_y)**2
        sigma_sq = (size / 10.0)**2  # Make the gaze area ~10% of the map
        mask = max_val * np.exp(-dist_sq / (2 * sigma_sq))
        return mask

    def step(self):
        if not SCIPY_AVAILABLE:
            return
            
        # --- 1. Get Inputs ---
        phase_field_in = self.get_blended_input('phase_field', 'mean')
        base_direction_in = self.get_blended_input('internal_direction', 'sum') or 0.0
        ext_theta_in = self.get_blended_input('ext_theta', 'sum')
        ext_gamma_in = self.get_blended_input('ext_gamma', 'sum')

        # --- 2. Update Oscillators (Paper 3) ---
        
        # Update Theta
        if ext_theta_in is not None:
            self.theta_phase_rad = np.arccos(np.clip(ext_theta_in, -1, 1))
        else:
            self.theta_phase_rad = (self.theta_phase_rad + self.theta_increment) % (2 * np.pi)
        
        current_theta_cos = np.cos(self.theta_phase_rad)
        
        # Update Gamma
        if ext_gamma_in is not None:
            # If external gamma is provided, we phase-lock to it
            self.gamma_phase_rad = np.arccos(np.clip(ext_gamma_in, -1, 1))
        else:
            self.gamma_phase_rad = (self.gamma_phase_rad + self.gamma_increment) % (2 * np.pi)

        # --- 3. Update Theta Sweep (Paper 2) ---
        
        # Check for theta trough (crossing from negative to positive)
        # This is when the sweep alternates [cite: 1424, 1560]
        if self.last_theta_cos < 0 and current_theta_cos >= 0:
            self.sweep_direction *= -1.0  # Flip direction
            
        self.last_theta_cos = current_theta_cos
        
        # Calculate sweep angle
        sweep_angle_rad = np.deg2rad(base_direction_in + (self.sweep_direction * self.sweep_angle_deg))
        
        # Theta phase drives sweep length (0 at trough, 1 at peak)
        # "sweeps linearly outwards from the animal's location" [cite: 1424]
        sweep_progress = (current_theta_cos + 1.0) / 2.0  # 0 -> 1
        sweep_length = (self.map_size / 2.0) * sweep_progress
        
        # Calculate current "gaze" position of the sweep
        center_x = self.map_size // 2 + sweep_length * np.cos(sweep_angle_rad)
        center_y = self.map_size // 2 + sweep_length * np.sin(sweep_angle_rad)
        self.sweep_x, self.sweep_y = center_x, center_y
        
        # Create a soft mask for the gaze location
        self.gaze_mask = self._create_gaze_mask(center_x, center_y, self.map_size)
        
        # --- 4. Apply Gamma Gating (Paper 4) ---
        
        # "communication through coherence" [cite: 6890]
        # The gate opens if the input gamma phase matches the internal gamma phase.
        if ext_gamma_in is not None:
            ext_gamma_rad = np.arccos(np.clip(ext_gamma_in, -1, 1))
            phase_difference = self.gamma_phase_rad - ext_gamma_rad
            # Gate is max (1) at 0 diff, min (0) at pi diff
            self.coherence_gate_out = (np.cos(phase_difference) + 1.0) / 2.0
        else:
            # No external gamma, so gate is just driven by internal excitability
            # "afferent spikes should be most effective when they arrive during the sensitive phase" [cite: 6256]
            self.coherence_gate_out = (np.cos(self.gamma_phase_rad) + 1.0) / 2.0 # Assumes peak is sensitive
            
        # --- 5. Process Signal (Write to Map) ---
        
        if phase_field_in is None:
            phase_field_in = np.random.rand(self.map_size, self.map_size)
        
        if phase_field_in.shape[0] != self.map_size:
            phase_field_in = cv2.resize(phase_field_in, (self.map_size, self.map_size))
            
        if phase_field_in.ndim == 3:
            phase_field_in = np.mean(phase_field_in, axis=2)
            
        # Apply gating: sensory input * sweep_location * gamma_gate
        gated_signal = phase_field_in * self.gaze_mask * self.coherence_gate_out
        
        # Update the memory map (Holographic/Fractal store)
        self.memory_map += gated_signal * self.learning_rate
        # Apply decay/forgetting
        self.memory_map = (self.memory_map * self.decay_rate).astype(np.float32)
        np.clip(self.memory_map, 0, 1, out=self.memory_map)
        
        # Prepare gated signal for output
        self.gated_output_img = (np.clip(gated_signal, 0, 1) * 255).astype(np.uint8)
        self.gated_output_img = cv2.cvtColor(self.gated_output_img, cv2.COLOR_GRAY2RGB)
        
        
    def get_output(self, port_name):
        if port_name == 'gated_output':
            return self.gated_output_img
        elif port_name == 'memory_map':
            return self.memory_map
        elif port_name == 'theta_phase':
            return np.cos(self.theta_phase_rad)
        elif port_name == 'gamma_phase':
            return np.cos(self.gamma_phase_rad)
        elif port_name == 'coherence_gate':
            return self.coherence_gate_out
        return None

    def get_display_image(self):
        if not SCIPY_AVAILABLE: return None
        
        # Create a detailed visualization
        display_w = 512
        display_h = 256
        display = np.zeros((display_h, display_w, 3), dtype=np.uint8)
        
        # Left side: Memory Map
        map_u8 = (np.clip(self.memory_map, 0, 1) * 255).astype(np.uint8)
        map_resized = cv2.resize(cv2.cvtColor(map_u8, cv2.COLOR_GRAY2RGB), 
                                 (display_h, display_h), 
                                 interpolation=cv2.INTER_NEAREST)
        
        # Draw the sweep line on the map
        line_start = (display_h // 2, display_h // 2)
        line_end = (int(self.sweep_x / self.map_size * display_h),
                    int(self.sweep_y / self.map_size * display_h))
        cv2.line(map_resized, line_start, line_end, (255, 0, 255), 2)
        cv2.circle(map_resized, line_end, 8, (255, 0, 255), -1)
        
        display[:, :display_h] = map_resized
        
        # Right side: Gated Input (What's being "seen")
        gated_resized = cv2.resize(self.gated_output_img, 
                                   (display_h, display_h), 
                                   interpolation=cv2.INTER_NEAREST)
        display[:, display_w-display_h:] = gated_resized
        
        # Add dividing line
        display[:, display_h-1:display_h+1] = [255, 255, 255]
        
        # Add labels
        font = cv2.FONT_HERSHEY_SIMPLEX
        cv2.putText(display, 'MEMORY MAP', (10, 20), font, 0.5, (255, 255, 255), 1, cv2.LINE_AA)
        cv2.putText(display, 'GATED SENSORY INPUT', (display_h + 10, 20), font, 0.5, (255, 255, 255), 1, cv2.LINE_AA)
        
        # Add oscillator info
        theta_val = np.cos(self.theta_phase_rad)
        gamma_val = np.cos(self.gamma_phase_rad)
        sweep_dir_str = "RIGHT" if self.sweep_direction > 0 else "LEFT"
        
        cv2.putText(display, f"THETA: {theta_val:+.2f} ({self.theta_freq_hz}Hz)", (10, display_h - 40), font, 0.4, (200, 200, 200), 1, cv2.LINE_AA)
        cv2.putText(display, f"GAMMA: {gamma_val:+.2f} ({self.gamma_freq_hz}Hz)", (10, display_h - 25), font, 0.4, (200, 200, 200), 1, cv2.LINE_AA)
        cv2.putText(display, f"SWEEP: {sweep_dir_str}", (10, display_h - 10), font, 0.4, (255, 0, 255), 1, cv2.LINE_AA)

        cv2.putText(display, f"COHERENCE: {self.coherence_gate_out:.2f}", (display_h + 10, display_h - 10), font, 0.4, (0, 255, 0), 1, cv2.LINE_AA)
        
        display = np.ascontiguousarray(display)
        return QtGui.QImage(display.data, display_w, display_h, 3*display_w, QtGui.QImage.Format.Format_RGB888)

    def get_config_options(self):
        return [
            ("Map Size", "map_size", self.map_size, None),
            ("Learning Rate", "learning_rate", self.learning_rate, None),
            ("Decay Rate", "decay_rate", self.decay_rate, None),
            ("Sweep Angle (deg)", "sweep_angle_deg", self.sweep_angle_deg, None),
            ("Theta Freq (Hz)", "theta_freq_hz", self.theta_freq_hz, None),
            ("Gamma Freq (Hz)", "gamma_freq_hz", self.gamma_freq_hz, None),
        ]

=== FILE: topologicalatomnode.py ===

"""
Topological Atom Node - Simulates a field configuration (atom) with resonant shell
structure and allows for rotational manipulation (phase twist) to test topological 
protection against substrate noise.

Ported from instantonassim x.py
Requires: pip install numpy scipy
"""

import numpy as np
from PyQt6 import QtGui
import cv2
from scipy.ndimage import gaussian_filter
import sys
import os

import __main__
BaseNode = __main__.BaseNode
PA_INSTANCE = getattr(__main__, "PA_INSTANCE", None)
QtGui = __main__.QtGui

try:
    from scipy.ndimage import gaussian_filter
    SCIPY_AVAILABLE = True
except ImportError:
    SCIPY_AVAILABLE = False
    print("Warning: TopologicalAtomNode requires 'scipy'.")


# --- Core Physics Engine (from instantonassim x.py) ---
class ResonantInstantonModel:
    def __init__(self, grid_size=96, dt=0.05, c=1.0, a=0.1, b=0.1, gamma=0.02, substrate_noise=0.0005):
        self.grid_size = grid_size
        self.dt = dt
        self.c = c
        self.a = a
        self.b = b
        self.gamma = gamma
        self.substrate_noise = substrate_noise
        
        self.phi = np.zeros((grid_size, grid_size))
        self.phi_prev = np.zeros((grid_size, grid_size))
        self.instanton_events = []
        self.stability_metric = 1.0
        self.topological_charge = 0.0 # New metric
        self.current_rotation = 0.0   # Current angle of the structure
        
        self.time = 0
        self.frame_count = 0
        
        self.initialize_atom(atomic_number=6, stable_isotope=True) # Default to stable Carbon

    def initialize_atom(self, atomic_number, position=None, stable_isotope=True):
        if position is None: position = (self.grid_size // 2, self.grid_size // 2)
        self.phi = np.zeros((self.grid_size, self.grid_size))
        self.phi_prev = np.zeros((self.grid_size, self.grid_size))
        self.instanton_events = []
        self.stability_metric = 1.0
        
        core_radius = 4 + np.log(1 + atomic_number)
        x, y = np.meshgrid(np.arange(self.grid_size), np.arange(self.grid_size))
        r = np.sqrt((x - position[0])**2 + (y - position[1])**2)
        core_amplitude = 1.0 + 0.2 * atomic_number
        
        # Create nuclear core
        self.phi = core_amplitude * np.exp(-r**2 / (2 * core_radius**2))
        
        # Add shells based on simplified quantum numbers (standing waves)
        shell_config = self._calculate_shell_configuration(atomic_number)
        for shell, electrons in enumerate(shell_config):
            if electrons > 0:
                shell_radius = self._shell_radius(shell + 1)
                shell_amplitude = 0.3 * (electrons / (2*(2*shell+1)**2))
                shell_wave = shell_amplitude * np.cos(np.pi * r / shell_radius)**2 * (r < 2*shell_radius)
                self.phi += shell_wave
        
        if not stable_isotope:
            asymmetry = 0.1 * np.sin(3 * np.arctan2(y - position[1], x - position[0]))
            self.phi += asymmetry * np.exp(-r**2 / (2 * core_radius**2))
            self.stability_metric = 0.7 + 0.3 * np.random.random()
        
        self.phi_prev = self.phi.copy()
        self.time = 0
        self.frame_count = 0

    def _calculate_shell_configuration(self, atomic_number):
        shell_capacity = [2, 8, 18, 32]
        shells = []
        electrons_left = atomic_number
        for capacity in shell_capacity:
            if electrons_left >= capacity:
                shells.append(capacity); electrons_left -= capacity
            else:
                shells.append(electrons_left); electrons_left = 0; break
        while electrons_left > 0:
            next_capacity = 2 * (len(shells) + 1)**2
            if electrons_left >= next_capacity:
                shells.append(next_capacity); electrons_left -= next_capacity
            else:
                shells.append(electrons_left); electrons_left = 0
        return shells
    
    def _shell_radius(self, n):
        base_radius = 8
        return base_radius * n**2
    
    def _laplacian(self, field):
        field_padded = np.pad(field, 1, mode='wrap')
        laplacian = (field_padded[:-2, 1:-1] + field_padded[2:, 1:-1] + 
                     field_padded[1:-1, :-2] + field_padded[1:-1, 2:] - 
                     4 * field_padded[1:-1, 1:-1])
        return laplacian
    
    def _detect_instanton_event(self, phi_old, phi_new):
        delta_phi = phi_new - phi_old
        delta_phi_smoothed = gaussian_filter(delta_phi, sigma=1.0)
        threshold = 0.1 * np.max(np.abs(self.phi))
        significant_changes = np.abs(delta_phi_smoothed) > threshold
        
        if np.any(significant_changes):
            self.instanton_events.append({'time': self.time, 'magnitude': np.max(np.abs(delta_phi_smoothed))})
            return True
        return False
    
    def _update_stability(self):
        recent_count = sum(1 for event in self.instanton_events 
                           if event['time'] > self.time - 100 * self.dt)
        if recent_count > 5: self.stability_metric -= 0.01
        else: self.stability_metric = min(1.0, self.stability_metric + 0.001)
        self.stability_metric = max(0.0, min(1.0, self.stability_metric))

    def rotate_field(self, angle_rad):
        """
        Applies a rotation (phase twist) to the current field configuration.
        This is the test for topological protection.
        """
        if abs(angle_rad) < 1e-6: return

        center = self.grid_size // 2
        
        # 1. Define the rotation matrix
        cos_a = np.cos(angle_rad)
        sin_a = np.sin(angle_rad)
        
        # 2. Get coordinates relative to center
        y, x = np.mgrid[:self.grid_size, :self.grid_size]
        x_c = x - center
        y_c = y - center

        # 3. Apply rotation to coordinates
        x_rot = x_c * cos_a - y_c * sin_a
        y_rot = x_c * sin_a + y_c * cos_a
        
        # 4. Map rotated coordinates back to grid indices
        x_rot_idx = np.clip(np.round(x_rot + center).astype(int), 0, self.grid_size - 1)
        y_rot_idx = np.clip(np.round(y_rot + center).astype(int), 0, self.grid_size - 1)

        # 5. Create a new field by sampling the old one at rotated positions
        phi_rotated = self.phi[y_rot_idx, x_rot_idx]
        
        # Update current field and record rotation
        self.phi = phi_rotated
        self.phi_prev = phi_rotated # Ensure stability after rotation
        self.current_rotation = (self.current_rotation + angle_rad) % (2 * np.pi)

    def compute_topological_charge(self):
        """
        Computes the topological charge (winding number) of the structure.
        For a purely radial field, this is near zero. For a vortex, it's non-zero.
        We simplify: Charge = Mean gradient magnitude divided by stability.
        """
        grad_mag = np.mean(np.abs(np.gradient(self.phi)))
        # Scale and use stability as a denominator (more stable = lower perceived charge)
        self.topological_charge = (grad_mag * 100) / (self.stability_metric + 0.1)

    def step(self):
        # Save current field
        phi_old = self.phi.copy()
        
        # Compute field evolution terms
        laplacian_phi = self._laplacian(self.phi)
        
        # Add substrate noise (decoherence force)
        noise = self.substrate_noise * np.random.normal(size=self.phi.shape)
        
        # Field equation (Simplified wave equation)
        accel = (self.c**2 * laplacian_phi + 
                 self.a * self.phi - 
                 self.b * self.phi**3 + 
                 noise)
        
        # Update field using velocity Verlet integration
        phi_new = 2 * self.phi - self.phi_prev + self.dt**2 * accel
        
        # Update field state
        self.phi_prev = self.phi
        self.phi = phi_new
        
        # Detect instanton events
        self._detect_instanton_event(phi_old, self.phi)
        
        # Update stability metric and charge
        self._update_stability()
        self.compute_topological_charge()
        
        self.time += self.dt
        self.frame_count += 1
        return self.stability_metric


class TopologicalAtomNode(BaseNode):
    NODE_CATEGORY = "Source"
    NODE_COLOR = QtGui.QColor(100, 50, 200) # Deep Quantum Purple
    
    def __init__(self, atomic_number=6, stable=True, rotation_speed=0.0):
        super().__init__()
        self.node_title = "Topological Atom"
        
        self.inputs = {
            'noise_strength': 'signal',   # Substrate noise (decoherence)
            'rotation_rate': 'signal',    # External rotation force
            'reset': 'signal'
        }
        self.outputs = {
            'field_image': 'image',
            'stability': 'signal',        # Stability Metric [0, 1]
            'charge': 'signal',           # Topological Charge
            'rotation_angle': 'signal'    # Current rotation angle [-1, 1]
        }
        
        if not SCIPY_AVAILABLE:
            self.node_title = "Atom (No SciPy!)"
            return
            
        self.atomic_number = int(atomic_number)
        self.stable = bool(stable)
        self.rotation_speed_base = float(rotation_speed)
        
        self.sim = ResonantInstantonModel(grid_size=96, substrate_noise=0.0005)
        self.sim.initialize_atom(self.atomic_number, stable_isotope=self.stable)
        self.last_reset_sig = 0.0

    def randomize(self):
        self.sim.initialize_atom(self.atomic_number, stable_isotope=self.stable)

    def step(self):
        if not SCIPY_AVAILABLE: return

        # 1. Handle Inputs
        noise_in = self.get_blended_input('noise_strength', 'sum')
        rotation_in = self.get_blended_input('rotation_rate', 'sum')
        reset_in = self.get_blended_input('reset', 'sum')

        # Update noise (decoherence)
        if noise_in is not None:
            self.sim.substrate_noise = np.clip(noise_in * 0.01, 0.0001, 0.01)

        # Update rotation
        rotation_rate = self.rotation_speed_base + (rotation_in * 0.1) if rotation_in is not None else self.rotation_speed_base
        self.sim.rotate_field(rotation_rate * self.sim.dt)

        # Handle reset
        if reset_in is not None and reset_in > 0.5 and self.last_reset_sig <= 0.5:
            self.randomize()
        self.last_reset_sig = reset_in or 0.0
        
        # 2. Evolve simulation
        self.sim.step()
    
    def get_output(self, port_name):
        if port_name == 'field_image':
            # Normalize field output for display
            phi = self.sim.phi
            v_abs = np.max(np.abs(phi))
            return np.clip(phi / (v_abs + 1e-9), -1.0, 1.0)
            
        elif port_name == 'stability':
            return self.sim.stability_metric
            
        elif port_name == 'charge':
            return self.sim.topological_charge
            
        elif port_name == 'rotation_angle':
            # Normalize angle [0, 2pi] to signal [-1, 1]
            return (self.sim.current_rotation / (2 * np.pi)) * 2.0 - 1.0
            
        return None
    
    def get_display_image(self):
        # Render the field configuration (Field amplitude)
        field_data = self.get_output('field_image')
        if field_data is None: return None

        # Map [-1, 1] data to Red/Blue color map
        img_rgb = np.zeros((*field_data.shape, 3), dtype=np.uint8)
        
        # Red: Positive field (Vacuum 1); Blue: Negative field (Vacuum 0)
        img_rgb[:, :, 0] = np.clip(field_data * 255, 0, 255) # Red channel (positive part)
        img_rgb[:, :, 2] = np.clip(-field_data * 255, 0, 255) # Blue channel (negative part)
        
        # Draw stability metric on top
        s = self.sim.stability_metric
        color = (255 * (1-s), 255 * s, 0) # Green for stable, Red for unstable (BGR)
        cv2.rectangle(img_rgb, (5, 5), (self.sim.grid_size - 5, 15), color, -1)
        
        # Resize to thumbnail
        img_thumb = cv2.resize(img_rgb, (96, 96), interpolation=cv2.INTER_LINEAR)
        img_thumb = np.ascontiguousarray(img_thumb)

        h, w = img_thumb.shape[:2]
        return QtGui.QImage(img_thumb.data, w, h, 3*w, QtGui.QImage.Format.Format_RGB888)

    def get_config_options(self):
        return [
            ("Atomic Number (Z)", "atomic_number", self.atomic_number, None),
            ("Stable Isotope?", "stable", self.stable, [(True, True), (False, False)]),
            ("Base Rot. Speed", "rotation_speed", self.rotation_speed_base, None),
        ]

=== FILE: topologicalsievenode.py ===

"""
Topological Sieve Node - Simulates a Quantum Cellular Automaton performing a
Prime Number Sieve via topological annihilation (interference).

Outputs:
- Information Density (The computation state image).
- Prime Index (The current prime being tested).
- Final Result (A signal that goes high when computation is complete).
Ported from topological_prime_sieve.py.
Requires: pip install numpy scipy
"""

import numpy as np
from PyQt6 import QtGui
import cv2
from scipy.ndimage import gaussian_filter
import sys
import os

import __main__
BaseNode = __main__.BaseNode
PA_INSTANCE = getattr(__main__, "PA_INSTANCE", None)
QtGui = __main__.QtGui

try:
    from scipy.ndimage import gaussian_filter
    SCIPY_AVAILABLE = True
except ImportError:
    SCIPY_AVAILABLE = False
    print("Warning: TopologicalSieveNode requires 'scipy'.")


# --- Simulation Constants (from source) ---
GRID_SIZE_X, GRID_SIZE_Y = 128, 128
SIEVE_ROWS, SIEVE_COLS = 10, 10
MAX_NUMBER = SIEVE_ROWS * SIEVE_COLS
PRIMES_TO_SIEVE = [2, 3, 5, 7] # Sieve for primes up to sqrt(100)
DT = 0.1  
DAMPING = 0.998


class TopologicalSieve:
    """Manages the dynamics of the Ï field within a lattice scaffold."""
    
    def __init__(self, width, height):
        self.width = width
        self.height = height
        self.psi = np.zeros((width, height), dtype=np.complex64)
        self.psi_prev = np.zeros((width, height), dtype=np.complex64)
        self.information_density = np.zeros((width, height), dtype=np.float32)
        
        self.lattice_locations, self.environmental_potential = self._create_lattice_scaffold(SIEVE_ROWS, SIEVE_COLS)
        self._initialize_atoms()
        
        self.frame = 0
        self.state = "SETTLING"
        self.prime_index_to_sieve = 0
        self.frames_since_last_action = 0
        self.last_trigger_val = 0.0

    def _create_lattice_scaffold(self, rows, cols):
        """Creates a grid of potential wells to represent numbers."""
        potential = np.zeros((self.width, self.height), dtype=np.float32)
        locations = {}
        
        spacing_x = self.width / (cols + 1)
        spacing_y = self.height / (rows + 1)

        for r in range(rows):
            for c in range(cols):
                number = r * cols + c + 1
                if number > MAX_NUMBER: continue
                
                cx = int((c + 1) * spacing_x)
                cy = int((r + 1) * spacing_y)
                locations[number] = (cx, cy)
                
                yy, xx = np.mgrid[0:self.height, 0:self.width]
                dist_sq = (xx - cx)**2 + (yy - cy)**2
                potential -= np.exp(-dist_sq / (spacing_x / 3)**2)

        return locations, gaussian_filter(potential, sigma=2.0)

    def _initialize_atoms(self):
        """Places a stable vortex-antivortex pair (an 'atom') in each well."""
        for number, (cx, cy) in self.lattice_locations.items():
            if number == 1: continue
            
            offset = 1 
            amplitude = 1.0 
            yy, xx = np.mgrid[0:self.height, 0:self.width]
            
            dist_sq_p = (xx - (cx + offset))**2 + (yy - cy)**2
            self.psi += (amplitude * np.exp(-dist_sq_p / 5.0)).T.astype(np.complex64)
            
            dist_sq_n = (xx - (cx - offset))**2 + (yy - cy)**2
            self.psi -= (amplitude * np.exp(-dist_sq_n / 5.0)).T.astype(np.complex64)
        self.psi_prev = self.psi.copy()
            
    def launch_sieve_wave(self, prime):
        """Launches a destructive wave tuned to annihilate multiples of the prime."""
        amplitude = 0.25 
        
        yy, xx = np.mgrid[0:self.height, 0:self.width]
        
        grid_spacing = self.width / (SIEVE_COLS + 1)
        k = 2 * np.pi / (prime * grid_spacing)
        
        wave = amplitude * (np.sin(k * xx) * np.sin(k * yy))
        
        self.psi += wave.T.astype(np.complex64)

    def evolve(self):
        """Evolve the field using non-linear dynamics."""
        self.frame += 1
        self.frames_since_last_action += 1
        
        # --- Field evolution physics ---
        laplacian = (np.roll(self.psi, 1, axis=0) + np.roll(self.psi, -1, axis=0) +
                     np.roll(self.psi, 1, axis=1) + np.roll(self.psi, -1, axis=1) - 4 * self.psi)
        
        psi_sq = np.abs(self.psi)**2
        nonlinear_term = self.psi * (psi_sq - 0.5)

        # Update equation (non-linear wave evolution)
        psi_next = (2 * self.psi - self.psi_prev * DAMPING +
                    DT**2 * (0.5 * laplacian - nonlinear_term) - 
                    0.1 * self.environmental_potential * self.psi)

        self.psi_prev, self.psi = self.psi, psi_next
        
        # Measure information density (gradient magnitude)
        grad_x, grad_y = np.gradient(np.abs(self.psi))
        self.information_density = grad_x**2 + grad_y**2


class TopologicalSieveNode(BaseNode):
    NODE_CATEGORY = "Source"
    NODE_COLOR = QtGui.QColor(150, 100, 255) # Quantum Computing Purple
    
    def __init__(self, size=96):
        super().__init__()
        self.node_title = "Topological Sieve"
        
        self.inputs = {
            'prime_trigger': 'signal', # Trigger the next sieving step
            'reset': 'signal'
        }
        self.outputs = {
            'image': 'image',              # Information Density
            'prime_index': 'signal',       # Current prime being processed (2, 3, 5, 7, 0)
            'computation_done': 'signal'   # 1.0 when complete, 0.0 otherwise
        }
        
        if not SCIPY_AVAILABLE:
            self.node_title = "Sieve (No SciPy!)"
            return
            
        self.size = int(size)
        self.sim = TopologicalSieve(self.size, self.size)
        
        self.last_trigger_val = 0.0
        self.time_of_last_launch = 0.0
        self.is_done = False
        self.current_prime = 0.0

    def _execute_sieve_step(self):
        """Advances the state machine: settles, sieves, or finishes."""
        
        if self.sim.state == "SETTLING" and self.sim.frames_since_last_action > 50:
            self.sim.state = "SIEVING"
            self.sim.frames_since_last_action = 0
            
        if self.sim.state == "SIEVING":
            # Check if current launch has settled (gives the wave time to annihilate)
            if self.sim.frames_since_last_action > 100: 
                if self.sim.prime_index_to_sieve < len(PRIMES_TO_SIEVE):
                    prime = PRIMES_TO_SIEVE[self.sim.prime_index_to_sieve]
                    self.sim.launch_sieve_wave(prime)
                    self.sim.prime_index_to_sieve += 1
                    self.sim.frames_since_last_action = 0
                    self.current_prime = float(prime)
                else:
                    self.sim.state = "DONE"
                    self.is_done = True
                    self.current_prime = 0.0

    def randomize(self):
        """Resets the simulation to the initial atomic state."""
        if SCIPY_AVAILABLE:
            self.sim = TopologicalSieve(self.size, self.size)
            self.is_done = False
            self.current_prime = 0.0

    def step(self):
        if not SCIPY_AVAILABLE:
            return
            
        # 1. Handle Inputs
        trigger_val = self.get_blended_input('prime_trigger', 'sum') or 0.0
        reset_sig = self.get_blended_input('reset', 'sum')

        if reset_sig is not None and reset_sig > 0.5:
            self.randomize()
            return
            
        # 2. Manual Step Control (Rising edge)
        if trigger_val > 0.5 and self.last_trigger_val <= 0.5 and not self.is_done:
            self._execute_sieve_step()
            
        self.last_trigger_val = trigger_val

        # 3. Always Evolve the Physics
        self.sim.evolve()


    def get_output(self, port_name):
        if port_name == 'image':
            # Output Information Density
            max_val = np.max(self.sim.information_density)
            if max_val > 1e-9:
                return self.sim.information_density.T / max_val
            return self.sim.information_density.T
            
        elif port_name == 'prime_index':
            # Output the current prime being processed
            return self.current_prime 
            
        elif port_name == 'computation_done':
            return 1.0 if self.is_done else 0.0
            
        return None
        
    def get_display_image(self):
        # Visualize Information Density
        img_data = self.get_output('image')
        if img_data is None: return None
        
        img_u8 = (np.clip(img_data, 0, 1) * 255).astype(np.uint8)
        
        # Apply colormap (Hot for Information Density)
        img_color = cv2.applyColorMap(img_u8, cv2.COLORMAP_HOT)
        
        # Add State overlay (Green for Settling, Red for Done)
        if self.sim.state == "DONE":
             cv2.rectangle(img_color, (0, 0), (self.size, 10), (0, 255, 0), -1) # Green bar
        elif self.sim.state == "SIEVING":
             cv2.rectangle(img_color, (0, 0), (self.size, 10), (255, 165, 0), -1) # Orange bar
             
        # Draw the labels for the surviving/annihilated atoms (complex, so skipping for now)
        
        # Resize to thumbnail size
        img_resized = cv2.resize(img_color, (96, 96), interpolation=cv2.INTER_LINEAR)
        img_resized = np.ascontiguousarray(img_resized)
        
        h, w = img_resized.shape[:2]
        return QtGui.QImage(img_resized.data, w, h, 3*w, QtGui.QImage.Format.Format_BGR888)

    def get_config_options(self):
        return [
            ("Resolution (NxN)", "size", self.size, None),
        ]

=== FILE: topologicalxornode.py ===

"""
Topological XOR Node - Simulates a logic gate (A XOR B) realized by the 
physical annihilation of wave-like particles (solitons) within a structured
potential scaffold.

Outputs the computation state as an image and the logical result as a signal.
Ported from topological_xor.py.
Requires: pip install numpy scipy
"""

import numpy as np
from PyQt6 import QtGui
import cv2
from scipy.ndimage import gaussian_filter

import __main__
BaseNode = __main__.BaseNode
PA_INSTANCE = getattr(__main__, "PA_INSTANCE", None)
QtGui = __main__.QtGui

try:
    from scipy.ndimage import gaussian_filter
    SCIPY_AVAILABLE = True
except ImportError:
    SCIPY_AVAILABLE = False
    print("Warning: TopologicalXORNode requires 'scipy'.")


# --- Simulation Constants (optimized for node) ---
GRID_SIZE = 96
DT = 0.1  
DAMPING = 0.99 
A_LAUNCH_POS = 0.25 # Y position for Input A (normalized)
B_LAUNCH_POS = 0.75 # Y position for Input B (normalized)
OUTPUT_Y_POS = 0.5  # Y position for the output channel (normalized)


class TopologicalGate:
    def __init__(self, size):
        self.size = size
        self.psi = np.zeros((size, size), dtype=np.complex64)
        self.psi_prev = np.zeros((size, size), dtype=np.complex64)
        self.information_density = np.zeros((size, size), dtype=np.float32)
        self.environmental_potential = self._create_xor_gate_scaffold()
        self.output_y_px = int(OUTPUT_Y_POS * self.size)
        
        # State tracking for XOR result
        self.result = 0.0
        self.last_state_check_time = 0

    def _create_xor_gate_scaffold(self):
        """Creates a hard-coded potential to act as an XOR gate."""
        potential = np.ones((self.size, self.size), dtype=np.float32) * 0.1

        channel_width = 8
        junction_x = self.size // 2

        # --- Input Wire A (from top-left) ---
        yA = int(A_LAUNCH_POS * self.size)
        potential[yA - channel_width//2 : yA + channel_width//2, :junction_x] = -0.1
        
        # --- Input Wire B (from bottom-left) ---
        yB = int(B_LAUNCH_POS * self.size)
        potential[yB - channel_width//2 : yB + channel_width//2, :junction_x] = -0.1
            
        # --- Output Wire C (to the right) ---
        output_y = int(OUTPUT_Y_POS * self.size)
        potential[output_y - channel_width//2 : output_y + channel_width//2, junction_x:] = -0.1
        
        return gaussian_filter(potential, sigma=2.0)

    def evolve(self):
        """Evolve the field using non-linear dynamics for particle interaction."""
        laplacian = (np.roll(self.psi, 1, axis=0) + np.roll(self.psi, -1, axis=0) +
                     np.roll(self.psi, 1, axis=1) + np.roll(self.psi, -1, axis=1) - 4 * self.psi)

        # Non-linear potential for annihilation/stability
        psi_sq = np.abs(self.psi)**2
        # Non-linear term (simplified Mexican Hat potential derivative)
        nonlinear_term = self.psi * (psi_sq - 1.0) 

        # The evolution equation (Non-linear wave evolution)
        psi_next = (2 * self.psi - self.psi_prev * DAMPING +
                    DT**2 * (laplacian - nonlinear_term) - 
                    self.environmental_potential * self.psi)

        self.psi_prev, self.psi = self.psi, psi_next
        
        # Calculate information density (gradient squared) for visualization
        grad_x, grad_y = np.gradient(np.abs(self.psi))
        self.information_density = grad_x**2 + grad_y**2

    def launch_soliton(self, start_y, amplitude=2.5):
        """Launches a soliton (a '1' bit) down the wire at a specific Y-position."""
        yy, xx = np.mgrid[0:self.size, 0:self.size]
        start_x = 5
        
        dist_sq = (xx - start_x)**2 + (yy - start_y)**2
        
        pulse = amplitude * np.exp(-dist_sq / 10.0)
        self.psi += pulse.astype(np.complex64)

    def measure_output(self, measure_window=5, measure_time_step=20):
        """Measures the field amplitude in the output channel to determine the XOR result."""
        
        # Only check once every X steps to give the field time to settle
        if self.last_state_check_time < measure_time_step:
            self.last_state_check_time += 1
            return self.result
            
        self.last_state_check_time = 0 # Reset timer
        
        # Define the measurement area (far right of the grid)
        measurement_area = self.psi[self.output_y_px - measure_window:self.output_y_px + measure_window, 
                                   self.size - measure_window*2:self.size - measure_window]
        
        # The result is 1 if the field amplitude is significant (a soliton survived)
        max_amplitude = np.max(np.abs(measurement_area))
        
        # If the amplitude is above a threshold, the result is 1
        if max_amplitude > 0.5:
            self.result = 1.0
            # Annihilate the soliton to prepare for the next computation
            self.psi[self.output_y_px - measure_window:self.output_y_px + measure_window, 
                     self.size - measure_window*2:self.size - measure_window] = 0j
        else:
            self.result = 0.0
            
        return self.result
        
    def reset_field(self):
        """Clear the field for a new computation."""
        self.psi.fill(0j)
        self.psi_prev.fill(0j)
        self.information_density.fill(0.0)
        self.result = 0.0
        self.last_state_check_time = 0


class TopologicalXORNode(BaseNode):
    NODE_CATEGORY = "Analysis"
    NODE_COLOR = QtGui.QColor(150, 50, 50) # Chaotic Red for Computation
    
    def __init__(self, size=96):
        super().__init__()
        self.node_title = "Topological XOR"
        
        self.inputs = {
            'input_A': 'signal', # 0 or 1 bit
            'input_B': 'signal', # 0 or 1 bit
            'compute_trigger': 'signal', # Rising edge triggers launch
            'reset': 'signal'
        }
        self.outputs = {
            'output_C': 'signal',          # XOR result (0 or 1)
            'computation_image': 'image',  # Information Density
            'xor_state': 'signal'          # 0=Idle, 1=Computing, 2=Done
        }
        
        if not SCIPY_AVAILABLE:
            self.node_title = "XOR (No SciPy!)"
            return
            
        self.size = int(size)
        self.sim = TopologicalGate(self.size)
        self.last_trigger_val = 0.0
        self.current_result = 0.0
        self.computation_state = 0.0 # 0=Idle, 1=Computing, 2=Done

    def _launch_if_one(self, signal, y_norm_pos):
        """Launches a soliton if the signal is high (>= 0.5)."""
        if signal >= 0.5:
            self.sim.launch_soliton(int(y_norm_pos * self.size))

    def randomize(self):
        """The 'randomize' button acts as a full reset here."""
        self.sim.reset_field()
        self.computation_state = 0.0

    def step(self):
        if not SCIPY_AVAILABLE:
            return
            
        # 1. Handle Inputs
        trigger_val = self.get_blended_input('compute_trigger', 'sum') or 0.0
        reset_sig = self.get_blended_input('reset', 'sum')
        input_A_sig = self.get_blended_input('input_A', 'sum') or 0.0
        input_B_sig = self.get_blended_input('input_B', 'sum') or 0.0

        if reset_sig is not None and reset_sig > 0.5:
            self.randomize()
            return
            
        # 2. Computation Logic (Rising edge triggers launch)
        if trigger_val > 0.5 and self.last_trigger_val <= 0.5:
            self.sim.reset_field() # Ensure clean start
            self.computation_state = 1.0 # State: Computing
            
            # Launch solitons based on input bits (0 or 1)
            self._launch_if_one(round(input_A_sig), A_LAUNCH_POS)
            self._launch_if_one(round(input_B_sig), B_LAUNCH_POS)
            
        self.last_trigger_val = trigger_val

        # 3. Always Evolve the Physics
        self.sim.evolve()
        
        # 4. Measure Output (if computing)
        if self.computation_state == 1.0:
            self.current_result = self.sim.measure_output()
            # If the output has been measured and the field is quiet, computation is done
            if self.current_result in [0.0, 1.0] and self.sim.last_state_check_time == 0:
                self.computation_state = 2.0 # State: Done

    def get_output(self, port_name):
        if port_name == 'output_C':
            return self.current_result
        elif port_name == 'computation_image':
            # Output Information Density
            max_val = np.max(self.sim.information_density)
            if max_val > 1e-9:
                return self.sim.information_density.T / max_val
            return self.sim.information_density.T
        elif port_name == 'xor_state':
            return self.computation_state
            
        return None
        
    def get_display_image(self):
        # 1. Base Visualization: Information Density
        img_data = self.get_output('computation_image')
        if img_data is None: 
            return None
            
        img_u8 = (np.clip(img_data, 0, 1) * 255).astype(np.uint8)
        img_color = cv2.applyColorMap(img_u8, cv2.COLORMAP_VIRIDIS)
        
        # 2. Output Indicator
        bar_color = (0, 0, 0)
        if self.computation_state == 2.0:
             bar_color = (0, 255, 0) if self.current_result == 1.0 else (0, 0, 255)
        elif self.computation_state == 1.0:
             bar_color = (255, 255, 0)
             
        h, w = img_color.shape[:2]
        cv2.rectangle(img_color, (w-15, 0), (w, 15), bar_color, -1) # Top right status light
        
        # 3. Resize to thumbnail size
        img_resized = cv2.resize(img_color, (96, 96), interpolation=cv2.INTER_LINEAR)
        img_resized = np.ascontiguousarray(img_resized)
        
        h, w = img_resized.shape[:2]
        return QtGui.QImage(img_resized.data, w, h, 3*w, QtGui.QImage.Format.Format_BGR888)

    def get_config_options(self):
        return [
            ("Resolution (NxN)", "size", self.size, None),
        ]

=== FILE: topologyanalyzernode.py ===

"""
TopologyAnalyzerNode

Analyzes the output of an InstantonFieldNode.
It finds the "stable, localized information structures" (instantons)
and calculates metrics like count, total accumulated "action",
and "long-range order."
"""

import numpy as np
import cv2

# --- Magic import block ---
import __main__
BaseNode = __main__.BaseNode
QtGui = __main__.QtGui
# --------------------------

class TopologyAnalyzerNode(BaseNode):
    """
    Finds instantons and calculates their properties.
    """
    NODE_CATEGORY = "Analyzer"
    NODE_COLOR = QtGui.QColor(220, 200, 100) # Gold

    def __init__(self, size=128):
        super().__init__()
        self.node_title = "Topology Analyzer"
        
        self.inputs = {
            'field_in': 'image',     # The raw 'field_out' from InstantonFieldNode
            'threshold': 'signal'    # 0-1, threshold to define an instanton
        }
        self.outputs = {
            'instanton_count': 'signal', # Number of instantons
            'total_action': 'signal',    # Total accumulated information
            'long_range_order': 'signal' # 0-1, how spread out instantons are
        }
        
        self.size = int(size)
        
        # Internal state
        self.instanton_count = 0.0
        self.total_action = 0.0
        self.long_range_order = 0.0

    def step(self):
        # --- 1. Get and Prepare Image ---
        field = self.get_blended_input('field_in', 'first')
        if field is None:
            return

        # Ensure field is 0-1 float
        if field.dtype != np.float32:
            field = field.astype(np.float32)
        if field.max() > 1.0:
            field /= 255.0
            
        # Resize and ensure grayscale
        field = cv2.resize(field, (self.size, self.size), 
                           interpolation=cv2.INTER_LINEAR)
        if field.ndim == 3:
            field_gray = cv2.cvtColor(field, cv2.COLOR_RGB2GRAY)
        else:
            field_gray = field
        
        # --- 2. Calculate Total "Action" ---
        # "Information weight accumulation"
        self.total_action = np.sum(field_gray) / self.size # Normalize by size

        # --- 3. Find Instantons ---
        threshold = self.get_blended_input('threshold', 'sum') or 0.5
        _ , binary = cv2.threshold(
            (field_gray * 255).astype(np.uint8), 
            int(threshold * 255), 
            255, 
            cv2.THRESH_BINARY
        )
        
        # Find contours (the instantons)
        contours, _ = cv2.findContours(binary, cv2.RETR_EXTERNAL, 
                                       cv2.CHAIN_APPROX_SIMPLE)
        
        self.instanton_count = len(contours)
        
        # --- 4. Calculate "Long-Range Order" ---
        if self.instanton_count > 1:
            centers = []
            for cnt in contours:
                M = cv2.moments(cnt)
                if M['m00'] > 0:
                    cx = M['m10'] / M['m00']
                    cy = M['m01'] / M['m00']
                    centers.append([cx, cy])
            
            if len(centers) > 1:
                # Calculate the std deviation of instanton positions
                # A high std dev means they are spread out (high long-range order)
                centers = np.array(centers)
                std_dev_x = np.std(centers[:, 0])
                std_dev_y = np.std(centers[:, 1])
                
                # Normalize by the max possible std dev (size / 2)
                self.long_range_order = (std_dev_x + std_dev_y) / self.size
                self.long_range_order = np.clip(self.long_range_order, 0, 1)
            else:
                self.long_range_order = 0.0
        else:
            self.long_range_order = 0.0

    def get_output(self, port_name):
        if port_name == 'instanton_count':
            return self.instanton_count
        elif port_name == 'total_action':
            return self.total_action
        elif port_name == 'long_range_order':
            return self.long_range_order
        return None

    def get_display_image(self):
        # Create a simple text display
        img = np.zeros((self.size, self.size, 3), dtype=np.uint8)
        font = cv2.FONT_HERSHEY_SIMPLEX
        
        cv2.putText(img, f"Instantons: {self.instanton_count}", (10, 20), 
                    font, 0.5, (255, 255, 255), 1, cv2.LINE_AA)
        cv2.putText(img, f"Total Action: {self.total_action:.2f}", (10, 40), 
                    font, 0.5, (255, 255, 255), 1, cv2.LINE_AA)
        cv2.putText(img, f"Long-Range Order: {self.long_range_order:.2f}", (10, 60), 
                    font, 0.5, (255, 255, 255), 1, cv2.LINE_AA)
                    
        return img.astype(np.float32) / 255.0

=== FILE: transform_node.py ===

"""
Spectral Memory Node - Applies temporal filtering in the frequency domain
Place this file in the 'nodes' folder
"""

import numpy as np
from PyQt6 import QtGui
import cv2

import sys
import os
# --- This is the new, correct block ---
import __main__
BaseNode = __main__.BaseNode
PA_INSTANCE = getattr(__main__, "PA_INSTANCE", None)
# ------------------------------------

class SpectralMemoryNode(BaseNode):
    NODE_CATEGORY = "Transform"
    NODE_COLOR = QtGui.QColor(100, 150, 255) # A "complex" blue
    
    def __init__(self, decay=0.9, boost=0.1):
        super().__init__()
        self.node_title = "Spectral Memory"
        self.inputs = {
            'complex_spectrum': 'complex_spectrum',
            'decay': 'signal',
            'boost': 'signal'
        }
        self.outputs = {'complex_spectrum': 'complex_spectrum', 'image': 'image'}
        
        self.decay = float(decay)
        self.boost = float(boost)
        
        # The memory
        self.memory = None
        self.vis_img = np.zeros((64, 64), dtype=np.float32)

    def step(self):
        # Get parameters from inputs
        decay_in = self.get_blended_input('decay', 'sum')
        boost_in = self.get_blended_input('boost', 'sum')
        
        if decay_in is not None:
            self.decay = np.clip(decay_in, 0.8, 1.0) # Map [0,1] to [0.8, 1.0]
        if boost_in is not None:
            self.boost = np.clip(boost_in, 0.0, 0.2) # Map [0,1] to [0.0, 0.2]
            
        # Get the input spectrum
        spec_in = self.get_blended_input('complex_spectrum', 'mean')
        
        if spec_in is None:
            # If no input, just decay the memory
            if self.memory is not None:
                self.memory *= self.decay
            self.vis_img *= 0.95
            return
            
        # Initialize memory if this is the first frame
        if self.memory is None or self.memory.shape != spec_in.shape:
            self.memory = np.zeros_like(spec_in, dtype=np.complex128)
            
        # Apply the leaky integrator (memory)
        # memory = memory * decay + new_input * (1.0 - decay)
        self.memory = (self.memory * self.decay) + (spec_in * (1.0 - self.decay))
        
        # Apply boost (adds a bit of the raw signal back in)
        output_spec = self.memory + (spec_in * self.boost)

        # Update visualization (log magnitude)
        mag = np.log1p(np.abs(output_spec))
        if mag.max() > mag.min():
            mag = (mag - mag.min()) / (mag.max() - mag.min())
        
        self.vis_img = cv2.resize(mag, (64, 64)).astype(np.float32)

    def get_output(self, port_name):
        if port_name == 'complex_spectrum':
            return self.memory
        elif port_name == 'image':
            return self.vis_img
        return None
        
    def get_display_image(self):
        img_u8 = (np.clip(self.vis_img, 0, 1) * 255).astype(np.uint8)
        img_u8 = np.ascontiguousarray(img_u8)
        return QtGui.QImage(img_u8.data, 64, 64, 64, QtGui.QImage.Format.Format_Grayscale8)

    def get_config_options(self):
        return [
            ("Decay (0.8-1.0)", "decay", self.decay, None),
            ("Boost (0.0-0.2)", "boost", self.boost, None),
        ]

=== FILE: vectorsplitternode.py ===

"""
Vector Splitter Node - ENHANCED
Splits latent vectors with scaling and visualization improvements
"""

import numpy as np
import cv2

import __main__
BaseNode = __main__.BaseNode
QtGui = __main__.QtGui

class VectorSplitterNode(BaseNode):
    """Splits a spectrum into N signal outputs with optional scaling"""
    NODE_CATEGORY = "Utility"
    NODE_COLOR = QtGui.QColor(150, 150, 150)
    
    def __init__(self, num_outputs=16, scale=1.0):  # Default 16 for VAE
        super().__init__()
        self.node_title = "Vector Splitter"
        
        self.num_outputs = int(num_outputs)
        self.scale = float(scale)
        
        self.inputs = {
            'spectrum_in': 'spectrum',
            'scale': 'signal'  # Optional dynamic scaling
        }
        
        self.outputs = {}
        for i in range(self.num_outputs):
            self.outputs[f'out_{i}'] = 'signal'
        
        self.output_values = np.zeros(self.num_outputs, dtype=np.float32)
    
    def step(self):
        vector = self.get_blended_input('spectrum_in', 'first')
        scale_signal = self.get_blended_input('scale', 'sum')
        
        if scale_signal is not None:
            scale = scale_signal
        else:
            scale = self.scale
        
        if vector is None:
            self.output_values.fill(0.0)
            return
        
        if vector.ndim > 1:
            vector = vector.flatten()
        
        for i in range(self.num_outputs):
            if i < len(vector):
                self.output_values[i] = float(vector[i]) * scale
            else:
                self.output_values[i] = 0.0
    
    def get_output(self, port_name):
        if port_name.startswith('out_'):
            try:
                index = int(port_name.split('_')[1])
                if 0 <= index < self.num_outputs:
                    return self.output_values[index]
            except (ValueError, IndexError):
                pass
        return None
    
    def get_display_image(self):
        w, h = 256, 128  # Bigger display
        img = np.zeros((h, w, 3), dtype=np.uint8)
        
        if self.output_values.size == 0:
            return QtGui.QImage(img.data, w, h, 3*w, QtGui.QImage.Format.Format_RGB888)
        
        bar_width = max(1, w // self.num_outputs)
        
        val_max = np.abs(self.output_values).max()
        if val_max < 1e-6: 
            val_max = 1.0
        
        for i, val in enumerate(self.output_values):
            x = i * bar_width
            norm_val = val / val_max
            bar_h = int(abs(norm_val) * (h/2 - 10))
            y_base = h // 2
            
            if val >= 0:
                color = (0, int(255 * abs(norm_val)), 0)
                cv2.rectangle(img, (x, y_base-bar_h), (x+bar_width-1, y_base), color, -1)
            else:
                color = (0, 0, int(255 * abs(norm_val)))
                cv2.rectangle(img, (x, y_base), (x+bar_width-1, y_base+bar_h), color, -1)
            
            # Label every 4th output
            if i % 4 == 0:
                cv2.putText(img, str(i), (x+2, h-5), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.3, (255,255,255), 1)
        
        # Baseline
        cv2.line(img, (0, h//2), (w, h//2), (100,100,100), 1)
        
        # Show scale
        cv2.putText(img, f"x{self.scale:.2f}", (5, 15), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255,255,0), 1)
        
        return QtGui.QImage(img.data, w, h, 3*w, QtGui.QImage.Format.Format_RGB888)
    
    def get_config_options(self):
        return [
            ("Num Outputs", "num_outputs", self.num_outputs, None),
            ("Scale", "scale", self.scale, None)
        ]


=== FILE: visual_scalogram.py ===

"""
Scalogram Analyzer Node - Computes a CWT scalogram from an image's center slice
Place this file in the 'nodes' folder
"""

import numpy as np
from PyQt6 import QtGui
import cv2

import sys
import os
# --- This is the new, correct block ---
import __main__
BaseNode = __main__.BaseNode
PA_INSTANCE = getattr(__main__, "PA_INSTANCE", None)
# ------------------------------------

try:
    import pywt
    PYWT_AVAILABLE = True
except ImportError:
    PYWT_AVAILABLE = False
    print("Warning: ScalogramAnalyzerNode requires 'PyWavelets'.")
    print("Please run: pip install PyWavelets")

class ScalogramAnalyzerNode(BaseNode):
    NODE_CATEGORY = "Transform"
    NODE_COLOR = QtGui.QColor(60, 180, 160) # A teal/aqua color
    
    def __init__(self, num_scales=64, wavelet_name='morl'):
        super().__init__()
        self.node_title = "Scalogram Analyzer"
        
        self.inputs = {'image': 'image'}
        self.outputs = {'image': 'image'}
        
        self.num_scales = int(num_scales)
        self.wavelet_name = str(wavelet_name)
        
        self.output_image = np.zeros((self.num_scales, 128), dtype=np.float32)
        
        if not PYWT_AVAILABLE:
            self.node_title = "Scalogram (No PyWT!)"

    def step(self):
        if not PYWT_AVAILABLE:
            return

        input_img = self.get_blended_input('image', 'mean')
        
        if input_img is None:
            self.output_image *= 0.95 # Fade to black
            return
            
        try:
            # Extract the middle row as a 1D signal
            h, w = input_img.shape
            signal_1d = input_img[h // 2, :]
            
            # Define the scales to analyze
            # We use a logarithmic space for scales, which is common
            scales = np.geomspace(1, w / 2, self.num_scales)
            
            # Compute the Continuous Wavelet Transform (CWT)
            cfs, freqs = pywt.cwt(signal_1d, scales, self.wavelet_name)
            
            # The result is the scalogram (magnitude of coefficients)
            scalogram = np.abs(cfs)
            
            # Normalize for visualization
            s_min, s_max = scalogram.min(), scalogram.max()
            if (s_max - s_min) > 1e-9:
                scalogram = (scalogram - s_min) / (s_max - s_min)
                
            # Resize to fit a standard display aspect
            self.output_image = cv2.resize(scalogram, (w, self.num_scales),
                                           interpolation=cv2.INTER_LINEAR)
                                           
        except Exception as e:
            print(f"Scalogram Error: {e}")
            self.output_image *= 0.95

    def get_output(self, port_name):
        if port_name == 'image':
            return self.output_image
        return None
        
    def get_display_image(self):
        img_u8 = (np.clip(self.output_image, 0, 1) * 255).astype(np.uint8)
        
        # Apply a colormap for better visibility
        img_color = cv2.applyColorMap(img_u8, cv2.COLORMAP_JET)
        
        img_color = np.ascontiguousarray(img_color)
        h, w = img_color.shape[:2]
        return QtGui.QImage(img_color.data, w, h, 3*w, QtGui.QImage.Format.Format_BGR888)

    def get_config_options(self):
        # Common wavelets for CWT
        wavelet_options = [
            ("Morlet ('morl')", "morl"),
            ("Mexican Hat ('mexh')", "mexh"),
            ("Gaussian 1 ('gaus1')", "gaus1"),
            ("Complex Morlet ('cmor1.5-1.0')", "cmor1.5-1.0")
        ]
        
        return [
            ("Wavelet", "wavelet_name", self.wavelet_name, wavelet_options),
            ("Number of Scales", "num_scales", self.num_scales, None),
        ]

=== FILE: wavelet_decompose.py ===

"""
Wavelet Decompose Node - Decomposes an image into DWT sub-bands (LL, LH, HL, HH)
Place this file in the 'nodes' folder
"""

import numpy as np
from PyQt6 import QtGui
import cv2

import sys
import os
# --- This is the new, correct block ---
import __main__
BaseNode = __main__.BaseNode
PA_INSTANCE = getattr(__main__, "PA_INSTANCE", None)
# ------------------------------------

try:
    import pywt
    PYWT_AVAILABLE = True
except ImportError:
    PYWT_AVAILABLE = False
    print("Warning: WaveletDecomposeNode requires 'PyWavelets'.")
    print("Please run: pip install PyWavelets")

class WaveletDecomposeNode(BaseNode):
    NODE_CATEGORY = "Transform"
    NODE_COLOR = QtGui.QColor(220, 120, 40) # Transform Orange
    
    def __init__(self, wavelet_name='haar', size=128):
        super().__init__()
        self.node_title = "Wavelet Decompose (DWT)"
        
        self.inputs = {'image': 'image'}
        self.outputs = {
            'LL': 'image', # Approximation
            'LH': 'image', # Horizontal Detail
            'HL': 'image', # Vertical Detail
            'HH': 'image'  # Diagonal Detail
        }
        
        self.wavelet_name = str(wavelet_name)
        self.size = int(size)
        
        self._init_arrays()
        
        if not PYWT_AVAILABLE:
            self.node_title = "DWT (No PyWT!)"

    def _init_arrays(self):
        """Initializes or re-initializes all internal arrays based on self.size"""
        self.size = int(self.size // 2 * 2) 
        
        try:
            # --- FIX: Get the filter_len (int) from the wavelet_name (str) ---
            wavelet = pywt.Wavelet(self.wavelet_name)
            filter_len = wavelet.dec_len 
            h = pywt.dwt_coeff_len(self.size, filter_len, mode='symmetric')
            # --- END FIX ---
            w = h
        except (ValueError, TypeError): # Catch errors from bad wavelet name or the pywt call
            h = self.size // 2
            w = self.size // 2

        self.ll_out = np.zeros((h, w), dtype=np.float32)
        self.lh_out = np.zeros((h, w), dtype=np.float32)
        self.hl_out = np.zeros((h, w), dtype=np.float32)
        self.hh_out = np.zeros((h, w), dtype=np.float32)
        
        self.display_tiled = np.zeros((h*2, w*2), dtype=np.float32)

    def _normalize(self, arr):
        """Normalize an array to [0, 1] for visualization."""
        arr_min, arr_max = arr.min(), arr.max()
        if (arr_max - arr_min) > 1e-9:
            return (arr - arr_min) / (arr_max - arr_min)
        return arr - arr_min # Return zero array

    def step(self):
        if not PYWT_AVAILABLE:
            return
            
        # --- FIX: Use the correct filter_len (int) in the check ---
        try:
            wavelet = pywt.Wavelet(self.wavelet_name)
            filter_len = wavelet.dec_len
            expected_h = pywt.dwt_coeff_len(self.size, filter_len, mode='symmetric')
        except (ValueError, TypeError):
            expected_h = self.size // 2
            
        if self.ll_out.shape[0] != expected_h:
            self._init_arrays()
        # --- END FIX ---

        input_img = self.get_blended_input('image', 'mean')
        
        if input_img is None:
            # Fade all outputs
            self.ll_out *= 0.95
            self.lh_out *= 0.95
            self.hl_out *= 0.95
            self.hh_out *= 0.95
            return
            
        try:
            # Resize image to a square power-of-2-like size
            img_resized = cv2.resize(input_img, (self.size, self.size), 
                                     interpolation=cv2.INTER_AREA)
            
            # Perform 2D Discrete Wavelet Transform
            coeffs = pywt.dwt2(img_resized, self.wavelet_name)
            LL, (LH, HL, HH) = coeffs
            
            # Store normalized components for output
            self.ll_out = self._normalize(LL)
            self.lh_out = self._normalize(LH)
            self.hl_out = self._normalize(HL)
            self.hh_out = self._normalize(HH)
            
        except Exception as e:
            print(f"DWT Error: {e}")

    def get_output(self, port_name):
        if port_name == 'LL':
            return self.ll_out
        elif port_name == 'LH':
            return self.lh_out
        elif port_name == 'HL':
            return self.hl_out
        elif port_name == 'HH':
            return self.hh_out
        return None
        
    def get_display_image(self):
        # Create a tiled image for the node's display
        # Use the shape of the component array, not self.size
        h, w = self.ll_out.shape 
        
        h_total, w_total = h*2, w*2
        if self.display_tiled.shape != (h_total, w_total):
            self.display_tiled = np.zeros((h_total, w_total), dtype=np.float32)
        
        self.display_tiled[:h, :w] = self.ll_out # Top-Left
        self.display_tiled[:h, w:w_total] = self.lh_out # Top-Right
        self.display_tiled[h:h_total, :w] = self.hl_out # Bottom-Left
        self.display_tiled[h:h_total, w:w_total] = self.hh_out # Bottom-Right
        
        img_u8 = (np.clip(self.display_tiled, 0, 1) * 255).astype(np.uint8)
        
        # Resize to a consistent display size (e.g., self.size) for the node preview
        img_u8_resized = cv2.resize(img_u8, (self.size, self.size), interpolation=cv2.INTER_NEAREST)
        img_u8_resized = np.ascontiguousarray(img_u8_resized)
        
        return QtGui.QImage(img_u8_resized.data, self.size, self.size, self.size, QtGui.QImage.Format.Format_Grayscale8)

    def get_config_options(self):
        # Get common wavelets
        wavelet_options = [
            ("Haar ('haar')", "haar"),
            ("Daubechies 1 ('db1')", "db1"),
            ("Daubechies 4 ('db4')", "db4"),
            ("Symlet 2 ('sym2')", "sym2"),
            ("Coiflet 1 ('coif1')", "coif1"),
        ]
        
        return [
            ("Wavelet", "wavelet_name", self.wavelet_name, wavelet_options),
            ("Resolution", "size", self.size, None),
        ]


=== FILE: webcamphasenode.py ===

"""
Webcam Phase Node - Extracts motion dynamics into 3D phase space coordinates
This is different from FFT - it tracks MOTION VECTORS and converts them to attractor-ready signals.

Inspired by the Neural String Attractor system.
Place this file in the 'nodes' folder
"""

import numpy as np
from PyQt6 import QtGui
import cv2

import __main__
BaseNode = __main__.BaseNode
PA_INSTANCE = getattr(__main__, "PA_INSTANCE", None)

class WebcamPhaseNode(BaseNode):
    NODE_CATEGORY = "Source"
    NODE_COLOR = QtGui.QColor(60, 140, 180)  # Webcam blue-cyan

    def __init__(self, device_id=0, motion_sensitivity=1.0):
        super().__init__()
        self.node_title = "Webcam Phase"

        self.outputs = {
            'phase_x': 'signal',      # X-axis motion (horizontal)
            'phase_y': 'signal',      # Y-axis motion (vertical)
            'phase_z': 'signal',      # Z-axis (temporal change/energy)
            'motion_image': 'image',  # Visual feedback
            'energy': 'signal'        # Total motion energy
        }

        self.device_id = int(device_id)
        self.motion_sensitivity = float(motion_sensitivity)

        # OpenCV capture
        self.cap = None
        self.previous_frame = None
        self.previous_gray = None

        # Motion history buffer (for temporal phase Z)
        self.motion_history = np.zeros(30, dtype=np.float32)
        self.history_idx = 0

        # Phase space coordinates
        self.phase_x = 0.0
        self.phase_y = 0.0
        self.phase_z = 0.0
        self.energy = 0.0

        # Motion visualization
        self.motion_vis = np.zeros((120, 160), dtype=np.uint8)

        # Lucas-Kanade optical flow parameters
        self.lk_params = dict(
            winSize=(15, 15),
            maxLevel=2,
            criteria=(cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03)
        )

        # Feature detection parameters
        self.feature_params = dict(
            maxCorners=50,
            qualityLevel=0.3,
            minDistance=7,
            blockSize=7
        )

        self.tracked_points = None

        self.setup_source()

    def setup_source(self):
        """Initialize webcam capture"""
        if self.cap and self.cap.isOpened():
            self.cap.release()

        try:
            self.cap = cv2.VideoCapture(self.device_id)
            if self.cap.isOpened():
                self.cap.set(cv2.CAP_PROP_FRAME_WIDTH, 320)
                self.cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 240)
                self.node_title = f"Webcam Phase ({self.device_id})"
            else:
                self.node_title = "Webcam Phase (NO CAM)"
        except Exception as e:
            print(f"Webcam Phase Error: {e}")
            self.cap = None
            self.node_title = "Webcam Phase (ERROR)"

    def step(self):
        if not self.cap or not self.cap.isOpened():
            # Decay outputs if no camera
            self.phase_x *= 0.95
            self.phase_y *= 0.95
            self.phase_z *= 0.95
            self.energy *= 0.95
            return

        # Capture frame
        ret, frame = self.cap.read()
        if not ret:
            return

        # Convert to grayscale
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        gray = cv2.GaussianBlur(gray, (5, 5), 0)

        if self.previous_gray is None:
            self.previous_gray = gray
            self.tracked_points = cv2.goodFeaturesToTrack(gray, mask=None, **self.feature_params)
            return

        # --- OPTICAL FLOW TRACKING ---
        if self.tracked_points is not None and len(self.tracked_points) > 0:
            # Calculate optical flow
            new_points, status, error = cv2.calcOpticalFlowPyrLK(
                self.previous_gray, gray, self.tracked_points, None, **self.lk_params
            )

            if new_points is not None:
                # Select good points
                good_new = new_points[status == 1]
                good_old = self.tracked_points[status == 1]

                if len(good_new) > 0:
                    # Calculate motion vectors
                    motion_vectors = good_new - good_old

                    # Extract phase coordinates from motion
                    # X: Horizontal motion (average X displacement)
                    self.phase_x = np.mean(motion_vectors[:, 0]) * self.motion_sensitivity

                    # Y: Vertical motion (average Y displacement)
                    self.phase_y = np.mean(motion_vectors[:, 1]) * self.motion_sensitivity

                    # Energy: Magnitude of motion
                    motion_magnitudes = np.linalg.norm(motion_vectors, axis=1)
                    self.energy = np.mean(motion_magnitudes) * self.motion_sensitivity * 0.1

                    # Store in history for Z calculation
                    self.motion_history[self.history_idx] = self.energy
                    self.history_idx = (self.history_idx + 1) % len(self.motion_history)

                    # Z: Temporal dynamics (change in energy over time)
                    energy_gradient = np.gradient(self.motion_history)
                    self.phase_z = np.mean(energy_gradient) * 10.0

                    # Clamp to reasonable ranges
                    self.phase_x = np.clip(self.phase_x, -1.0, 1.0)
                    self.phase_y = np.clip(self.phase_y, -1.0, 1.0)
                    self.phase_z = np.clip(self.phase_z, -1.0, 1.0)
                    self.energy = np.clip(self.energy, 0.0, 1.0)

                    # Update tracked points
                    self.tracked_points = good_new.reshape(-1, 1, 2)

                    # Create motion visualization
                    self.create_motion_visualization(gray, good_old, good_new)
                else:
                    # No good points, re-detect
                    self.tracked_points = cv2.goodFeaturesToTrack(gray, mask=None, **self.feature_params)
            else:
                # Flow calculation failed, re-detect
                self.tracked_points = cv2.goodFeaturesToTrack(gray, mask=None, **self.feature_params)
        else:
            # No points tracked, detect new ones
            self.tracked_points = cv2.goodFeaturesToTrack(gray, mask=None, **self.feature_params)

        # Refresh points periodically
        if np.random.rand() < 0.05:  # 5% chance each frame
            self.tracked_points = cv2.goodFeaturesToTrack(gray, mask=None, **self.feature_params)

        self.previous_gray = gray

    def create_motion_visualization(self, gray, old_points, new_points):
        """Create a visual representation of motion vectors"""
        # Resize for output
        vis_gray = cv2.resize(gray, (160, 120))

        # Normalize to 0-255
        vis = cv2.normalize(vis_gray, None, 0, 255, cv2.NORM_MINMAX)

        # Draw motion vectors
        scale = 160 / gray.shape[1]  # Scaling factor for coordinates

        for old_pt, new_pt in zip(old_points, new_points):
            old_scaled = (int(old_pt[0] * scale), int(old_pt[1] * scale))
            new_scaled = (int(new_pt[0] * scale), int(new_pt[1] * scale))

            # Draw line
            cv2.arrowedLine(vis, old_scaled, new_scaled, 255, 1, tipLength=0.3)
            # Draw points
            cv2.circle(vis, new_scaled, 2, 255, -1)

        self.motion_vis = vis

    def get_output(self, port_name):
        if port_name == 'phase_x':
            return self.phase_x
        elif port_name == 'phase_y':
            return self.phase_y
        elif port_name == 'phase_z':
            return self.phase_z
        elif port_name == 'energy':
            return self.energy
        elif port_name == 'motion_image':
            return self.motion_vis.astype(np.float32) / 255.0
        return None

    def get_display_image(self):
        # Show motion visualization
        img = np.ascontiguousarray(self.motion_vis)
        h, w = img.shape
        return QtGui.QImage(img.data, w, h, w, QtGui.QImage.Format.Format_Grayscale8)

    def get_config_options(self):
        # Get available cameras
        camera_options = [("Default Camera (0)", 0), ("Secondary (1)", 1), ("Third (2)", 2)]

        return [
            ("Camera Device", "device_id", self.device_id, camera_options),
            ("Motion Sensitivity", "motion_sensitivity", self.motion_sensitivity, None),
        ]

=== FILE: worldsubstratenode.py ===

"""
World Substrate Node - The "External World" for the Human Attractor.
A complex, self-evolving field that generates perception, reward, and pain signals.

- Perception (psi_external) = Average field energy
- Reward (dopamine) = Field stability/coherence
- Pain (pain_stimulus) = Sudden, chaotic instanton/decay events

Based on the physics of ResonantInstantonModel from instantonassim x.py
Requires: pip install numpy scipy
"""

import numpy as np
from PyQt6 import QtGui
import cv2
from scipy.ndimage import gaussian_filter
import sys
import os

import __main__
BaseNode = __main__.BaseNode
PA_INSTANCE = getattr(__main__, "PA_INSTANCE", None)
QtGui = __main__.QtGui

try:
    from scipy.ndimage import gaussian_filter
    SCIPY_AVAILABLE = True
except ImportError:
    SCIPY_AVAILABLE = False
    print("Warning: WorldSubstrateNode requires 'scipy'.")

# --- Core Physics Engine (from instantonassim x.py) ---
class WorldField:
    def __init__(self, grid_size=96, dt=0.05, c=1.0, a=0.1, b=0.1, gamma=0.02, substrate_noise=0.0005):
        self.grid_size = grid_size
        self.dt = dt
        self.c = c
        self.a = a
        self.b = b
        self.gamma = gamma
        self.substrate_noise = substrate_noise
        
        self.phi = np.zeros((grid_size, grid_size))
        self.phi_prev = np.zeros((grid_size, grid_size))
        self.stability_metric = 1.0
        self.time = 0.0
        
        # Output signals
        self.psi_external_out = 0.0
        self.dopamine_out = 0.0
        self.pain_out = 0.0
        
        self.initialize_field()

    def initialize_field(self):
        """Initialize with a complex, multi-modal field."""
        position = (self.grid_size // 2, self.grid_size // 2)
        self.phi = np.zeros((self.grid_size, self.grid_size))
        
        # Create a complex initial state
        x, y = np.meshgrid(np.arange(self.grid_size), np.arange(self.grid_size))
        r = np.sqrt((x - position[0])**2 + (y - position[1])**2)
        
        # Add a few "lumps" (pseudo-atoms)
        self.phi += 1.5 * np.exp(-r**2 / (2 * (self.grid_size/8)**2))
        self.phi += 1.0 * np.exp(-((x - 20)**2 + (y - 30)**2) / (2 * (self.grid_size/12)**2))
        self.phi -= 1.0 * np.exp(-((x - 70)**2 + (y - 60)**2) / (2 * (self.grid_size/10)**2))
        
        self.phi_prev = self.phi.copy()
        self.time = 0.0
        self.stability_metric = 1.0

    def _laplacian(self, field):
        field_padded = np.pad(field, 1, mode='wrap')
        laplacian = (field_padded[:-2, 1:-1] + field_padded[2:, 1:-1] + 
                     field_padded[1:-1, :-2] + field_padded[1:-1, 2:] - 
                     4 * field_padded[1:-1, 1:-1])
        return laplacian
    
    def _biharmonic(self, field):
        return self._laplacian(self._laplacian(field))

    def step(self):
        phi_old = self.phi.copy()
        
        laplacian_phi = self._laplacian(self.phi)
        biharmonic_phi = self._biharmonic(self.phi) if self.gamma != 0 else 0
        noise = self.substrate_noise * np.random.normal(size=self.phi.shape)
        
        accel = (self.c**2 * laplacian_phi + 
                 self.a * self.phi - 
                 self.b * self.phi**3 - 
                 self.gamma * biharmonic_phi + 
                 noise)
        
        phi_new = 2 * self.phi - self.phi_prev + self.dt**2 * accel
        
        self.phi_prev = self.phi
        self.phi = phi_new
        self.time += self.dt
        
        # --- Compute Outputs for the Human ---
        
        # 1. Pain Signal (Instanton Event)
        # A sudden, chaotic change in the field = pain
        delta_phi_mag = np.mean(np.abs(phi_new - phi_old))
        # If change is large (> 0.01), register as a pain event
        self.pain_out = np.clip(delta_phi_mag * 100.0, 0.0, 1.0)
        
        # 2. Dopamine Signal (Stability)
        # Stability is high if the field is coherent (low variance)
        field_variance = np.std(self.phi)
        self.stability_metric = np.clip(1.0 - field_variance, 0.0, 1.0)
        # Dopamine is high when stability is high
        self.dopamine_out = self.stability_metric
        
        # 3. Perception Signal (psi_external)
        # What the human "sees" is the total energy/activity of the field
        self.psi_external_out = np.mean(np.abs(self.phi))


class WorldSubstrateNode(BaseNode):
    NODE_CATEGORY = "Source"
    NODE_COLOR = QtGui.QColor(20, 150, 150) # Biological Teal
    
    def __init__(self, grid_size=96, substrate_noise=0.0005):
        super().__init__()
        self.node_title = "World Substrate"
        
        self.inputs = {
            'reset': 'signal'
        }
        self.outputs = {
            'field_image': 'image',        # The "World"
            'psi_external': 'signal',      # World perception signal
            'dopamine': 'signal',          # World stability (reward)
            'pain_stimulus': 'signal'      # World instability (pain)
        }
        
        if not SCIPY_AVAILABLE:
            self.node_title = "World (No SciPy!)"
            return
            
        self.grid_size = int(grid_size)
        self.substrate_noise = float(substrate_noise)
        
        self.sim = WorldField(grid_size=self.grid_size, substrate_noise=self.substrate_noise)
        self.last_reset_sig = 0.0

    def randomize(self):
        if SCIPY_AVAILABLE:
            self.sim.initialize_field()

    def step(self):
        if not SCIPY_AVAILABLE: return

        reset_in = self.get_blended_input('reset', 'sum')
        if reset_in is not None and reset_in > 0.5 and self.last_reset_sig <= 0.5:
            self.randomize()
        self.last_reset_sig = reset_in or 0.0
        
        self.sim.step()
    
    def get_output(self, port_name):
        if port_name == 'field_image':
            phi_norm = (self.sim.phi - np.min(self.sim.phi)) / (np.max(self.sim.phi) - np.min(self.sim.phi) + 1e-9)
            return phi_norm.astype(np.float32)
            
        elif port_name == 'psi_external':
            return self.sim.psi_external_out
            
        elif port_name == 'dopamine':
            return self.sim.dopamine_out
            
        elif port_name == 'pain_stimulus':
            return self.sim.pain_out
            
        return None
    
    def get_display_image(self):
        field_data = self.get_output('field_image')
        if field_data is None: return None

        img_u8 = (np.clip(field_data, 0, 1) * 255).astype(np.uint8)
        img_color = cv2.applyColorMap(img_u8, cv2.COLORMAP_VIRIDIS)
        
        # Draw stability metric
        s = self.sim.stability_metric
        color = (0, 255 * s, 255 * (1-s)) # Green for stable, Red for unstable (BGR)
        cv2.rectangle(img_color, (5, 5), (self.sim.grid_size - 5, 15), color, -1)
        cv2.putText(img_color, f"Stab: {s:.2f}", (10, 12), cv2.FONT_HERSHEY_SIMPLEX, 0.3, (0,0,0), 1)

        # Draw pain metric
        p = self.sim.pain_out
        if p > 0.3:
             cv2.putText(img_color, f"PAIN!", (self.sim.grid_size//2 - 10, self.sim.grid_size//2),
                         cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2)
        
        img_thumb = cv2.resize(img_color, (96, 96), interpolation=cv2.INTER_LINEAR)
        img_thumb = np.ascontiguousarray(img_thumb)

        h, w = img_thumb.shape[:2]
        return QtGui.QImage(img_thumb.data, w, h, 3*w, QtGui.QImage.Format.Format_RGB888)

    def get_config_options(self):
        return [
            ("Grid Size (NxN)", "grid_size", self.grid_size, None),
            ("Substrate Noise", "substrate_noise", self.substrate_noise, None),
        ]